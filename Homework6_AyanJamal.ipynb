{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Homework5.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ayannj13/Data-Science/blob/main/Homework6_AyanJamal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Case Intro\n",
        "Term deposits are a major source of income for a bank. A term deposit is a cash investment held at a financial institution. Your money is invested for an agreed rate of interest over a fixed amount of time, or term. The bank has various outreach plans to sell term deposits to their customers such as email marketing, advertisements, telephonic marketing, and digital marketing.\n",
        "\n",
        "Telephonic marketing campaigns still remain one of the most effective way to reach out to people. However, they require huge investment as large call centers are hired to actually execute these campaigns. Hence, it is crucial to identify the customers most likely to convert beforehand so that they can be specifically targeted via call.\n",
        "\n",
        "The data is related to direct marketing campaigns (phone calls) of a Portuguese banking institution. The classification goal is to predict if the client will subscribe to a term deposit (variable y).\n",
        "\n",
        "Content\n",
        "The data is related to the direct marketing campaigns of a Portuguese banking institution. The marketing campaigns were based on phone calls. Often, more than one contact to the same client was required, in order to access if the product (bank term deposit) would be ('yes') or not ('no') subscribed by the customer or not. The data folder contains two datasets:-\n",
        "\n",
        "Bank.csv: 45,211 rows and 18 columns ordered by date (from May 2008 to November 2010)\n",
        "\n",
        "Detailed Column Descriptions\n",
        "bank client data:\n",
        "\n",
        "1 - age (numeric)\n",
        "\n",
        "2 - job : type of job (categorical: \"admin.\",\"unknown\",\"unemployed\",\"management\",\"housemaid\",\"entrepreneur\",\"student\",\n",
        "\"blue-collar\",\"self-employed\",\"retired\",\"technician\",\"services\")\n",
        "\n",
        "3 - marital : marital status (categorical: \"married\",\"divorced\",\"single\"; note: \"divorced\" means divorced or widowed)\n",
        "\n",
        "4 - education (categorical: \"unknown\",\"secondary\",\"primary\",\"tertiary\")\n",
        "\n",
        "5 - default: has credit in default? (binary: \"yes\",\"no\")\n",
        "\n",
        "6 - balance: average yearly balance, in euros (numeric)\n",
        "\n",
        "7 - housing: has housing loan? (binary: \"yes\",\"no\")\n",
        "\n",
        "8 - loan: has personal loan? (binary: \"yes\",\"no\")\n",
        "# related with the last contact of the current campaign:\n",
        "9 - contact: contact communication type (categorical: \"unknown\",\"telephone\",\"cellular\")\n",
        "10 - day: last contact day of the month (numeric)\n",
        "\n",
        "11 - month: last contact month of year (categorical: \"jan\", \"feb\", \"mar\", …, \"nov\", \"dec\")\n",
        "\n",
        "12 - duration: last contact duration, in seconds (numeric)\n",
        "\n",
        "# other attributes:\n",
        "13 - campaign: number of contacts performed during this campaign and for this client (numeric, includes last contact)\n",
        "\n",
        "14 - pdays: number of days that passed by after the client was last contacted from a previous campaign (numeric, -1 means client was not previously contacted)\n",
        "\n",
        "15 - previous: number of contacts performed before this campaign and for this client (numeric)\n",
        "\n",
        "16 - poutcome: outcome of the previous marketing campaign (categorical: \"unknown\",\"other\",\"failure\",\"success\")\n",
        "\n",
        "Output variable (desired target):\n",
        "\n",
        "17 - y - has the client subscribed a term deposit? (binary: \"yes\",\"no\")\n",
        "\n",
        "Missing Attribute Values: None\n"
      ],
      "metadata": {
        "id": "Vk6nkafwIHMh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "df=pd.read_csv('https://raw.githubusercontent.com/ogut77/DataScience/main/data/Bank.csv',sep = ';')\n",
        "df"
      ],
      "metadata": {
        "id": "EZdL1N4MZqZF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "4f020a77-64a5-4a85-c02c-7fa15946a451"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       age           job   marital  education default  balance housing loan  \\\n",
              "0       58    management   married   tertiary      no     2143     yes   no   \n",
              "1       44    technician    single  secondary      no       29     yes   no   \n",
              "2       33  entrepreneur   married  secondary      no        2     yes  yes   \n",
              "3       47   blue-collar   married    unknown      no     1506     yes   no   \n",
              "4       33       unknown    single    unknown      no        1      no   no   \n",
              "...    ...           ...       ...        ...     ...      ...     ...  ...   \n",
              "45206   51    technician   married   tertiary      no      825      no   no   \n",
              "45207   71       retired  divorced    primary      no     1729      no   no   \n",
              "45208   72       retired   married  secondary      no     5715      no   no   \n",
              "45209   57   blue-collar   married  secondary      no      668      no   no   \n",
              "45210   37  entrepreneur   married  secondary      no     2971      no   no   \n",
              "\n",
              "         contact  day month  duration  campaign  pdays  previous poutcome    y  \n",
              "0        unknown    5   may       261         1     -1         0  unknown   no  \n",
              "1        unknown    5   may       151         1     -1         0  unknown   no  \n",
              "2        unknown    5   may        76         1     -1         0  unknown   no  \n",
              "3        unknown    5   may        92         1     -1         0  unknown   no  \n",
              "4        unknown    5   may       198         1     -1         0  unknown   no  \n",
              "...          ...  ...   ...       ...       ...    ...       ...      ...  ...  \n",
              "45206   cellular   17   nov       977         3     -1         0  unknown  yes  \n",
              "45207   cellular   17   nov       456         2     -1         0  unknown  yes  \n",
              "45208   cellular   17   nov      1127         5    184         3  success  yes  \n",
              "45209  telephone   17   nov       508         4     -1         0  unknown   no  \n",
              "45210   cellular   17   nov       361         2    188        11    other   no  \n",
              "\n",
              "[45211 rows x 17 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0c145ab1-9620-4a1f-a2ea-7cc5153a05fe\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>job</th>\n",
              "      <th>marital</th>\n",
              "      <th>education</th>\n",
              "      <th>default</th>\n",
              "      <th>balance</th>\n",
              "      <th>housing</th>\n",
              "      <th>loan</th>\n",
              "      <th>contact</th>\n",
              "      <th>day</th>\n",
              "      <th>month</th>\n",
              "      <th>duration</th>\n",
              "      <th>campaign</th>\n",
              "      <th>pdays</th>\n",
              "      <th>previous</th>\n",
              "      <th>poutcome</th>\n",
              "      <th>y</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>58</td>\n",
              "      <td>management</td>\n",
              "      <td>married</td>\n",
              "      <td>tertiary</td>\n",
              "      <td>no</td>\n",
              "      <td>2143</td>\n",
              "      <td>yes</td>\n",
              "      <td>no</td>\n",
              "      <td>unknown</td>\n",
              "      <td>5</td>\n",
              "      <td>may</td>\n",
              "      <td>261</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>unknown</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>44</td>\n",
              "      <td>technician</td>\n",
              "      <td>single</td>\n",
              "      <td>secondary</td>\n",
              "      <td>no</td>\n",
              "      <td>29</td>\n",
              "      <td>yes</td>\n",
              "      <td>no</td>\n",
              "      <td>unknown</td>\n",
              "      <td>5</td>\n",
              "      <td>may</td>\n",
              "      <td>151</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>unknown</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>33</td>\n",
              "      <td>entrepreneur</td>\n",
              "      <td>married</td>\n",
              "      <td>secondary</td>\n",
              "      <td>no</td>\n",
              "      <td>2</td>\n",
              "      <td>yes</td>\n",
              "      <td>yes</td>\n",
              "      <td>unknown</td>\n",
              "      <td>5</td>\n",
              "      <td>may</td>\n",
              "      <td>76</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>unknown</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>47</td>\n",
              "      <td>blue-collar</td>\n",
              "      <td>married</td>\n",
              "      <td>unknown</td>\n",
              "      <td>no</td>\n",
              "      <td>1506</td>\n",
              "      <td>yes</td>\n",
              "      <td>no</td>\n",
              "      <td>unknown</td>\n",
              "      <td>5</td>\n",
              "      <td>may</td>\n",
              "      <td>92</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>unknown</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>33</td>\n",
              "      <td>unknown</td>\n",
              "      <td>single</td>\n",
              "      <td>unknown</td>\n",
              "      <td>no</td>\n",
              "      <td>1</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>unknown</td>\n",
              "      <td>5</td>\n",
              "      <td>may</td>\n",
              "      <td>198</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>unknown</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45206</th>\n",
              "      <td>51</td>\n",
              "      <td>technician</td>\n",
              "      <td>married</td>\n",
              "      <td>tertiary</td>\n",
              "      <td>no</td>\n",
              "      <td>825</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>cellular</td>\n",
              "      <td>17</td>\n",
              "      <td>nov</td>\n",
              "      <td>977</td>\n",
              "      <td>3</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>unknown</td>\n",
              "      <td>yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45207</th>\n",
              "      <td>71</td>\n",
              "      <td>retired</td>\n",
              "      <td>divorced</td>\n",
              "      <td>primary</td>\n",
              "      <td>no</td>\n",
              "      <td>1729</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>cellular</td>\n",
              "      <td>17</td>\n",
              "      <td>nov</td>\n",
              "      <td>456</td>\n",
              "      <td>2</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>unknown</td>\n",
              "      <td>yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45208</th>\n",
              "      <td>72</td>\n",
              "      <td>retired</td>\n",
              "      <td>married</td>\n",
              "      <td>secondary</td>\n",
              "      <td>no</td>\n",
              "      <td>5715</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>cellular</td>\n",
              "      <td>17</td>\n",
              "      <td>nov</td>\n",
              "      <td>1127</td>\n",
              "      <td>5</td>\n",
              "      <td>184</td>\n",
              "      <td>3</td>\n",
              "      <td>success</td>\n",
              "      <td>yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45209</th>\n",
              "      <td>57</td>\n",
              "      <td>blue-collar</td>\n",
              "      <td>married</td>\n",
              "      <td>secondary</td>\n",
              "      <td>no</td>\n",
              "      <td>668</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>telephone</td>\n",
              "      <td>17</td>\n",
              "      <td>nov</td>\n",
              "      <td>508</td>\n",
              "      <td>4</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>unknown</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45210</th>\n",
              "      <td>37</td>\n",
              "      <td>entrepreneur</td>\n",
              "      <td>married</td>\n",
              "      <td>secondary</td>\n",
              "      <td>no</td>\n",
              "      <td>2971</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>cellular</td>\n",
              "      <td>17</td>\n",
              "      <td>nov</td>\n",
              "      <td>361</td>\n",
              "      <td>2</td>\n",
              "      <td>188</td>\n",
              "      <td>11</td>\n",
              "      <td>other</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>45211 rows × 17 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0c145ab1-9620-4a1f-a2ea-7cc5153a05fe')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-0c145ab1-9620-4a1f-a2ea-7cc5153a05fe button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-0c145ab1-9620-4a1f-a2ea-7cc5153a05fe');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-bb6e36e2-30ad-4e3c-ae52-881797ec517b\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-bb6e36e2-30ad-4e3c-ae52-881797ec517b')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-bb6e36e2-30ad-4e3c-ae52-881797ec517b button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_7af98220-e0a5-44fa-ac06-f4ca3cf48b4a\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_7af98220-e0a5-44fa-ac06-f4ca3cf48b4a button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 45211,\n  \"fields\": [\n    {\n      \"column\": \"age\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 10,\n        \"min\": 18,\n        \"max\": 95,\n        \"num_unique_values\": 77,\n        \"samples\": [\n          35,\n          34,\n          53\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"job\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 12,\n        \"samples\": [\n          \"housemaid\",\n          \"unemployed\",\n          \"management\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"marital\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"married\",\n          \"single\",\n          \"divorced\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"education\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"secondary\",\n          \"primary\",\n          \"tertiary\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"default\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"yes\",\n          \"no\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"balance\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3044,\n        \"min\": -8019,\n        \"max\": 102127,\n        \"num_unique_values\": 7168,\n        \"samples\": [\n          3276,\n          43\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"housing\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"no\",\n          \"yes\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"loan\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"yes\",\n          \"no\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"contact\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"unknown\",\n          \"cellular\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"day\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 8,\n        \"min\": 1,\n        \"max\": 31,\n        \"num_unique_values\": 31,\n        \"samples\": [\n          1,\n          27\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"month\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 12,\n        \"samples\": [\n          \"apr\",\n          \"mar\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"duration\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 257,\n        \"min\": 0,\n        \"max\": 4918,\n        \"num_unique_values\": 1573,\n        \"samples\": [\n          835,\n          1135\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"campaign\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3,\n        \"min\": 1,\n        \"max\": 63,\n        \"num_unique_values\": 48,\n        \"samples\": [\n          41,\n          27\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"pdays\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 100,\n        \"min\": -1,\n        \"max\": 871,\n        \"num_unique_values\": 559,\n        \"samples\": [\n          249,\n          551\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"previous\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2,\n        \"min\": 0,\n        \"max\": 275,\n        \"num_unique_values\": 41,\n        \"samples\": [\n          17,\n          9\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"poutcome\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"failure\",\n          \"success\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"y\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"yes\",\n          \"no\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.shape)\n",
        "df.info()\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "hm9ZMVW9aJNh",
        "outputId": "dad81b4a-e947-4473-cc0c-91e8cc555b35"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(45211, 17)\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 45211 entries, 0 to 45210\n",
            "Data columns (total 17 columns):\n",
            " #   Column     Non-Null Count  Dtype \n",
            "---  ------     --------------  ----- \n",
            " 0   age        45211 non-null  int64 \n",
            " 1   job        45211 non-null  object\n",
            " 2   marital    45211 non-null  object\n",
            " 3   education  45211 non-null  object\n",
            " 4   default    45211 non-null  object\n",
            " 5   balance    45211 non-null  int64 \n",
            " 6   housing    45211 non-null  object\n",
            " 7   loan       45211 non-null  object\n",
            " 8   contact    45211 non-null  object\n",
            " 9   day        45211 non-null  int64 \n",
            " 10  month      45211 non-null  object\n",
            " 11  duration   45211 non-null  int64 \n",
            " 12  campaign   45211 non-null  int64 \n",
            " 13  pdays      45211 non-null  int64 \n",
            " 14  previous   45211 non-null  int64 \n",
            " 15  poutcome   45211 non-null  object\n",
            " 16  y          45211 non-null  object\n",
            "dtypes: int64(7), object(10)\n",
            "memory usage: 5.9+ MB\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "age          0\n",
              "job          0\n",
              "marital      0\n",
              "education    0\n",
              "default      0\n",
              "balance      0\n",
              "housing      0\n",
              "loan         0\n",
              "contact      0\n",
              "day          0\n",
              "month        0\n",
              "duration     0\n",
              "campaign     0\n",
              "pdays        0\n",
              "previous     0\n",
              "poutcome     0\n",
              "y            0\n",
              "dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>age</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>job</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>marital</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>education</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>default</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>balance</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>housing</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>loan</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>contact</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>day</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>month</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>duration</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>campaign</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pdays</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>previous</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>poutcome</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>y</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#For object check the data\n",
        "for cn in df.columns:\n",
        "  if(df[cn].dtype==object):\n",
        "    print(df[cn].value_counts())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_LD1CE208fhi",
        "outputId": "b3fbe6b0-a320-46b7-e621-26a76f228b96"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "job\n",
            "blue-collar      9732\n",
            "management       9458\n",
            "technician       7597\n",
            "admin.           5171\n",
            "services         4154\n",
            "retired          2264\n",
            "self-employed    1579\n",
            "entrepreneur     1487\n",
            "unemployed       1303\n",
            "housemaid        1240\n",
            "student           938\n",
            "unknown           288\n",
            "Name: count, dtype: int64\n",
            "marital\n",
            "married     27214\n",
            "single      12790\n",
            "divorced     5207\n",
            "Name: count, dtype: int64\n",
            "education\n",
            "secondary    23202\n",
            "tertiary     13301\n",
            "primary       6851\n",
            "unknown       1857\n",
            "Name: count, dtype: int64\n",
            "default\n",
            "no     44396\n",
            "yes      815\n",
            "Name: count, dtype: int64\n",
            "housing\n",
            "yes    25130\n",
            "no     20081\n",
            "Name: count, dtype: int64\n",
            "loan\n",
            "no     37967\n",
            "yes     7244\n",
            "Name: count, dtype: int64\n",
            "contact\n",
            "cellular     29285\n",
            "unknown      13020\n",
            "telephone     2906\n",
            "Name: count, dtype: int64\n",
            "month\n",
            "may    13766\n",
            "jul     6895\n",
            "aug     6247\n",
            "jun     5341\n",
            "nov     3970\n",
            "apr     2932\n",
            "feb     2649\n",
            "jan     1403\n",
            "oct      738\n",
            "sep      579\n",
            "mar      477\n",
            "dec      214\n",
            "Name: count, dtype: int64\n",
            "poutcome\n",
            "unknown    36959\n",
            "failure     4901\n",
            "other       1840\n",
            "success     1511\n",
            "Name: count, dtype: int64\n",
            "y\n",
            "no     39922\n",
            "yes     5289\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Following function converts non-numeric variables (e.g., 'category', 'object') into numeric using label encoding\n",
        "# Note:\n",
        "#Label encoding converts categorical values into integer codes. Each unique category is assigned a distinct number, such as 1, 2, 3, 4, etc.\n",
        "#Example : In our data, education variable has following values:{ 'primary' 'secondary', 'tertiary', 'unknown'}\n",
        "#and it is mapped { 'primary': 0, 'secondary': 1, 'tertiary': 2, 'unknown': 3}\n",
        "\n",
        "#  education  education_encoded\n",
        "#  secondary                  1\n",
        "#   tertiary                  2\n",
        "#    primary                  0\n",
        "#    unknown                  3\n",
        "#   tertiary                  2\n",
        "#    primary                  0\n",
        "#\n",
        "#Label encoding is generally suitable for tree-based models (e.g., decision trees, random forests, boosting methods).\n",
        "#However, it may not be appropriate for models where the objective function relies on distance-based calculations, such as neural networks, support vector machines (SVM), or linear regression.\n",
        "#For nominal features (categories with no intrinsic ordering, e.g., \"red,\" \"blue,\" \"green\"), label encoding can mislead the model by implying an ordinal relationship where none exists.\n",
        "#In such cases, one-hot encoding is usually preferred.\n",
        "#One-hot encoding creates binary columns (dummy variables) for each category in a categorical feature, avoiding the introduction of unintended ordinality.\n",
        "\n",
        "def Encoder(df):\n",
        "          from sklearn import preprocessing\n",
        "          columnsToEncode = list(df.select_dtypes(include=['category','object']))\n",
        "          le = preprocessing.LabelEncoder()\n",
        "          for feature in columnsToEncode:\n",
        "              try:\n",
        "                  df[feature] = le.fit_transform(df[feature])\n",
        "              except:\n",
        "                  print('Error encoding '+feature)\n",
        "          return df\n"
      ],
      "metadata": {
        "id": "JqEV5gKmD6_v"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=Encoder(df)\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "U_XhnTUFBfMj",
        "outputId": "b1664d9f-0b58-48e2-e06b-807763933a7d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       age  job  marital  education  default  balance  housing  loan  contact  \\\n",
              "0       58    4        1          2        0     2143        1     0        2   \n",
              "1       44    9        2          1        0       29        1     0        2   \n",
              "2       33    2        1          1        0        2        1     1        2   \n",
              "3       47    1        1          3        0     1506        1     0        2   \n",
              "4       33   11        2          3        0        1        0     0        2   \n",
              "...    ...  ...      ...        ...      ...      ...      ...   ...      ...   \n",
              "45206   51    9        1          2        0      825        0     0        0   \n",
              "45207   71    5        0          0        0     1729        0     0        0   \n",
              "45208   72    5        1          1        0     5715        0     0        0   \n",
              "45209   57    1        1          1        0      668        0     0        1   \n",
              "45210   37    2        1          1        0     2971        0     0        0   \n",
              "\n",
              "       day  month  duration  campaign  pdays  previous  poutcome  y  \n",
              "0        5      8       261         1     -1         0         3  0  \n",
              "1        5      8       151         1     -1         0         3  0  \n",
              "2        5      8        76         1     -1         0         3  0  \n",
              "3        5      8        92         1     -1         0         3  0  \n",
              "4        5      8       198         1     -1         0         3  0  \n",
              "...    ...    ...       ...       ...    ...       ...       ... ..  \n",
              "45206   17      9       977         3     -1         0         3  1  \n",
              "45207   17      9       456         2     -1         0         3  1  \n",
              "45208   17      9      1127         5    184         3         2  1  \n",
              "45209   17      9       508         4     -1         0         3  0  \n",
              "45210   17      9       361         2    188        11         1  0  \n",
              "\n",
              "[45211 rows x 17 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0ec0377d-9071-4dce-aa43-c4362ac0ef70\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>job</th>\n",
              "      <th>marital</th>\n",
              "      <th>education</th>\n",
              "      <th>default</th>\n",
              "      <th>balance</th>\n",
              "      <th>housing</th>\n",
              "      <th>loan</th>\n",
              "      <th>contact</th>\n",
              "      <th>day</th>\n",
              "      <th>month</th>\n",
              "      <th>duration</th>\n",
              "      <th>campaign</th>\n",
              "      <th>pdays</th>\n",
              "      <th>previous</th>\n",
              "      <th>poutcome</th>\n",
              "      <th>y</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>58</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2143</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "      <td>261</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>44</td>\n",
              "      <td>9</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>29</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "      <td>151</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>33</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "      <td>76</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>47</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>1506</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "      <td>92</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>33</td>\n",
              "      <td>11</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "      <td>198</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45206</th>\n",
              "      <td>51</td>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>825</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>17</td>\n",
              "      <td>9</td>\n",
              "      <td>977</td>\n",
              "      <td>3</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45207</th>\n",
              "      <td>71</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1729</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>17</td>\n",
              "      <td>9</td>\n",
              "      <td>456</td>\n",
              "      <td>2</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45208</th>\n",
              "      <td>72</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>5715</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>17</td>\n",
              "      <td>9</td>\n",
              "      <td>1127</td>\n",
              "      <td>5</td>\n",
              "      <td>184</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45209</th>\n",
              "      <td>57</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>668</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>17</td>\n",
              "      <td>9</td>\n",
              "      <td>508</td>\n",
              "      <td>4</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45210</th>\n",
              "      <td>37</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2971</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>17</td>\n",
              "      <td>9</td>\n",
              "      <td>361</td>\n",
              "      <td>2</td>\n",
              "      <td>188</td>\n",
              "      <td>11</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>45211 rows × 17 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0ec0377d-9071-4dce-aa43-c4362ac0ef70')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-0ec0377d-9071-4dce-aa43-c4362ac0ef70 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-0ec0377d-9071-4dce-aa43-c4362ac0ef70');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-93adb523-eddc-48c0-a5a3-44718f2d2959\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-93adb523-eddc-48c0-a5a3-44718f2d2959')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-93adb523-eddc-48c0-a5a3-44718f2d2959 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_c3320a32-5641-4c64-99ac-90a8c798e48d\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_c3320a32-5641-4c64-99ac-90a8c798e48d button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 45211,\n  \"fields\": [\n    {\n      \"column\": \"age\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 10,\n        \"min\": 18,\n        \"max\": 95,\n        \"num_unique_values\": 77,\n        \"samples\": [\n          35,\n          34,\n          53\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"job\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3,\n        \"min\": 0,\n        \"max\": 11,\n        \"num_unique_values\": 12,\n        \"samples\": [\n          3,\n          10,\n          4\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"marital\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 2,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          1,\n          2,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"education\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 3,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          1,\n          0,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"default\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"balance\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3044,\n        \"min\": -8019,\n        \"max\": 102127,\n        \"num_unique_values\": 7168,\n        \"samples\": [\n          3276,\n          43\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"housing\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"loan\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"contact\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 2,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          2,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"day\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 8,\n        \"min\": 1,\n        \"max\": 31,\n        \"num_unique_values\": 31,\n        \"samples\": [\n          1,\n          27\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"month\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3,\n        \"min\": 0,\n        \"max\": 11,\n        \"num_unique_values\": 12,\n        \"samples\": [\n          0,\n          7\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"duration\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 257,\n        \"min\": 0,\n        \"max\": 4918,\n        \"num_unique_values\": 1573,\n        \"samples\": [\n          835,\n          1135\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"campaign\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3,\n        \"min\": 1,\n        \"max\": 63,\n        \"num_unique_values\": 48,\n        \"samples\": [\n          41,\n          27\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"pdays\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 100,\n        \"min\": -1,\n        \"max\": 871,\n        \"num_unique_values\": 559,\n        \"samples\": [\n          249,\n          551\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"previous\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2,\n        \"min\": 0,\n        \"max\": 275,\n        \"num_unique_values\": 41,\n        \"samples\": [\n          17,\n          9\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"poutcome\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 3,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"y\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y = df['y'] #Output\n",
        "X = df.drop('y',axis=1)\n",
        "X"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "s8C7J0ObyNs5",
        "outputId": "15ec5bce-291d-4efe-dadb-383d06582ff8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       age  job  marital  education  default  balance  housing  loan  contact  \\\n",
              "0       58    4        1          2        0     2143        1     0        2   \n",
              "1       44    9        2          1        0       29        1     0        2   \n",
              "2       33    2        1          1        0        2        1     1        2   \n",
              "3       47    1        1          3        0     1506        1     0        2   \n",
              "4       33   11        2          3        0        1        0     0        2   \n",
              "...    ...  ...      ...        ...      ...      ...      ...   ...      ...   \n",
              "45206   51    9        1          2        0      825        0     0        0   \n",
              "45207   71    5        0          0        0     1729        0     0        0   \n",
              "45208   72    5        1          1        0     5715        0     0        0   \n",
              "45209   57    1        1          1        0      668        0     0        1   \n",
              "45210   37    2        1          1        0     2971        0     0        0   \n",
              "\n",
              "       day  month  duration  campaign  pdays  previous  poutcome  \n",
              "0        5      8       261         1     -1         0         3  \n",
              "1        5      8       151         1     -1         0         3  \n",
              "2        5      8        76         1     -1         0         3  \n",
              "3        5      8        92         1     -1         0         3  \n",
              "4        5      8       198         1     -1         0         3  \n",
              "...    ...    ...       ...       ...    ...       ...       ...  \n",
              "45206   17      9       977         3     -1         0         3  \n",
              "45207   17      9       456         2     -1         0         3  \n",
              "45208   17      9      1127         5    184         3         2  \n",
              "45209   17      9       508         4     -1         0         3  \n",
              "45210   17      9       361         2    188        11         1  \n",
              "\n",
              "[45211 rows x 16 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-65cb10da-6d18-453e-b9f0-9880fdf7c1e4\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>job</th>\n",
              "      <th>marital</th>\n",
              "      <th>education</th>\n",
              "      <th>default</th>\n",
              "      <th>balance</th>\n",
              "      <th>housing</th>\n",
              "      <th>loan</th>\n",
              "      <th>contact</th>\n",
              "      <th>day</th>\n",
              "      <th>month</th>\n",
              "      <th>duration</th>\n",
              "      <th>campaign</th>\n",
              "      <th>pdays</th>\n",
              "      <th>previous</th>\n",
              "      <th>poutcome</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>58</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2143</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "      <td>261</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>44</td>\n",
              "      <td>9</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>29</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "      <td>151</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>33</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "      <td>76</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>47</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>1506</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "      <td>92</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>33</td>\n",
              "      <td>11</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "      <td>198</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45206</th>\n",
              "      <td>51</td>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>825</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>17</td>\n",
              "      <td>9</td>\n",
              "      <td>977</td>\n",
              "      <td>3</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45207</th>\n",
              "      <td>71</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1729</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>17</td>\n",
              "      <td>9</td>\n",
              "      <td>456</td>\n",
              "      <td>2</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45208</th>\n",
              "      <td>72</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>5715</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>17</td>\n",
              "      <td>9</td>\n",
              "      <td>1127</td>\n",
              "      <td>5</td>\n",
              "      <td>184</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45209</th>\n",
              "      <td>57</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>668</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>17</td>\n",
              "      <td>9</td>\n",
              "      <td>508</td>\n",
              "      <td>4</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45210</th>\n",
              "      <td>37</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2971</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>17</td>\n",
              "      <td>9</td>\n",
              "      <td>361</td>\n",
              "      <td>2</td>\n",
              "      <td>188</td>\n",
              "      <td>11</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>45211 rows × 16 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-65cb10da-6d18-453e-b9f0-9880fdf7c1e4')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-65cb10da-6d18-453e-b9f0-9880fdf7c1e4 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-65cb10da-6d18-453e-b9f0-9880fdf7c1e4');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-b93376bc-5a6e-4895-adac-ee04df379dde\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-b93376bc-5a6e-4895-adac-ee04df379dde')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-b93376bc-5a6e-4895-adac-ee04df379dde button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_dfa1f6af-de10-4f98-8ca4-27360e14ae67\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('X')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_dfa1f6af-de10-4f98-8ca4-27360e14ae67 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('X');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "X",
              "summary": "{\n  \"name\": \"X\",\n  \"rows\": 45211,\n  \"fields\": [\n    {\n      \"column\": \"age\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 10,\n        \"min\": 18,\n        \"max\": 95,\n        \"num_unique_values\": 77,\n        \"samples\": [\n          35,\n          34,\n          53\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"job\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3,\n        \"min\": 0,\n        \"max\": 11,\n        \"num_unique_values\": 12,\n        \"samples\": [\n          3,\n          10,\n          4\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"marital\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 2,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          1,\n          2,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"education\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 3,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          1,\n          0,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"default\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"balance\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3044,\n        \"min\": -8019,\n        \"max\": 102127,\n        \"num_unique_values\": 7168,\n        \"samples\": [\n          3276,\n          43\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"housing\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"loan\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"contact\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 2,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          2,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"day\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 8,\n        \"min\": 1,\n        \"max\": 31,\n        \"num_unique_values\": 31,\n        \"samples\": [\n          1,\n          27\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"month\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3,\n        \"min\": 0,\n        \"max\": 11,\n        \"num_unique_values\": 12,\n        \"samples\": [\n          0,\n          7\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"duration\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 257,\n        \"min\": 0,\n        \"max\": 4918,\n        \"num_unique_values\": 1573,\n        \"samples\": [\n          835,\n          1135\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"campaign\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3,\n        \"min\": 1,\n        \"max\": 63,\n        \"num_unique_values\": 48,\n        \"samples\": [\n          41,\n          27\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"pdays\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 100,\n        \"min\": -1,\n        \"max\": 871,\n        \"num_unique_values\": 559,\n        \"samples\": [\n          249,\n          551\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"previous\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2,\n        \"min\": 0,\n        \"max\": 275,\n        \"num_unique_values\": 41,\n        \"samples\": [\n          17,\n          9\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"poutcome\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 3,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import  train_test_split\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=17)"
      ],
      "metadata": {
        "id": "aC4sGAvybIGe"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "Q1)Using  Random Forest,XGBoost, Light GBM and Gradient Boosting Classifier with default parameters (no parameter specifications except random_state) calculate Accuracy on Test data. Which method gives the best accuracy on test data"
      ],
      "metadata": {
        "id": "VbZLvi_k2kjt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Random Forest\n",
        "rf = RandomForestClassifier(random_state=17)\n",
        "rf.fit(X_train, y_train)\n",
        "rf_pred = rf.predict(X_test)\n",
        "rf_acc = accuracy_score(y_test, rf_pred)\n",
        "\n",
        "# Gradient Boosting\n",
        "gb = GradientBoostingClassifier(random_state=17)\n",
        "gb.fit(X_train, y_train)\n",
        "gb_pred = gb.predict(X_test)\n",
        "gb_acc = accuracy_score(y_test, gb_pred)\n",
        "\n",
        "# XGBoost\n",
        "from xgboost import XGBClassifier\n",
        "xgb = XGBClassifier(random_state=17, use_label_encoder=False, eval_metric='logloss')\n",
        "xgb.fit(X_train, y_train)\n",
        "xgb_pred = xgb.predict(X_test)\n",
        "xgb_acc = accuracy_score(y_test, xgb_pred)\n",
        "\n",
        "# LightGBM\n",
        "from lightgbm import LGBMClassifier\n",
        "lgb = LGBMClassifier(random_state=17)\n",
        "lgb.fit(X_train, y_train)\n",
        "lgb_pred = lgb.predict(X_test)\n",
        "lgb_acc = accuracy_score(y_test, lgb_pred)\n",
        "\n",
        "# Print all accuracies\n",
        "print(\"Random Forest Accuracy:\", rf_acc)\n",
        "print(\"Gradient Boosting Accuracy:\", gb_acc)\n",
        "print(\"XGBoost Accuracy:\", xgb_acc)\n",
        "print(\"LightGBM Accuracy:\", lgb_acc)\n",
        "\n",
        "# Best model\n",
        "best_model = max(\n",
        "    [('Random Forest', rf_acc), ('Gradient Boosting', gb_acc),\n",
        "     ('XGBoost', xgb_acc), ('LightGBM', lgb_acc)],\n",
        "    key=lambda x: x[1]\n",
        ")\n",
        "print(f\"\\n✅ Best performing model: {best_model[0]} with accuracy: {best_model[1]:.4f}\")\n"
      ],
      "metadata": {
        "id": "6tnO_YSnkcwL",
        "outputId": "1acb0e1a-bdee-4d46-e595-53fdc510c25f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [09:54:15] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 3940, number of negative: 29968\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004556 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 988\n",
            "[LightGBM] [Info] Number of data points in the train set: 33908, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116197 -> initscore=-2.028949\n",
            "[LightGBM] [Info] Start training from score -2.028949\n",
            "Random Forest Accuracy: 0.9025037600636999\n",
            "Gradient Boosting Accuracy: 0.9018844554543042\n",
            "XGBoost Accuracy: 0.9036538971954349\n",
            "LightGBM Accuracy: 0.9073697248518092\n",
            "\n",
            "✅ Best performing model: LightGBM with accuracy: 0.9074\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q1**: Best model is LightGBM, as it achieved the highest - 90.74% test accuracy."
      ],
      "metadata": {
        "id": "zEIUUn3OkmZ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2) Using optuna hyperparmeter optimization technique and 100 trial\n",
        "\n",
        " a)find best methods with  parameters  using Cross validation (CV=3) technique for the range of   parameters below. What are the best parameters for the method with highest cross validation accuracy?\n",
        " For random forest\n",
        "\n",
        "\n",
        "  \"max_depth\"   : trial.suggest_int(\"max_depth\", 2,  X_train.shape[1]),\n",
        "  \"max_features\": trial.suggest_int(\"max_features\", 2, X_train.shape[1])\n",
        "\n",
        "For XGBoost, Light GBM and Gradient Boosting Classifier\n",
        "\n",
        "  \"max_depth\": trial.suggest_int(\"max_depth\", 2, X_train.shape[1]),\n",
        "  \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.001, 0.3,log=True)\n",
        "\n",
        "where X_train.shape[1] is number of columnns in the train data.\n",
        "\n",
        " b)Evaluate the performance of the  method with highest cross validation accuracy on test data.What is the accuracy value? Are there any improvement of the same method with default parameters?\n"
      ],
      "metadata": {
        "id": "TK77k75qO7dW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna --quiet"
      ],
      "metadata": {
        "id": "Q1FO5AHzmEzT",
        "outputId": "51635f7e-7102-4a3d-dfed-daccdcaf1d6b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/383.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m383.6/383.6 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/231.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m231.9/231.9 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q2 a)\n",
        "#importing libraries\n",
        "\n",
        "import optuna\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "A0CRnF9UmKIO"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Random forest\n",
        "def rf_objective(trial):\n",
        "    max_depth = trial.suggest_int(\"max_depth\", 2, X_train.shape[1])\n",
        "    max_features = trial.suggest_int(\"max_features\", 2, X_train.shape[1])\n",
        "\n",
        "    model = RandomForestClassifier(\n",
        "        max_depth=max_depth,\n",
        "        max_features=max_features,\n",
        "        random_state=17\n",
        "    )\n",
        "\n",
        "    score = cross_val_score(model, X_train, y_train, cv=3, scoring='accuracy')\n",
        "    return score.mean()"
      ],
      "metadata": {
        "id": "PN65mNXwmT4A"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Gradient Boosting\n",
        "def gb_objective(trial):\n",
        "    max_depth = trial.suggest_int(\"max_depth\", 2, X_train.shape[1])\n",
        "    learning_rate = trial.suggest_float(\"learning_rate\", 0.001, 0.3, log=True)\n",
        "\n",
        "    model = GradientBoostingClassifier(\n",
        "        max_depth=max_depth,\n",
        "        learning_rate=learning_rate,\n",
        "        random_state=17\n",
        "    )\n",
        "\n",
        "    score = cross_val_score(model, X_train, y_train, cv=3, scoring='accuracy')\n",
        "    return score.mean()\n"
      ],
      "metadata": {
        "id": "721WyWHomWPr"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Xgboost\n",
        "def xgb_objective(trial):\n",
        "    max_depth = trial.suggest_int(\"max_depth\", 2, X_train.shape[1])\n",
        "    learning_rate = trial.suggest_float(\"learning_rate\", 0.001, 0.3, log=True)\n",
        "\n",
        "    model = XGBClassifier(\n",
        "        max_depth=max_depth,\n",
        "        learning_rate=learning_rate,\n",
        "        use_label_encoder=False,\n",
        "        eval_metric='logloss',\n",
        "        random_state=17\n",
        "    )\n",
        "\n",
        "    score = cross_val_score(model, X_train, y_train, cv=3, scoring='accuracy')\n",
        "    return score.mean()"
      ],
      "metadata": {
        "id": "yuqhD3iCmaub"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#LightGBM\n",
        "def lgb_objective(trial):\n",
        "    max_depth = trial.suggest_int(\"max_depth\", 2, X_train.shape[1])\n",
        "    learning_rate = trial.suggest_float(\"learning_rate\", 0.001, 0.3, log=True)\n",
        "\n",
        "    model = LGBMClassifier(\n",
        "        max_depth=max_depth,\n",
        "        learning_rate=learning_rate,\n",
        "        random_state=17\n",
        "    )\n",
        "\n",
        "    score = cross_val_score(model, X_train, y_train, cv=3, scoring='accuracy')\n",
        "    return score.mean()"
      ],
      "metadata": {
        "id": "dxNuDbR1mc-6"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#running optuna\n",
        "study_rf = optuna.create_study(direction=\"maximize\")\n",
        "study_rf.optimize(rf_objective, n_trials=100)\n",
        "\n",
        "study_gb = optuna.create_study(direction=\"maximize\")\n",
        "study_gb.optimize(gb_objective, n_trials=100)\n",
        "\n",
        "study_xgb = optuna.create_study(direction=\"maximize\")\n",
        "study_xgb.optimize(xgb_objective, n_trials=100)\n",
        "\n",
        "study_lgb = optuna.create_study(direction=\"maximize\")\n",
        "study_lgb.optimize(lgb_objective, n_trials=100)"
      ],
      "metadata": {
        "id": "JyZ1-yromiQS",
        "outputId": "5c53b616-fe8d-48ef-dca8-c81bb419941b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-13 10:03:07,027] A new study created in memory with name: no-name-a0f5c727-b597-4595-a869-03ccfa8b79db\n",
            "[I 2025-04-13 10:03:11,488] Trial 0 finished with value: 0.8939778360748277 and parameters: {'max_depth': 6, 'max_features': 3}. Best is trial 0 with value: 0.8939778360748277.\n",
            "[I 2025-04-13 10:03:35,554] Trial 1 finished with value: 0.9044178450302717 and parameters: {'max_depth': 9, 'max_features': 14}. Best is trial 1 with value: 0.9044178450302717.\n",
            "[I 2025-04-13 10:04:00,056] Trial 2 finished with value: 0.9047717960025198 and parameters: {'max_depth': 13, 'max_features': 10}. Best is trial 2 with value: 0.9047717960025198.\n",
            "[I 2025-04-13 10:04:11,527] Trial 3 finished with value: 0.9044768629523222 and parameters: {'max_depth': 8, 'max_features': 9}. Best is trial 2 with value: 0.9047717960025198.\n",
            "[I 2025-04-13 10:04:31,054] Trial 4 finished with value: 0.9044178450302717 and parameters: {'max_depth': 9, 'max_features': 14}. Best is trial 2 with value: 0.9047717960025198.\n",
            "[I 2025-04-13 10:04:54,807] Trial 5 finished with value: 0.9034741271123298 and parameters: {'max_depth': 14, 'max_features': 12}. Best is trial 2 with value: 0.9047717960025198.\n",
            "[I 2025-04-13 10:05:06,299] Trial 6 finished with value: 0.9049192677462871 and parameters: {'max_depth': 9, 'max_features': 8}. Best is trial 6 with value: 0.9049192677462871.\n",
            "[I 2025-04-13 10:05:27,100] Trial 7 finished with value: 0.904152449643776 and parameters: {'max_depth': 15, 'max_features': 10}. Best is trial 6 with value: 0.9049192677462871.\n",
            "[I 2025-04-13 10:05:34,343] Trial 8 finished with value: 0.9030612912940779 and parameters: {'max_depth': 9, 'max_features': 4}. Best is trial 6 with value: 0.9049192677462871.\n",
            "[I 2025-04-13 10:05:38,166] Trial 9 finished with value: 0.8939778360748277 and parameters: {'max_depth': 6, 'max_features': 3}. Best is trial 6 with value: 0.9049192677462871.\n",
            "[I 2025-04-13 10:05:41,204] Trial 10 finished with value: 0.8839801849453323 and parameters: {'max_depth': 2, 'max_features': 7}. Best is trial 6 with value: 0.9049192677462871.\n",
            "[I 2025-04-13 10:05:54,432] Trial 11 finished with value: 0.9048897405199221 and parameters: {'max_depth': 12, 'max_features': 7}. Best is trial 6 with value: 0.9049192677462871.\n",
            "[I 2025-04-13 10:06:11,767] Trial 12 finished with value: 0.9057155347951357 and parameters: {'max_depth': 12, 'max_features': 6}. Best is trial 12 with value: 0.9057155347951357.\n",
            "[I 2025-04-13 10:06:29,846] Trial 13 finished with value: 0.905302638962196 and parameters: {'max_depth': 11, 'max_features': 6}. Best is trial 12 with value: 0.9057155347951357.\n",
            "[I 2025-04-13 10:06:40,733] Trial 14 finished with value: 0.905862985664229 and parameters: {'max_depth': 12, 'max_features': 5}. Best is trial 14 with value: 0.905862985664229.\n",
            "[I 2025-04-13 10:06:55,959] Trial 15 finished with value: 0.9053321087832075 and parameters: {'max_depth': 16, 'max_features': 5}. Best is trial 14 with value: 0.905862985664229.\n",
            "[I 2025-04-13 10:07:04,229] Trial 16 finished with value: 0.898224618892251 and parameters: {'max_depth': 11, 'max_features': 2}. Best is trial 14 with value: 0.905862985664229.\n",
            "[I 2025-04-13 10:07:14,477] Trial 17 finished with value: 0.905862985664229 and parameters: {'max_depth': 12, 'max_features': 5}. Best is trial 14 with value: 0.905862985664229.\n",
            "[I 2025-04-13 10:07:29,202] Trial 18 finished with value: 0.905538574965017 and parameters: {'max_depth': 14, 'max_features': 5}. Best is trial 14 with value: 0.905862985664229.\n",
            "[I 2025-04-13 10:07:48,363] Trial 19 finished with value: 0.9031497320691234 and parameters: {'max_depth': 6, 'max_features': 16}. Best is trial 14 with value: 0.905862985664229.\n",
            "[I 2025-04-13 10:07:50,939] Trial 20 finished with value: 0.8838032329432165 and parameters: {'max_depth': 2, 'max_features': 2}. Best is trial 14 with value: 0.905862985664229.\n",
            "[I 2025-04-13 10:08:02,788] Trial 21 finished with value: 0.905862985664229 and parameters: {'max_depth': 12, 'max_features': 5}. Best is trial 14 with value: 0.905862985664229.\n",
            "[I 2025-04-13 10:08:16,222] Trial 22 finished with value: 0.9046538071264353 and parameters: {'max_depth': 11, 'max_features': 4}. Best is trial 14 with value: 0.905862985664229.\n",
            "[I 2025-04-13 10:08:29,812] Trial 23 finished with value: 0.9056565534037645 and parameters: {'max_depth': 13, 'max_features': 5}. Best is trial 14 with value: 0.905862985664229.\n",
            "[I 2025-04-13 10:08:52,018] Trial 24 finished with value: 0.9046538097357696 and parameters: {'max_depth': 16, 'max_features': 7}. Best is trial 14 with value: 0.905862985664229.\n",
            "[I 2025-04-13 10:09:02,641] Trial 25 finished with value: 0.9043883882559314 and parameters: {'max_depth': 12, 'max_features': 4}. Best is trial 14 with value: 0.905862985664229.\n",
            "[I 2025-04-13 10:09:20,405] Trial 26 finished with value: 0.9052731587038475 and parameters: {'max_depth': 10, 'max_features': 8}. Best is trial 14 with value: 0.905862985664229.\n",
            "[I 2025-04-13 10:09:41,466] Trial 27 finished with value: 0.9053911110492526 and parameters: {'max_depth': 14, 'max_features': 6}. Best is trial 14 with value: 0.905862985664229.\n",
            "[I 2025-04-13 10:09:46,968] Trial 28 finished with value: 0.8955998556495418 and parameters: {'max_depth': 7, 'max_features': 3}. Best is trial 14 with value: 0.905862985664229.\n",
            "[I 2025-04-13 10:09:54,756] Trial 29 finished with value: 0.903533142425046 and parameters: {'max_depth': 13, 'max_features': 3}. Best is trial 14 with value: 0.905862985664229.\n",
            "[I 2025-04-13 10:10:12,881] Trial 30 finished with value: 0.9042704098171838 and parameters: {'max_depth': 15, 'max_features': 9}. Best is trial 14 with value: 0.905862985664229.\n",
            "[I 2025-04-13 10:10:24,545] Trial 31 finished with value: 0.9057155347951357 and parameters: {'max_depth': 12, 'max_features': 6}. Best is trial 14 with value: 0.905862985664229.\n",
            "[I 2025-04-13 10:10:33,495] Trial 32 finished with value: 0.9049487480046355 and parameters: {'max_depth': 10, 'max_features': 5}. Best is trial 14 with value: 0.905862985664229.\n",
            "[I 2025-04-13 10:10:45,251] Trial 33 finished with value: 0.9057155347951357 and parameters: {'max_depth': 12, 'max_features': 6}. Best is trial 14 with value: 0.905862985664229.\n",
            "[I 2025-04-13 10:10:58,035] Trial 34 finished with value: 0.9052731587038475 and parameters: {'max_depth': 10, 'max_features': 8}. Best is trial 14 with value: 0.905862985664229.\n",
            "[I 2025-04-13 10:11:07,338] Trial 35 finished with value: 0.9052141277351257 and parameters: {'max_depth': 13, 'max_features': 4}. Best is trial 14 with value: 0.905862985664229.\n",
            "[I 2025-04-13 10:11:25,305] Trial 36 finished with value: 0.9048307043325319 and parameters: {'max_depth': 11, 'max_features': 11}. Best is trial 14 with value: 0.905862985664229.\n",
            "[I 2025-04-13 10:11:32,900] Trial 37 finished with value: 0.9032381963281773 and parameters: {'max_depth': 8, 'max_features': 5}. Best is trial 14 with value: 0.905862985664229.\n",
            "[I 2025-04-13 10:11:47,846] Trial 38 finished with value: 0.9051256791320773 and parameters: {'max_depth': 15, 'max_features': 7}. Best is trial 14 with value: 0.905862985664229.\n",
            "[I 2025-04-13 10:11:55,568] Trial 39 finished with value: 0.903533142425046 and parameters: {'max_depth': 13, 'max_features': 3}. Best is trial 14 with value: 0.905862985664229.\n",
            "[I 2025-04-13 10:12:13,420] Trial 40 finished with value: 0.9053320879085335 and parameters: {'max_depth': 14, 'max_features': 9}. Best is trial 14 with value: 0.905862985664229.\n",
            "[I 2025-04-13 10:12:25,179] Trial 41 finished with value: 0.9057155347951357 and parameters: {'max_depth': 12, 'max_features': 6}. Best is trial 14 with value: 0.905862985664229.\n",
            "[I 2025-04-13 10:12:35,471] Trial 42 finished with value: 0.9048012840888711 and parameters: {'max_depth': 10, 'max_features': 6}. Best is trial 14 with value: 0.905862985664229.\n",
            "[I 2025-04-13 10:12:43,512] Trial 43 finished with value: 0.9043883882559314 and parameters: {'max_depth': 12, 'max_features': 4}. Best is trial 14 with value: 0.905862985664229.\n",
            "[I 2025-04-13 10:12:55,869] Trial 44 finished with value: 0.9050961962643947 and parameters: {'max_depth': 11, 'max_features': 7}. Best is trial 14 with value: 0.905862985664229.\n",
            "[I 2025-04-13 10:13:12,145] Trial 45 finished with value: 0.9047422948694973 and parameters: {'max_depth': 13, 'max_features': 8}. Best is trial 14 with value: 0.905862985664229.\n",
            "[I 2025-04-13 10:13:21,656] Trial 46 finished with value: 0.905862985664229 and parameters: {'max_depth': 12, 'max_features': 5}. Best is trial 14 with value: 0.905862985664229.\n",
            "[I 2025-04-13 10:13:39,735] Trial 47 finished with value: 0.9048897170359139 and parameters: {'max_depth': 9, 'max_features': 13}. Best is trial 14 with value: 0.905862985664229.\n",
            "[I 2025-04-13 10:13:45,523] Trial 48 finished with value: 0.9015571353474043 and parameters: {'max_depth': 14, 'max_features': 2}. Best is trial 14 with value: 0.905862985664229.\n",
            "[I 2025-04-13 10:13:52,045] Trial 49 finished with value: 0.9024124751143227 and parameters: {'max_depth': 8, 'max_features': 4}. Best is trial 14 with value: 0.905862985664229.\n",
            "[I 2025-04-13 10:14:01,648] Trial 50 finished with value: 0.9054205756515955 and parameters: {'max_depth': 11, 'max_features': 5}. Best is trial 14 with value: 0.905862985664229.\n",
            "[I 2025-04-13 10:14:12,990] Trial 51 finished with value: 0.9057155347951357 and parameters: {'max_depth': 12, 'max_features': 6}. Best is trial 14 with value: 0.905862985664229.\n",
            "[I 2025-04-13 10:14:22,904] Trial 52 finished with value: 0.905862985664229 and parameters: {'max_depth': 12, 'max_features': 5}. Best is trial 14 with value: 0.905862985664229.\n",
            "[I 2025-04-13 10:14:33,740] Trial 53 finished with value: 0.9056565534037645 and parameters: {'max_depth': 13, 'max_features': 5}. Best is trial 14 with value: 0.905862985664229.\n",
            "[I 2025-04-13 10:14:36,524] Trial 54 finished with value: 0.885277799039503 and parameters: {'max_depth': 3, 'max_features': 4}. Best is trial 14 with value: 0.905862985664229.\n",
            "[I 2025-04-13 10:14:45,540] Trial 55 finished with value: 0.9049487480046355 and parameters: {'max_depth': 10, 'max_features': 5}. Best is trial 14 with value: 0.905862985664229.\n",
            "[I 2025-04-13 10:14:52,496] Trial 56 finished with value: 0.9020585502354169 and parameters: {'max_depth': 11, 'max_features': 3}. Best is trial 14 with value: 0.905862985664229.\n",
            "[I 2025-04-13 10:15:03,816] Trial 57 finished with value: 0.9056269896467201 and parameters: {'max_depth': 15, 'max_features': 5}. Best is trial 14 with value: 0.905862985664229.\n",
            "[I 2025-04-13 10:15:23,228] Trial 58 finished with value: 0.9060104365333222 and parameters: {'max_depth': 14, 'max_features': 10}. Best is trial 58 with value: 0.9060104365333222.\n",
            "[I 2025-04-13 10:15:43,182] Trial 59 finished with value: 0.9060104365333222 and parameters: {'max_depth': 14, 'max_features': 10}. Best is trial 58 with value: 0.9060104365333222.\n",
            "[I 2025-04-13 10:16:05,303] Trial 60 finished with value: 0.9058629595708864 and parameters: {'max_depth': 16, 'max_features': 11}. Best is trial 58 with value: 0.9060104365333222.\n",
            "[I 2025-04-13 10:16:23,476] Trial 61 finished with value: 0.9053320879085335 and parameters: {'max_depth': 14, 'max_features': 9}. Best is trial 58 with value: 0.9060104365333222.\n",
            "[I 2025-04-13 10:16:43,693] Trial 62 finished with value: 0.904152449643776 and parameters: {'max_depth': 15, 'max_features': 10}. Best is trial 58 with value: 0.9060104365333222.\n",
            "[I 2025-04-13 10:17:04,096] Trial 63 finished with value: 0.9052731378291735 and parameters: {'max_depth': 13, 'max_features': 11}. Best is trial 58 with value: 0.9060104365333222.\n",
            "[I 2025-04-13 10:17:24,076] Trial 64 finished with value: 0.9060104365333222 and parameters: {'max_depth': 14, 'max_features': 10}. Best is trial 58 with value: 0.9060104365333222.\n",
            "[I 2025-04-13 10:17:43,276] Trial 65 finished with value: 0.9060104365333222 and parameters: {'max_depth': 14, 'max_features': 10}. Best is trial 58 with value: 0.9060104365333222.\n",
            "[I 2025-04-13 10:18:03,162] Trial 66 finished with value: 0.9060104365333222 and parameters: {'max_depth': 14, 'max_features': 10}. Best is trial 58 with value: 0.9060104365333222.\n",
            "[I 2025-04-13 10:18:23,119] Trial 67 finished with value: 0.904152449643776 and parameters: {'max_depth': 15, 'max_features': 10}. Best is trial 58 with value: 0.9060104365333222.\n",
            "[I 2025-04-13 10:18:46,212] Trial 68 finished with value: 0.9034741271123298 and parameters: {'max_depth': 14, 'max_features': 12}. Best is trial 58 with value: 0.9060104365333222.\n",
            "[I 2025-04-13 10:19:07,235] Trial 69 finished with value: 0.9045948100790587 and parameters: {'max_depth': 16, 'max_features': 10}. Best is trial 58 with value: 0.9060104365333222.\n",
            "[I 2025-04-13 10:19:30,428] Trial 70 finished with value: 0.9034741271123298 and parameters: {'max_depth': 14, 'max_features': 12}. Best is trial 58 with value: 0.9060104365333222.\n",
            "[I 2025-04-13 10:19:51,431] Trial 71 finished with value: 0.9039165292969605 and parameters: {'max_depth': 14, 'max_features': 11}. Best is trial 58 with value: 0.9060104365333222.\n",
            "[I 2025-04-13 10:20:10,478] Trial 72 finished with value: 0.9047717960025198 and parameters: {'max_depth': 13, 'max_features': 10}. Best is trial 58 with value: 0.9060104365333222.\n",
            "[I 2025-04-13 10:20:28,691] Trial 73 finished with value: 0.9042704098171838 and parameters: {'max_depth': 15, 'max_features': 9}. Best is trial 58 with value: 0.9060104365333222.\n",
            "[I 2025-04-13 10:20:47,875] Trial 74 finished with value: 0.9047717960025198 and parameters: {'max_depth': 13, 'max_features': 10}. Best is trial 58 with value: 0.9060104365333222.\n",
            "[I 2025-04-13 10:20:55,509] Trial 75 finished with value: 0.9015572110180976 and parameters: {'max_depth': 5, 'max_features': 9}. Best is trial 58 with value: 0.9060104365333222.\n",
            "[I 2025-04-13 10:21:17,674] Trial 76 finished with value: 0.9058629595708864 and parameters: {'max_depth': 16, 'max_features': 11}. Best is trial 58 with value: 0.9060104365333222.\n",
            "[I 2025-04-13 10:21:41,845] Trial 77 finished with value: 0.9036511208637936 and parameters: {'max_depth': 15, 'max_features': 12}. Best is trial 58 with value: 0.9060104365333222.\n",
            "[I 2025-04-13 10:21:58,617] Trial 78 finished with value: 0.9053026572275357 and parameters: {'max_depth': 14, 'max_features': 8}. Best is trial 58 with value: 0.9060104365333222.\n",
            "[I 2025-04-13 10:22:17,134] Trial 79 finished with value: 0.9047717960025198 and parameters: {'max_depth': 13, 'max_features': 10}. Best is trial 58 with value: 0.9060104365333222.\n",
            "[I 2025-04-13 10:22:38,703] Trial 80 finished with value: 0.9039165292969605 and parameters: {'max_depth': 14, 'max_features': 11}. Best is trial 58 with value: 0.9060104365333222.\n",
            "[I 2025-04-13 10:22:55,403] Trial 81 finished with value: 0.9050076980839957 and parameters: {'max_depth': 13, 'max_features': 9}. Best is trial 58 with value: 0.9060104365333222.\n",
            "[I 2025-04-13 10:23:21,047] Trial 82 finished with value: 0.9046538488757833 and parameters: {'max_depth': 12, 'max_features': 15}. Best is trial 58 with value: 0.9060104365333222.\n",
            "[I 2025-04-13 10:23:34,829] Trial 83 finished with value: 0.9050372044356866 and parameters: {'max_depth': 11, 'max_features': 8}. Best is trial 58 with value: 0.9060104365333222.\n",
            "[I 2025-04-13 10:23:54,745] Trial 84 finished with value: 0.9060104365333222 and parameters: {'max_depth': 14, 'max_features': 10}. Best is trial 58 with value: 0.9060104365333222.\n",
            "[I 2025-04-13 10:24:14,664] Trial 85 finished with value: 0.904152449643776 and parameters: {'max_depth': 15, 'max_features': 10}. Best is trial 58 with value: 0.9060104365333222.\n",
            "[I 2025-04-13 10:24:39,266] Trial 86 finished with value: 0.903444654681984 and parameters: {'max_depth': 14, 'max_features': 13}. Best is trial 58 with value: 0.9060104365333222.\n",
            "[I 2025-04-13 10:24:59,105] Trial 87 finished with value: 0.9060104365333222 and parameters: {'max_depth': 14, 'max_features': 10}. Best is trial 58 with value: 0.9060104365333222.\n",
            "[I 2025-04-13 10:25:18,831] Trial 88 finished with value: 0.9060104365333222 and parameters: {'max_depth': 14, 'max_features': 10}. Best is trial 58 with value: 0.9060104365333222.\n",
            "[I 2025-04-13 10:25:38,052] Trial 89 finished with value: 0.9060104365333222 and parameters: {'max_depth': 14, 'max_features': 10}. Best is trial 58 with value: 0.9060104365333222.\n",
            "[I 2025-04-13 10:25:57,840] Trial 90 finished with value: 0.9060104365333222 and parameters: {'max_depth': 14, 'max_features': 10}. Best is trial 58 with value: 0.9060104365333222.\n",
            "[I 2025-04-13 10:26:17,008] Trial 91 finished with value: 0.9060104365333222 and parameters: {'max_depth': 14, 'max_features': 10}. Best is trial 58 with value: 0.9060104365333222.\n",
            "[I 2025-04-13 10:26:39,238] Trial 92 finished with value: 0.9053615942602246 and parameters: {'max_depth': 15, 'max_features': 11}. Best is trial 58 with value: 0.9060104365333222.\n",
            "[I 2025-04-13 10:26:58,902] Trial 93 finished with value: 0.9060104365333222 and parameters: {'max_depth': 14, 'max_features': 10}. Best is trial 58 with value: 0.9060104365333222.\n",
            "[I 2025-04-13 10:27:17,059] Trial 94 finished with value: 0.9042704098171838 and parameters: {'max_depth': 15, 'max_features': 9}. Best is trial 58 with value: 0.9060104365333222.\n",
            "[I 2025-04-13 10:27:34,408] Trial 95 finished with value: 0.9053320879085335 and parameters: {'max_depth': 14, 'max_features': 9}. Best is trial 58 with value: 0.9060104365333222.\n",
            "[I 2025-04-13 10:27:56,474] Trial 96 finished with value: 0.9053615942602246 and parameters: {'max_depth': 15, 'max_features': 11}. Best is trial 58 with value: 0.9060104365333222.\n",
            "[I 2025-04-13 10:28:16,429] Trial 97 finished with value: 0.9060104365333222 and parameters: {'max_depth': 14, 'max_features': 10}. Best is trial 58 with value: 0.9060104365333222.\n",
            "[I 2025-04-13 10:28:40,557] Trial 98 finished with value: 0.9039755106883315 and parameters: {'max_depth': 16, 'max_features': 12}. Best is trial 58 with value: 0.9060104365333222.\n",
            "[I 2025-04-13 10:28:58,778] Trial 99 finished with value: 0.9047717960025198 and parameters: {'max_depth': 13, 'max_features': 10}. Best is trial 58 with value: 0.9060104365333222.\n",
            "[I 2025-04-13 10:28:58,779] A new study created in memory with name: no-name-f3ccec91-2620-455f-996d-3f1654f51b72\n",
            "[I 2025-04-13 10:29:06,592] Trial 0 finished with value: 0.9042114753938293 and parameters: {'max_depth': 2, 'learning_rate': 0.23844537630129373}. Best is trial 0 with value: 0.9042114753938293.\n",
            "[I 2025-04-13 10:30:00,229] Trial 1 finished with value: 0.897959166100402 and parameters: {'max_depth': 11, 'learning_rate': 0.022653493856121733}. Best is trial 0 with value: 0.9042114753938293.\n",
            "[I 2025-04-13 10:30:08,010] Trial 2 finished with value: 0.8838032329432165 and parameters: {'max_depth': 2, 'learning_rate': 0.003283502743792402}. Best is trial 0 with value: 0.9042114753938293.\n",
            "[I 2025-04-13 10:30:15,091] Trial 3 finished with value: 0.8838032329432165 and parameters: {'max_depth': 2, 'learning_rate': 0.001398855419887777}. Best is trial 0 with value: 0.9042114753938293.\n",
            "[I 2025-04-13 10:31:33,510] Trial 4 finished with value: 0.8838032329432165 and parameters: {'max_depth': 15, 'learning_rate': 0.002650896118184272}. Best is trial 0 with value: 0.9042114753938293.\n",
            "[I 2025-04-13 10:33:30,198] Trial 5 finished with value: 0.8933879517090927 and parameters: {'max_depth': 16, 'learning_rate': 0.05808270462782109}. Best is trial 0 with value: 0.9042114753938293.\n",
            "[I 2025-04-13 10:33:52,196] Trial 6 finished with value: 0.8838032329432165 and parameters: {'max_depth': 6, 'learning_rate': 0.005261643672284331}. Best is trial 0 with value: 0.9042114753938293.\n",
            "[I 2025-04-13 10:34:06,520] Trial 7 finished with value: 0.9032382511241964 and parameters: {'max_depth': 4, 'learning_rate': 0.03361573124743349}. Best is trial 0 with value: 0.9042114753938293.\n",
            "[I 2025-04-13 10:34:55,471] Trial 8 finished with value: 0.8999941545694147 and parameters: {'max_depth': 11, 'learning_rate': 0.1604679613093328}. Best is trial 0 with value: 0.9042114753938293.\n",
            "[I 2025-04-13 10:35:09,724] Trial 9 finished with value: 0.8838032329432165 and parameters: {'max_depth': 4, 'learning_rate': 0.005110963371757596}. Best is trial 0 with value: 0.9042114753938293.\n",
            "[I 2025-04-13 10:35:35,519] Trial 10 finished with value: 0.8998761291626507 and parameters: {'max_depth': 7, 'learning_rate': 0.28247636577577534}. Best is trial 0 with value: 0.9042114753938293.\n",
            "[I 2025-04-13 10:35:53,738] Trial 11 finished with value: 0.9049191999035965 and parameters: {'max_depth': 5, 'learning_rate': 0.05216058410494823}. Best is trial 11 with value: 0.9049191999035965.\n",
            "[I 2025-04-13 10:36:19,459] Trial 12 finished with value: 0.904624308602747 and parameters: {'max_depth': 7, 'learning_rate': 0.0989399563950161}. Best is trial 11 with value: 0.9049191999035965.\n",
            "[I 2025-04-13 10:36:50,694] Trial 13 finished with value: 0.9036510817237798 and parameters: {'max_depth': 8, 'learning_rate': 0.06692240415654835}. Best is trial 11 with value: 0.9049191999035965.\n",
            "[I 2025-04-13 10:37:32,796] Trial 14 finished with value: 0.9022059958858417 and parameters: {'max_depth': 10, 'learning_rate': 0.09789266363509952}. Best is trial 11 with value: 0.9049191999035965.\n",
            "[I 2025-04-13 10:37:50,867] Trial 15 finished with value: 0.8956884268912999 and parameters: {'max_depth': 5, 'learning_rate': 0.013709560787090812}. Best is trial 11 with value: 0.9049191999035965.\n",
            "[I 2025-04-13 10:39:05,940] Trial 16 finished with value: 0.8965435579114782 and parameters: {'max_depth': 13, 'learning_rate': 0.015912326071550992}. Best is trial 11 with value: 0.9049191999035965.\n",
            "[I 2025-04-13 10:39:36,884] Trial 17 finished with value: 0.9023534337082638 and parameters: {'max_depth': 8, 'learning_rate': 0.03875617429439293}. Best is trial 11 with value: 0.9049191999035965.\n",
            "[I 2025-04-13 10:39:55,059] Trial 18 finished with value: 0.9056860153967733 and parameters: {'max_depth': 5, 'learning_rate': 0.12061317213615748}. Best is trial 18 with value: 0.9056860153967733.\n",
            "[I 2025-04-13 10:40:09,247] Trial 19 finished with value: 0.9058039990541894 and parameters: {'max_depth': 4, 'learning_rate': 0.16166063445952442}. Best is trial 19 with value: 0.9058039990541894.\n",
            "[I 2025-04-13 10:40:23,407] Trial 20 finished with value: 0.9056270757547503 and parameters: {'max_depth': 4, 'learning_rate': 0.14325454304026894}. Best is trial 19 with value: 0.9058039990541894.\n",
            "[I 2025-04-13 10:40:37,590] Trial 21 finished with value: 0.9053616777589205 and parameters: {'max_depth': 4, 'learning_rate': 0.1654779365234358}. Best is trial 19 with value: 0.9058039990541894.\n",
            "[I 2025-04-13 10:40:48,617] Trial 22 finished with value: 0.9043294668792482 and parameters: {'max_depth': 3, 'learning_rate': 0.1192006720222929}. Best is trial 19 with value: 0.9058039990541894.\n",
            "[I 2025-04-13 10:41:10,354] Trial 23 finished with value: 0.9051256634760719 and parameters: {'max_depth': 6, 'learning_rate': 0.2100560356215735}. Best is trial 19 with value: 0.9058039990541894.\n",
            "[I 2025-04-13 10:41:28,020] Trial 24 finished with value: 0.9053910797372415 and parameters: {'max_depth': 5, 'learning_rate': 0.09435757100555489}. Best is trial 19 with value: 0.9058039990541894.\n",
            "[I 2025-04-13 10:41:38,992] Trial 25 finished with value: 0.9050372018263523 and parameters: {'max_depth': 3, 'learning_rate': 0.15795738759375166}. Best is trial 19 with value: 0.9058039990541894.\n",
            "[I 2025-04-13 10:42:00,613] Trial 26 finished with value: 0.9010853155284471 and parameters: {'max_depth': 6, 'learning_rate': 0.2980337113176056}. Best is trial 19 with value: 0.9058039990541894.\n",
            "[I 2025-04-13 10:42:38,283] Trial 27 finished with value: 0.8947740952956735 and parameters: {'max_depth': 9, 'learning_rate': 0.009751955837959422}. Best is trial 19 with value: 0.9058039990541894.\n",
            "[I 2025-04-13 10:42:49,322] Trial 28 finished with value: 0.8996697386515343 and parameters: {'max_depth': 3, 'learning_rate': 0.025927370990704125}. Best is trial 19 with value: 0.9058039990541894.\n",
            "[I 2025-04-13 10:42:56,495] Trial 29 finished with value: 0.8998761996146755 and parameters: {'max_depth': 2, 'learning_rate': 0.07056141197477986}. Best is trial 19 with value: 0.9058039990541894.\n",
            "[I 2025-04-13 10:43:22,396] Trial 30 finished with value: 0.9020290047437123 and parameters: {'max_depth': 7, 'learning_rate': 0.19742886092499337}. Best is trial 19 with value: 0.9058039990541894.\n",
            "[I 2025-04-13 10:43:40,268] Trial 31 finished with value: 0.90559751460704 and parameters: {'max_depth': 5, 'learning_rate': 0.10234222473437048}. Best is trial 19 with value: 0.9058039990541894.\n",
            "[I 2025-04-13 10:43:54,446] Trial 32 finished with value: 0.90547961444832 and parameters: {'max_depth': 4, 'learning_rate': 0.11262523249411252}. Best is trial 19 with value: 0.9058039990541894.\n",
            "[I 2025-04-13 10:44:12,732] Trial 33 finished with value: 0.9054500585192783 and parameters: {'max_depth': 5, 'learning_rate': 0.13632211009769668}. Best is trial 19 with value: 0.9058039990541894.\n",
            "[I 2025-04-13 10:44:23,373] Trial 34 finished with value: 0.9015572240647688 and parameters: {'max_depth': 3, 'learning_rate': 0.04181877105469964}. Best is trial 19 with value: 0.9058039990541894.\n",
            "[I 2025-04-13 10:44:30,981] Trial 35 finished with value: 0.9001121330081623 and parameters: {'max_depth': 2, 'learning_rate': 0.07829357710040134}. Best is trial 19 with value: 0.9058039990541894.\n",
            "[I 2025-04-13 10:44:52,694] Trial 36 finished with value: 0.9033266527592283 and parameters: {'max_depth': 6, 'learning_rate': 0.2180366924016181}. Best is trial 19 with value: 0.9058039990541894.\n",
            "[I 2025-04-13 10:45:23,808] Trial 37 finished with value: 0.9029432345753028 and parameters: {'max_depth': 8, 'learning_rate': 0.024248257356367122}. Best is trial 19 with value: 0.9058039990541894.\n",
            "[I 2025-04-13 10:45:38,856] Trial 38 finished with value: 0.903444665119321 and parameters: {'max_depth': 4, 'learning_rate': 0.052168300860396574}. Best is trial 19 with value: 0.9058039990541894.\n",
            "[I 2025-04-13 10:45:45,914] Trial 39 finished with value: 0.8838032329432165 and parameters: {'max_depth': 2, 'learning_rate': 0.0014256678508130938}. Best is trial 19 with value: 0.9058039990541894.\n",
            "[I 2025-04-13 10:46:45,517] Trial 40 finished with value: 0.9013212384845969 and parameters: {'max_depth': 12, 'learning_rate': 0.13532573626289543}. Best is trial 19 with value: 0.9058039990541894.\n",
            "[I 2025-04-13 10:46:59,779] Trial 41 finished with value: 0.9053321374858841 and parameters: {'max_depth': 4, 'learning_rate': 0.10520336136337508}. Best is trial 19 with value: 0.9058039990541894.\n",
            "[I 2025-04-13 10:47:18,196] Trial 42 finished with value: 0.905980875385612 and parameters: {'max_depth': 5, 'learning_rate': 0.08113277114086674}. Best is trial 42 with value: 0.905980875385612.\n",
            "[I 2025-04-13 10:47:35,981] Trial 43 finished with value: 0.9055385175596635 and parameters: {'max_depth': 5, 'learning_rate': 0.07699936591479072}. Best is trial 42 with value: 0.905980875385612.\n",
            "[I 2025-04-13 10:48:01,639] Trial 44 finished with value: 0.9023534624109405 and parameters: {'max_depth': 7, 'learning_rate': 0.18169148730344056}. Best is trial 42 with value: 0.905980875385612.\n",
            "[I 2025-04-13 10:48:23,316] Trial 45 finished with value: 0.9005249975290909 and parameters: {'max_depth': 6, 'learning_rate': 0.2456275815335019}. Best is trial 42 with value: 0.905980875385612.\n",
            "[I 2025-04-13 10:48:41,336] Trial 46 finished with value: 0.9051256138987211 and parameters: {'max_depth': 5, 'learning_rate': 0.05705172506323498}. Best is trial 42 with value: 0.905980875385612.\n",
            "[I 2025-04-13 10:48:52,127] Trial 47 finished with value: 0.9009379037993677 and parameters: {'max_depth': 3, 'learning_rate': 0.03288381966548702}. Best is trial 42 with value: 0.905980875385612.\n",
            "[I 2025-04-13 10:49:06,919] Trial 48 finished with value: 0.9056860440994502 and parameters: {'max_depth': 4, 'learning_rate': 0.13778549293092787}. Best is trial 42 with value: 0.905980875385612.\n",
            "[I 2025-04-13 10:50:47,396] Trial 49 finished with value: 0.8997581689892429 and parameters: {'max_depth': 15, 'learning_rate': 0.14173185053462747}. Best is trial 42 with value: 0.905980875385612.\n",
            "[I 2025-04-13 10:51:01,460] Trial 50 finished with value: 0.9057450489748294 and parameters: {'max_depth': 4, 'learning_rate': 0.25097519603978125}. Best is trial 42 with value: 0.905980875385612.\n",
            "[I 2025-04-13 10:51:15,582] Trial 51 finished with value: 0.9055680265206889 and parameters: {'max_depth': 4, 'learning_rate': 0.23797501208052874}. Best is trial 42 with value: 0.905980875385612.\n",
            "[I 2025-04-13 10:51:26,541] Trial 52 finished with value: 0.905391105830584 and parameters: {'max_depth': 3, 'learning_rate': 0.1771698115600666}. Best is trial 42 with value: 0.905980875385612.\n",
            "[I 2025-04-13 10:51:40,686] Trial 53 finished with value: 0.9052731039078283 and parameters: {'max_depth': 4, 'learning_rate': 0.28992172294792046}. Best is trial 42 with value: 0.905980875385612.\n",
            "[I 2025-04-13 10:51:48,528] Trial 54 finished with value: 0.9008788963146541 and parameters: {'max_depth': 2, 'learning_rate': 0.08437467042041444}. Best is trial 42 with value: 0.905980875385612.\n",
            "[I 2025-04-13 10:52:09,833] Trial 55 finished with value: 0.9058039625235099 and parameters: {'max_depth': 6, 'learning_rate': 0.12731553974925006}. Best is trial 42 with value: 0.905980875385612.\n",
            "[I 2025-04-13 10:52:31,608] Trial 56 finished with value: 0.905302586775511 and parameters: {'max_depth': 6, 'learning_rate': 0.12484549437220778}. Best is trial 42 with value: 0.905980875385612.\n",
            "[I 2025-04-13 10:52:57,181] Trial 57 finished with value: 0.9005249688264142 and parameters: {'max_depth': 7, 'learning_rate': 0.24081964061870104}. Best is trial 42 with value: 0.905980875385612.\n",
            "[I 2025-04-13 10:53:15,746] Trial 58 finished with value: 0.9052731065171625 and parameters: {'max_depth': 5, 'learning_rate': 0.06128013953900656}. Best is trial 42 with value: 0.905980875385612.\n",
            "[I 2025-04-13 10:53:53,527] Trial 59 finished with value: 0.8894951198639176 and parameters: {'max_depth': 9, 'learning_rate': 0.0077596730488118425}. Best is trial 42 with value: 0.905980875385612.\n",
            "[I 2025-04-13 10:54:15,368] Trial 60 finished with value: 0.9052141668751394 and parameters: {'max_depth': 6, 'learning_rate': 0.17282414713364924}. Best is trial 42 with value: 0.905980875385612.\n",
            "[I 2025-04-13 10:54:29,595] Trial 61 finished with value: 0.9055386297610363 and parameters: {'max_depth': 4, 'learning_rate': 0.14172901944908647}. Best is trial 42 with value: 0.905980875385612.\n",
            "[I 2025-04-13 10:54:47,281] Trial 62 finished with value: 0.906157840434399 and parameters: {'max_depth': 5, 'learning_rate': 0.08750410874687871}. Best is trial 62 with value: 0.906157840434399.\n",
            "[I 2025-04-13 10:55:05,665] Trial 63 finished with value: 0.9053321113925418 and parameters: {'max_depth': 5, 'learning_rate': 0.09058870852074412}. Best is trial 62 with value: 0.906157840434399.\n",
            "[I 2025-04-13 10:55:31,859] Trial 64 finished with value: 0.9041229615574249 and parameters: {'max_depth': 7, 'learning_rate': 0.045162344196581626}. Best is trial 62 with value: 0.906157840434399.\n",
            "[I 2025-04-13 10:55:49,474] Trial 65 finished with value: 0.9067181740897606 and parameters: {'max_depth': 5, 'learning_rate': 0.1164613636909911}. Best is trial 65 with value: 0.9067181740897606.\n",
            "[I 2025-04-13 10:56:00,544] Trial 66 finished with value: 0.9026188812814445 and parameters: {'max_depth': 3, 'learning_rate': 0.06484903407680899}. Best is trial 65 with value: 0.9067181740897606.\n",
            "[I 2025-04-13 10:56:22,359] Trial 67 finished with value: 0.9045358156410165 and parameters: {'max_depth': 6, 'learning_rate': 0.1916167314170298}. Best is trial 65 with value: 0.9067181740897606.\n",
            "[I 2025-04-13 10:56:40,219] Trial 68 finished with value: 0.9058039573048413 and parameters: {'max_depth': 5, 'learning_rate': 0.08880792003687235}. Best is trial 65 with value: 0.9067181740897606.\n",
            "[I 2025-04-13 10:56:58,655] Trial 69 finished with value: 0.9037690288505164 and parameters: {'max_depth': 5, 'learning_rate': 0.03208613994020654}. Best is trial 65 with value: 0.9067181740897606.\n",
            "[I 2025-04-13 10:57:20,536] Trial 70 finished with value: 0.9058334740938694 and parameters: {'max_depth': 6, 'learning_rate': 0.10652193483705044}. Best is trial 65 with value: 0.9067181740897606.\n",
            "[I 2025-04-13 10:57:41,890] Trial 71 finished with value: 0.9048307121605346 and parameters: {'max_depth': 6, 'learning_rate': 0.11240696614052124}. Best is trial 65 with value: 0.9067181740897606.\n",
            "[I 2025-04-13 10:58:24,703] Trial 72 finished with value: 0.901675126832823 and parameters: {'max_depth': 10, 'learning_rate': 0.09144060695420339}. Best is trial 65 with value: 0.9067181740897606.\n",
            "[I 2025-04-13 10:58:56,193] Trial 73 finished with value: 0.901262236218552 and parameters: {'max_depth': 8, 'learning_rate': 0.018485796356538447}. Best is trial 65 with value: 0.9067181740897606.\n",
            "[I 2025-04-13 10:59:14,256] Trial 74 finished with value: 0.9053025998221823 and parameters: {'max_depth': 5, 'learning_rate': 0.07344100946939935}. Best is trial 65 with value: 0.9067181740897606.\n",
            "[I 2025-04-13 10:59:36,072] Trial 75 finished with value: 0.9044178841702855 and parameters: {'max_depth': 6, 'learning_rate': 0.15541285080381798}. Best is trial 65 with value: 0.9067181740897606.\n",
            "[I 2025-04-13 10:59:53,854] Trial 76 finished with value: 0.9050076850373244 and parameters: {'max_depth': 5, 'learning_rate': 0.05100020918255803}. Best is trial 65 with value: 0.9067181740897606.\n",
            "[I 2025-04-13 11:00:19,749] Trial 77 finished with value: 0.9040049518066663 and parameters: {'max_depth': 7, 'learning_rate': 0.11190572269288628}. Best is trial 65 with value: 0.9067181740897606.\n",
            "[I 2025-04-13 11:00:34,230] Trial 78 finished with value: 0.905627052270742 and parameters: {'max_depth': 4, 'learning_rate': 0.2117976466388465}. Best is trial 65 with value: 0.9067181740897606.\n",
            "[I 2025-04-13 11:00:56,125] Trial 79 finished with value: 0.9065117600946362 and parameters: {'max_depth': 6, 'learning_rate': 0.08017347655320685}. Best is trial 65 with value: 0.9067181740897606.\n",
            "[I 2025-04-13 11:01:22,121] Trial 80 finished with value: 0.9048602419962339 and parameters: {'max_depth': 7, 'learning_rate': 0.08106865267726311}. Best is trial 65 with value: 0.9067181740897606.\n",
            "[I 2025-04-13 11:01:44,134] Trial 81 finished with value: 0.905450055909944 and parameters: {'max_depth': 6, 'learning_rate': 0.10156217027990894}. Best is trial 65 with value: 0.9067181740897606.\n",
            "[I 2025-04-13 11:02:02,417] Trial 82 finished with value: 0.9060988746990336 and parameters: {'max_depth': 5, 'learning_rate': 0.12753916975981983}. Best is trial 65 with value: 0.9067181740897606.\n",
            "[I 2025-04-13 11:02:23,820] Trial 83 finished with value: 0.9066296915653672 and parameters: {'max_depth': 6, 'learning_rate': 0.07121624446768493}. Best is trial 65 with value: 0.9067181740897606.\n",
            "[I 2025-04-13 11:02:45,968] Trial 84 finished with value: 0.9050076772093217 and parameters: {'max_depth': 6, 'learning_rate': 0.07179476237124763}. Best is trial 65 with value: 0.9067181740897606.\n",
            "[I 2025-04-13 11:03:16,871] Trial 85 finished with value: 0.9041229511200878 and parameters: {'max_depth': 8, 'learning_rate': 0.12616174442413122}. Best is trial 65 with value: 0.9067181740897606.\n",
            "[I 2025-04-13 11:03:34,677] Trial 86 finished with value: 0.9056269818187174 and parameters: {'max_depth': 5, 'learning_rate': 0.05814811941056663}. Best is trial 65 with value: 0.9067181740897606.\n",
            "[I 2025-04-13 11:04:00,296] Trial 87 finished with value: 0.9026188838907787 and parameters: {'max_depth': 7, 'learning_rate': 0.1569912827026065}. Best is trial 65 with value: 0.9067181740897606.\n",
            "[I 2025-04-13 11:04:22,195] Trial 88 finished with value: 0.9050961205937015 and parameters: {'max_depth': 6, 'learning_rate': 0.10239899351305314}. Best is trial 65 with value: 0.9067181740897606.\n",
            "[I 2025-04-13 11:04:40,497] Trial 89 finished with value: 0.9041819090274505 and parameters: {'max_depth': 5, 'learning_rate': 0.04702684872289069}. Best is trial 65 with value: 0.9067181740897606.\n",
            "[I 2025-04-13 11:05:07,119] Trial 90 finished with value: 0.9040049648533376 and parameters: {'max_depth': 7, 'learning_rate': 0.04004823331096362}. Best is trial 65 with value: 0.9067181740897606.\n",
            "[I 2025-04-13 11:05:24,964] Trial 91 finished with value: 0.9063348080925203 and parameters: {'max_depth': 5, 'learning_rate': 0.08778845288588434}. Best is trial 65 with value: 0.9067181740897606.\n",
            "[I 2025-04-13 11:05:47,034] Trial 92 finished with value: 0.9051551437344204 and parameters: {'max_depth': 6, 'learning_rate': 0.12010661647663576}. Best is trial 65 with value: 0.9067181740897606.\n",
            "[I 2025-04-13 11:06:04,901] Trial 93 finished with value: 0.9048896935519055 and parameters: {'max_depth': 5, 'learning_rate': 0.0660451484211864}. Best is trial 65 with value: 0.9067181740897606.\n",
            "[I 2025-04-13 11:06:19,947] Trial 94 finished with value: 0.9054501107059633 and parameters: {'max_depth': 4, 'learning_rate': 0.08207806207761431}. Best is trial 65 with value: 0.9067181740897606.\n",
            "[I 2025-04-13 11:06:37,615] Trial 95 finished with value: 0.9059219305249204 and parameters: {'max_depth': 5, 'learning_rate': 0.09737995575938652}. Best is trial 65 with value: 0.9067181740897606.\n",
            "[I 2025-04-13 11:06:51,919] Trial 96 finished with value: 0.9053615942602246 and parameters: {'max_depth': 4, 'learning_rate': 0.09470563387739024}. Best is trial 65 with value: 0.9067181740897606.\n",
            "[I 2025-04-13 11:07:02,929] Trial 97 finished with value: 0.8838032329432165 and parameters: {'max_depth': 3, 'learning_rate': 0.0021930417476507718}. Best is trial 65 with value: 0.9067181740897606.\n",
            "[I 2025-04-13 11:07:21,388] Trial 98 finished with value: 0.904978186513636 and parameters: {'max_depth': 5, 'learning_rate': 0.054949358056302064}. Best is trial 65 with value: 0.9067181740897606.\n",
            "[I 2025-04-13 11:08:35,767] Trial 99 finished with value: 0.8992862917649322 and parameters: {'max_depth': 13, 'learning_rate': 0.07275511845318383}. Best is trial 65 with value: 0.9067181740897606.\n",
            "[I 2025-04-13 11:08:35,768] A new study created in memory with name: no-name-3b61dabc-9928-4211-ba47-33a83e96b45f\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:08:35] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:08:36] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:08:36] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-13 11:08:36,423] Trial 0 finished with value: 0.9054796092296516 and parameters: {'max_depth': 3, 'learning_rate': 0.15206139664695795}. Best is trial 0 with value: 0.9054796092296516.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:08:36] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:08:37] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:08:39] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-13 11:08:40,349] Trial 1 finished with value: 0.9002300436042194 and parameters: {'max_depth': 12, 'learning_rate': 0.02144463171079796}. Best is trial 0 with value: 0.9054796092296516.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:08:40] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:08:40] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:08:41] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-13 11:08:41,689] Trial 2 finished with value: 0.9057450254908211 and parameters: {'max_depth': 7, 'learning_rate': 0.10896930724839593}. Best is trial 2 with value: 0.9057450254908211.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:08:41] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:08:43] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:08:44] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-13 11:08:47,960] Trial 3 finished with value: 0.8838032329432165 and parameters: {'max_depth': 16, 'learning_rate': 0.0017410028939488205}. Best is trial 2 with value: 0.9057450254908211.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:08:48] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:08:48] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:08:49] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-13 11:08:50,733] Trial 4 finished with value: 0.8838032329432165 and parameters: {'max_depth': 10, 'learning_rate': 0.0013164200688129822}. Best is trial 2 with value: 0.9057450254908211.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:08:50] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:08:52] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:08:54] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-13 11:08:55,703] Trial 5 finished with value: 0.9031497607718002 and parameters: {'max_depth': 16, 'learning_rate': 0.03984480126514888}. Best is trial 2 with value: 0.9057450254908211.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:08:55] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:08:55] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:08:56] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-13 11:08:56,273] Trial 6 finished with value: 0.8981361441958603 and parameters: {'max_depth': 2, 'learning_rate': 0.07391159668034929}. Best is trial 2 with value: 0.9057450254908211.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:08:56] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:08:59] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:09:01] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-13 11:09:02,613] Trial 7 finished with value: 0.8916480137103188 and parameters: {'max_depth': 14, 'learning_rate': 0.008791085543256846}. Best is trial 2 with value: 0.9057450254908211.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:09:02] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:09:03] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:09:04] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-13 11:09:04,785] Trial 8 finished with value: 0.901911070663647 and parameters: {'max_depth': 11, 'learning_rate': 0.22946886264712657}. Best is trial 2 with value: 0.9057450254908211.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:09:04] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:09:05] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:09:05] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-13 11:09:05,719] Trial 9 finished with value: 0.8838032329432165 and parameters: {'max_depth': 5, 'learning_rate': 0.0010381126400043216}. Best is trial 2 with value: 0.9057450254908211.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:09:05] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:09:06] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:09:06] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-13 11:09:07,214] Trial 10 finished with value: 0.8838032329432165 and parameters: {'max_depth': 7, 'learning_rate': 0.005228515534541022}. Best is trial 2 with value: 0.9057450254908211.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:09:07] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:09:07] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:09:07] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-13 11:09:07,802] Trial 11 finished with value: 0.9056270548800764 and parameters: {'max_depth': 2, 'learning_rate': 0.288761187136023}. Best is trial 2 with value: 0.9057450254908211.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:09:07] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:09:08] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:09:08] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-13 11:09:09,048] Trial 12 finished with value: 0.9024125012076651 and parameters: {'max_depth': 7, 'learning_rate': 0.29501986379449097}. Best is trial 2 with value: 0.9057450254908211.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:09:09] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:09:09] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:09:10] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-13 11:09:11,339] Trial 13 finished with value: 0.9062168479191127 and parameters: {'max_depth': 5, 'learning_rate': 0.07879873352380394}. Best is trial 13 with value: 0.9062168479191127.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:09:11] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:09:12] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:09:12] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-13 11:09:13,016] Trial 14 finished with value: 0.9058039625235099 and parameters: {'max_depth': 7, 'learning_rate': 0.06529911891255999}. Best is trial 13 with value: 0.9062168479191127.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:09:13] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:09:13] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:09:13] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-13 11:09:13,827] Trial 15 finished with value: 0.9031497685998029 and parameters: {'max_depth': 4, 'learning_rate': 0.0429081879307239}. Best is trial 13 with value: 0.9062168479191127.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:09:13] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:09:14] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:09:14] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-13 11:09:14,809] Trial 16 finished with value: 0.9052141642658053 and parameters: {'max_depth': 5, 'learning_rate': 0.05412670060401158}. Best is trial 13 with value: 0.9062168479191127.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:09:14] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:09:15] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:09:16] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-13 11:09:17,131] Trial 17 finished with value: 0.9027663191038666 and parameters: {'max_depth': 9, 'learning_rate': 0.021256514120527766}. Best is trial 13 with value: 0.9062168479191127.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:09:17] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:09:17] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:09:18] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-13 11:09:19,007] Trial 18 finished with value: 0.8938008762447093 and parameters: {'max_depth': 8, 'learning_rate': 0.01027299155325437}. Best is trial 13 with value: 0.9062168479191127.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:09:19] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:09:19] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:09:19] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-13 11:09:19,941] Trial 19 finished with value: 0.9063642987882058 and parameters: {'max_depth': 5, 'learning_rate': 0.09963999338576969}. Best is trial 19 with value: 0.9063642987882058.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:09:20] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:09:20] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:09:20] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-13 11:09:20,869] Trial 20 finished with value: 0.9071016053203573 and parameters: {'max_depth': 5, 'learning_rate': 0.12122554872072336}. Best is trial 20 with value: 0.9071016053203573.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:09:20] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:09:21] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:09:21] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-13 11:09:21,811] Trial 21 finished with value: 0.9066297359240494 and parameters: {'max_depth': 5, 'learning_rate': 0.12440944080014686}. Best is trial 20 with value: 0.9071016053203573.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:09:21] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:09:22] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:09:23] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-13 11:09:24,168] Trial 22 finished with value: 0.9064823059296302 and parameters: {'max_depth': 4, 'learning_rate': 0.15062674840055712}. Best is trial 20 with value: 0.9071016053203573.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:09:24] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:09:24] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:09:24] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-13 11:09:24,850] Trial 23 finished with value: 0.905804035584869 and parameters: {'max_depth': 3, 'learning_rate': 0.18316448626453583}. Best is trial 20 with value: 0.9071016053203573.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:09:24] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:09:25] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:09:25] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-13 11:09:25,652] Trial 24 finished with value: 0.9068656693175363 and parameters: {'max_depth': 4, 'learning_rate': 0.14419818046071875}. Best is trial 20 with value: 0.9071016053203573.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:09:25] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:09:26] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:09:26] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-13 11:09:26,861] Trial 25 finished with value: 0.9040934447683968 and parameters: {'max_depth': 6, 'learning_rate': 0.0316012680568148}. Best is trial 20 with value: 0.9071016053203573.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:09:26] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:09:27] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:09:27] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-13 11:09:27,556] Trial 26 finished with value: 0.9048602759175791 and parameters: {'max_depth': 3, 'learning_rate': 0.1371042359773365}. Best is trial 20 with value: 0.9071016053203573.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:09:27] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:09:28] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:09:28] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-13 11:09:29,467] Trial 27 finished with value: 0.9056860336621132 and parameters: {'max_depth': 9, 'learning_rate': 0.10196999634041984}. Best is trial 20 with value: 0.9071016053203573.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:09:29] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:09:29] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:09:30] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-13 11:09:30,639] Trial 28 finished with value: 0.8838032329432165 and parameters: {'max_depth': 6, 'learning_rate': 0.0029525606362217527}. Best is trial 20 with value: 0.9071016053203573.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:09:30] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:09:30] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:09:31] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-13 11:09:31,392] Trial 29 finished with value: 0.9073965566358947 and parameters: {'max_depth': 4, 'learning_rate': 0.19068390552988063}. Best is trial 29 with value: 0.9073965566358947.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:09:31] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:09:31] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:09:31] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-13 11:09:31,973] Trial 30 finished with value: 0.9049487871446494 and parameters: {'max_depth': 2, 'learning_rate': 0.19728035068554617}. Best is trial 29 with value: 0.9073965566358947.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:09:32] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:09:32] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:09:32] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-13 11:09:32,756] Trial 31 finished with value: 0.9062168714031208 and parameters: {'max_depth': 4, 'learning_rate': 0.14027547883320762}. Best is trial 29 with value: 0.9073965566358947.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:09:32] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:09:33] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:09:33] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-13 11:09:33,508] Trial 32 finished with value: 0.9072785338384649 and parameters: {'max_depth': 4, 'learning_rate': 0.21650233511933284}. Best is trial 29 with value: 0.9073965566358947.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:09:33] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:09:33] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:09:34] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-13 11:09:34,237] Trial 33 finished with value: 0.9060399585410187 and parameters: {'max_depth': 3, 'learning_rate': 0.20354228284455195}. Best is trial 29 with value: 0.9073965566358947.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:09:34] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:09:35] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:09:36] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-13 11:09:36,536] Trial 34 finished with value: 0.9060694544553728 and parameters: {'max_depth': 4, 'learning_rate': 0.08912535811611615}. Best is trial 29 with value: 0.9073965566358947.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:09:36] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:09:36] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:09:37] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-13 11:09:37,610] Trial 35 finished with value: 0.9049782204349813 and parameters: {'max_depth': 6, 'learning_rate': 0.25631415784489486}. Best is trial 29 with value: 0.9073965566358947.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:09:37] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:09:39] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:09:40] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-13 11:09:41,821] Trial 36 finished with value: 0.901763622403888 and parameters: {'max_depth': 13, 'learning_rate': 0.029880040883624802}. Best is trial 29 with value: 0.9073965566358947.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:09:41] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:09:42] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:09:43] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-13 11:09:43,526] Trial 37 finished with value: 0.9055090555666547 and parameters: {'max_depth': 8, 'learning_rate': 0.05475365401328664}. Best is trial 29 with value: 0.9073965566358947.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:09:43] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:09:43] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:09:43] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-13 11:09:44,127] Trial 38 finished with value: 0.9038280833032464 and parameters: {'max_depth': 2, 'learning_rate': 0.1722903965314224}. Best is trial 29 with value: 0.9073965566358947.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:09:44] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:09:44] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:09:44] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-13 11:09:44,837] Trial 39 finished with value: 0.9043884326146138 and parameters: {'max_depth': 3, 'learning_rate': 0.12066119643880988}. Best is trial 29 with value: 0.9073965566358947.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:09:44] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:09:45] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:09:45] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-13 11:09:46,061] Trial 40 finished with value: 0.8898785302198401 and parameters: {'max_depth': 6, 'learning_rate': 0.010331190692524023}. Best is trial 29 with value: 0.9073965566358947.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:09:46] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:09:46] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:09:47] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-13 11:09:48,252] Trial 41 finished with value: 0.9068657162855528 and parameters: {'max_depth': 4, 'learning_rate': 0.11658803445148688}. Best is trial 29 with value: 0.9073965566358947.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:09:48] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:09:48] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:09:49] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-13 11:09:49,246] Trial 42 finished with value: 0.9063938364519079 and parameters: {'max_depth': 4, 'learning_rate': 0.21274230682190395}. Best is trial 29 with value: 0.9073965566358947.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:09:49] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:09:49] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:09:49] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-13 11:09:49,918] Trial 43 finished with value: 0.9032382198121853 and parameters: {'max_depth': 3, 'learning_rate': 0.06984597189788061}. Best is trial 29 with value: 0.9073965566358947.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:09:49] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:09:50] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:09:50] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-13 11:09:50,697] Trial 44 finished with value: 0.9071900982820879 and parameters: {'max_depth': 4, 'learning_rate': 0.15446577997735214}. Best is trial 29 with value: 0.9073965566358947.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:09:50] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:09:51] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:09:52] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-13 11:09:52,803] Trial 45 finished with value: 0.9034446625099868 and parameters: {'max_depth': 11, 'learning_rate': 0.2930886773791418}. Best is trial 29 with value: 0.9073965566358947.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:09:52] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:09:53] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:09:53] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-13 11:09:53,398] Trial 46 finished with value: 0.9015866651831036 and parameters: {'max_depth': 2, 'learning_rate': 0.09676016617117997}. Best is trial 29 with value: 0.9073965566358947.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:09:53] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:09:54] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:09:55] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-13 11:09:56,260] Trial 47 finished with value: 0.9024124438023117 and parameters: {'max_depth': 15, 'learning_rate': 0.2405785743340553}. Best is trial 29 with value: 0.9073965566358947.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:09:56] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:09:56] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:09:56] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-13 11:09:57,236] Trial 48 finished with value: 0.9056565351384248 and parameters: {'max_depth': 5, 'learning_rate': 0.05358108098168855}. Best is trial 29 with value: 0.9073965566358947.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:09:57] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:09:57] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:09:58] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-13 11:09:59,153] Trial 49 finished with value: 0.9059809093069573 and parameters: {'max_depth': 8, 'learning_rate': 0.11818739645868019}. Best is trial 29 with value: 0.9073965566358947.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:09:59] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:10:00] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:10:01] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-13 11:10:01,465] Trial 50 finished with value: 0.9064822928829589 and parameters: {'max_depth': 6, 'learning_rate': 0.17140185935806942}. Best is trial 29 with value: 0.9073965566358947.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:10:01] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:10:01] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:10:02] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-13 11:10:02,253] Trial 51 finished with value: 0.9064527995779391 and parameters: {'max_depth': 4, 'learning_rate': 0.15298615221916068}. Best is trial 29 with value: 0.9073965566358947.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:10:02] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:10:02] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:10:02] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-13 11:10:03,159] Trial 52 finished with value: 0.9068361760125166 and parameters: {'max_depth': 5, 'learning_rate': 0.22823496086548967}. Best is trial 29 with value: 0.9073965566358947.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:10:03] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:10:03] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:10:03] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-13 11:10:03,865] Trial 53 finished with value: 0.9037396347001981 and parameters: {'max_depth': 3, 'learning_rate': 0.08895921495449574}. Best is trial 29 with value: 0.9073965566358947.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:10:03] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:10:04] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:10:04] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-13 11:10:04,629] Trial 54 finished with value: 0.9070721407180145 and parameters: {'max_depth': 4, 'learning_rate': 0.16130347728415803}. Best is trial 29 with value: 0.9073965566358947.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:10:04] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:10:04] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:10:05] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-13 11:10:05,310] Trial 55 finished with value: 0.9031792462488172 and parameters: {'max_depth': 3, 'learning_rate': 0.0764920187722315}. Best is trial 29 with value: 0.9073965566358947.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:10:05] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:10:05] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:10:05] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-13 11:10:06,107] Trial 56 finished with value: 0.9070131410613036 and parameters: {'max_depth': 4, 'learning_rate': 0.11269986468149853}. Best is trial 29 with value: 0.9073965566358947.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:10:06] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:10:06] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:10:06] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-13 11:10:07,017] Trial 57 finished with value: 0.9066296915653672 and parameters: {'max_depth': 5, 'learning_rate': 0.17650027764437407}. Best is trial 29 with value: 0.9073965566358947.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:10:07] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:10:07] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:10:08] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-13 11:10:08,519] Trial 58 finished with value: 0.8982836707356467 and parameters: {'max_depth': 7, 'learning_rate': 0.014340680336773874}. Best is trial 29 with value: 0.9073965566358947.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:10:08] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:10:08] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:10:08] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-13 11:10:09,124] Trial 59 finished with value: 0.8922083708496888 and parameters: {'max_depth': 2, 'learning_rate': 0.03800370148310716}. Best is trial 29 with value: 0.9073965566358947.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:10:09] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:10:09] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:10:09] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-13 11:10:10,030] Trial 60 finished with value: 0.9066887251434231 and parameters: {'max_depth': 5, 'learning_rate': 0.2599906116114887}. Best is trial 29 with value: 0.9073965566358947.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:10:10] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:10:10] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:10:10] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-13 11:10:10,836] Trial 61 finished with value: 0.9064527995779391 and parameters: {'max_depth': 4, 'learning_rate': 0.10893457351889373}. Best is trial 29 with value: 0.9073965566358947.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:10:10] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:10:11] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:10:12] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-13 11:10:13,116] Trial 62 finished with value: 0.9065707727980182 and parameters: {'max_depth': 4, 'learning_rate': 0.13060288882672258}. Best is trial 29 with value: 0.9073965566358947.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:10:13] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:10:13] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:10:13] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-13 11:10:13,884] Trial 63 finished with value: 0.9051847127101335 and parameters: {'max_depth': 3, 'learning_rate': 0.15712539650009152}. Best is trial 29 with value: 0.9073965566358947.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:10:13] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:10:14] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:10:14] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-13 11:10:14,868] Trial 64 finished with value: 0.9055680369580258 and parameters: {'max_depth': 5, 'learning_rate': 0.06071725024652295}. Best is trial 29 with value: 0.9073965566358947.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:10:14] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:10:15] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:10:15] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-13 11:10:15,696] Trial 65 finished with value: 0.9054795857456432 and parameters: {'max_depth': 4, 'learning_rate': 0.08226469779288417}. Best is trial 29 with value: 0.9073965566358947.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:10:15] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:10:16] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:10:16] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-13 11:10:16,750] Trial 66 finished with value: 0.9062169235898058 and parameters: {'max_depth': 6, 'learning_rate': 0.2025451164928515}. Best is trial 29 with value: 0.9073965566358947.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:10:16] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:10:17] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:10:17] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-13 11:10:17,337] Trial 67 finished with value: 0.8838032329432165 and parameters: {'max_depth': 2, 'learning_rate': 0.004032527259943298}. Best is trial 29 with value: 0.9073965566358947.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:10:17] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:10:17] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:10:17] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-13 11:10:18,110] Trial 68 finished with value: 0.9071311247187196 and parameters: {'max_depth': 4, 'learning_rate': 0.11547546521477332}. Best is trial 29 with value: 0.9073965566358947.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:10:18] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:10:18] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:10:19] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-13 11:10:19,547] Trial 69 finished with value: 0.9048897274732508 and parameters: {'max_depth': 7, 'learning_rate': 0.04515734816495031}. Best is trial 29 with value: 0.9073965566358947.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:10:19] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:10:19] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:10:20] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-13 11:10:20,448] Trial 70 finished with value: 0.9062463568801378 and parameters: {'max_depth': 5, 'learning_rate': 0.2492930808474987}. Best is trial 29 with value: 0.9073965566358947.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:10:20] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:10:20] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:10:21] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-13 11:10:21,252] Trial 71 finished with value: 0.9063348472325341 and parameters: {'max_depth': 4, 'learning_rate': 0.11161127546554399}. Best is trial 29 with value: 0.9073965566358947.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:10:21] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:10:21] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:10:21] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-13 11:10:21,915] Trial 72 finished with value: 0.905538595839691 and parameters: {'max_depth': 3, 'learning_rate': 0.13373865713156055}. Best is trial 29 with value: 0.9073965566358947.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:10:21] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:10:22] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:10:22] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-13 11:10:22,678] Trial 73 finished with value: 0.8838032329432165 and parameters: {'max_depth': 4, 'learning_rate': 0.0017597158311123888}. Best is trial 29 with value: 0.9073965566358947.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:10:22] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:10:23] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:10:23] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-13 11:10:24,520] Trial 74 finished with value: 0.906010423486651 and parameters: {'max_depth': 6, 'learning_rate': 0.1886852213813796}. Best is trial 29 with value: 0.9073965566358947.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:10:24] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:10:25] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:10:25] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-13 11:10:26,095] Trial 75 finished with value: 0.906010433923988 and parameters: {'max_depth': 4, 'learning_rate': 0.10660999713637999}. Best is trial 29 with value: 0.9073965566358947.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:10:26] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:10:26] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:10:26] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-13 11:10:26,792] Trial 76 finished with value: 0.9055680917540451 and parameters: {'max_depth': 3, 'learning_rate': 0.15713721244239032}. Best is trial 29 with value: 0.9073965566358947.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:10:26] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:10:27] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:10:27] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-13 11:10:27,698] Trial 77 finished with value: 0.9050961597337152 and parameters: {'max_depth': 5, 'learning_rate': 0.2950559749391875}. Best is trial 29 with value: 0.9073965566358947.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:10:27] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:10:28] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:10:28] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-13 11:10:28,536] Trial 78 finished with value: 0.9059809510563053 and parameters: {'max_depth': 4, 'learning_rate': 0.08980182670880801}. Best is trial 29 with value: 0.9073965566358947.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:10:28] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:10:28] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:10:28] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-13 11:10:29,111] Trial 79 finished with value: 0.9047718220958623 and parameters: {'max_depth': 2, 'learning_rate': 0.21670792692921162}. Best is trial 29 with value: 0.9073965566358947.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:10:29] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:10:29] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:10:30] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-13 11:10:31,461] Trial 80 finished with value: 0.9045063536480077 and parameters: {'max_depth': 10, 'learning_rate': 0.06848986631577236}. Best is trial 29 with value: 0.9073965566358947.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:10:31] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:10:31] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:10:32] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-13 11:10:32,159] Trial 81 finished with value: 0.9052731639225159 and parameters: {'max_depth': 3, 'learning_rate': 0.15382072504903538}. Best is trial 29 with value: 0.9073965566358947.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:10:32] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:10:32] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:10:32] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-13 11:10:32,953] Trial 82 finished with value: 0.9072785990718212 and parameters: {'max_depth': 4, 'learning_rate': 0.129657639589202}. Best is trial 29 with value: 0.9073965566358947.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:10:33] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:10:33] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:10:33] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-13 11:10:33,903] Trial 83 finished with value: 0.907485005238943 and parameters: {'max_depth': 5, 'learning_rate': 0.12865417881274127}. Best is trial 83 with value: 0.907485005238943.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:10:33] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:10:34] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:10:34] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-13 11:10:34,840] Trial 84 finished with value: 0.9066002608843694 and parameters: {'max_depth': 5, 'learning_rate': 0.1371936077571857}. Best is trial 83 with value: 0.907485005238943.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:10:34] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:10:35] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:10:35] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-13 11:10:36,256] Trial 85 finished with value: 0.907131119500051 and parameters: {'max_depth': 6, 'learning_rate': 0.16969819716608217}. Best is trial 83 with value: 0.907485005238943.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:10:36] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:10:37] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:10:38] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-13 11:10:38,583] Trial 86 finished with value: 0.9055090555666547 and parameters: {'max_depth': 6, 'learning_rate': 0.1767980437561233}. Best is trial 83 with value: 0.907485005238943.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:10:38] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:10:38] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:10:39] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-13 11:10:39,476] Trial 87 finished with value: 0.9055975172163744 and parameters: {'max_depth': 5, 'learning_rate': 0.2088034390684572}. Best is trial 83 with value: 0.907485005238943.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:10:39] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:10:39] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:10:40] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-13 11:10:40,583] Trial 88 finished with value: 0.9034446886033294 and parameters: {'max_depth': 6, 'learning_rate': 0.262469700933636}. Best is trial 83 with value: 0.907485005238943.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:10:40] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:10:41] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:10:41] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-13 11:10:41,969] Trial 89 finished with value: 0.9063643301002168 and parameters: {'max_depth': 7, 'learning_rate': 0.09305159011690957}. Best is trial 83 with value: 0.907485005238943.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:10:42] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:10:42] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:10:42] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-13 11:10:42,881] Trial 90 finished with value: 0.9060988955737076 and parameters: {'max_depth': 5, 'learning_rate': 0.12680677135109264}. Best is trial 83 with value: 0.907485005238943.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:10:42] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:10:43] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:10:43] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-13 11:10:43,667] Trial 91 finished with value: 0.906777241589162 and parameters: {'max_depth': 4, 'learning_rate': 0.17747317687248823}. Best is trial 83 with value: 0.907485005238943.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:10:43] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:10:43] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:10:44] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-13 11:10:44,353] Trial 92 finished with value: 0.9050667264433833 and parameters: {'max_depth': 3, 'learning_rate': 0.10715431159048053}. Best is trial 83 with value: 0.907485005238943.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:10:44] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:10:44] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:10:44] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-13 11:10:45,147] Trial 93 finished with value: 0.9064823215856358 and parameters: {'max_depth': 4, 'learning_rate': 0.14798012595053292}. Best is trial 83 with value: 0.907485005238943.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:10:45] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:10:45] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:10:45] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-13 11:10:46,123] Trial 94 finished with value: 0.9063348446231999 and parameters: {'max_depth': 5, 'learning_rate': 0.07719047759810857}. Best is trial 83 with value: 0.907485005238943.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:10:46] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:10:46] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:10:46] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-13 11:10:46,877] Trial 95 finished with value: 0.9074260316755747 and parameters: {'max_depth': 4, 'learning_rate': 0.2299706146497536}. Best is trial 83 with value: 0.907485005238943.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:10:46] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:10:47] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:10:47] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-13 11:10:47,564] Trial 96 finished with value: 0.9066592344477377 and parameters: {'max_depth': 3, 'learning_rate': 0.2024612053224796}. Best is trial 83 with value: 0.907485005238943.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:10:47] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:10:47] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:10:48] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-13 11:10:48,697] Trial 97 finished with value: 0.9067772441984964 and parameters: {'max_depth': 4, 'learning_rate': 0.23298290118039158}. Best is trial 83 with value: 0.907485005238943.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:10:48] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:10:50] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:10:50] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-13 11:10:50,995] Trial 98 finished with value: 0.9059514290486087 and parameters: {'max_depth': 6, 'learning_rate': 0.1576920107029658}. Best is trial 83 with value: 0.907485005238943.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:10:51] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:10:51] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:10:51] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-13 11:10:51,904] Trial 99 finished with value: 0.9059219540089286 and parameters: {'max_depth': 5, 'learning_rate': 0.26330417347457746}. Best is trial 83 with value: 0.907485005238943.\n",
            "[I 2025-04-13 11:10:51,906] A new study created in memory with name: no-name-c88087ba-bfc3-4574-bfee-b94ccd176eba\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002841 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002857 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002869 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-13 11:10:53,377] Trial 0 finished with value: 0.9053025893848452 and parameters: {'max_depth': 15, 'learning_rate': 0.02441263819093759}. Best is trial 0 with value: 0.9053025893848452.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002846 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002699 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002966 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-13 11:10:54,579] Trial 1 finished with value: 0.905921956618263 and parameters: {'max_depth': 16, 'learning_rate': 0.23514223759893044}. Best is trial 1 with value: 0.905921956618263.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002824 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002732 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002705 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-13 11:10:55,972] Trial 2 finished with value: 0.8914415527471778 and parameters: {'max_depth': 9, 'learning_rate': 0.011103408101844866}. Best is trial 1 with value: 0.905921956618263.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002795 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002722 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002808 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-13 11:10:57,272] Trial 3 finished with value: 0.8838032329432165 and parameters: {'max_depth': 10, 'learning_rate': 0.0032378489025534293}. Best is trial 1 with value: 0.905921956618263.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002831 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002861 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002803 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-13 11:10:58,538] Trial 4 finished with value: 0.9073670346281982 and parameters: {'max_depth': 15, 'learning_rate': 0.17056040823740326}. Best is trial 4 with value: 0.9073670346281982.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002804 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002718 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002728 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-13 11:10:59,994] Trial 5 finished with value: 0.9082812227104409 and parameters: {'max_depth': 15, 'learning_rate': 0.05239962673645042}. Best is trial 5 with value: 0.9082812227104409.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002784 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011686 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004020 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-13 11:11:01,800] Trial 6 finished with value: 0.8997582368319333 and parameters: {'max_depth': 5, 'learning_rate': 0.020378004741128997}. Best is trial 5 with value: 0.9082812227104409.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004188 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004027 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002731 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-13 11:11:03,204] Trial 7 finished with value: 0.9046538201731066 and parameters: {'max_depth': 4, 'learning_rate': 0.08507156855960922}. Best is trial 5 with value: 0.9082812227104409.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002732 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002811 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002911 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-13 11:11:04,021] Trial 8 finished with value: 0.8838032329432165 and parameters: {'max_depth': 3, 'learning_rate': 0.00124888156861582}. Best is trial 5 with value: 0.9082812227104409.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002819 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002748 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002851 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-13 11:11:05,318] Trial 9 finished with value: 0.8838032329432165 and parameters: {'max_depth': 15, 'learning_rate': 0.002067064057008681}. Best is trial 5 with value: 0.9082812227104409.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002808 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002772 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002732 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-13 11:11:06,771] Trial 10 finished with value: 0.9078978462758635 and parameters: {'max_depth': 11, 'learning_rate': 0.05205065023436333}. Best is trial 5 with value: 0.9082812227104409.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002855 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002814 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002865 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-13 11:11:08,193] Trial 11 finished with value: 0.9082517685454349 and parameters: {'max_depth': 11, 'learning_rate': 0.06442790584890268}. Best is trial 5 with value: 0.9082812227104409.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002803 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002880 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002841 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-13 11:11:09,639] Trial 12 finished with value: 0.9086056516749924 and parameters: {'max_depth': 12, 'learning_rate': 0.060251879695890825}. Best is trial 12 with value: 0.9086056516749924.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002885 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002811 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002739 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-13 11:11:11,040] Trial 13 finished with value: 0.8839801849453323 and parameters: {'max_depth': 13, 'learning_rate': 0.007659597780780734}. Best is trial 12 with value: 0.9086056516749924.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002792 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002786 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002724 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-13 11:11:12,484] Trial 14 finished with value: 0.9063938025305626 and parameters: {'max_depth': 7, 'learning_rate': 0.03471471196988715}. Best is trial 12 with value: 0.9086056516749924.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002688 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003949 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004514 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-13 11:11:14,287] Trial 15 finished with value: 0.9074555249805946 and parameters: {'max_depth': 13, 'learning_rate': 0.12737205033402435}. Best is trial 12 with value: 0.9086056516749924.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008172 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002846 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002772 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-13 11:11:15,937] Trial 16 finished with value: 0.8839506942496467 and parameters: {'max_depth': 13, 'learning_rate': 0.007215607273809079}. Best is trial 12 with value: 0.9086056516749924.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002840 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002830 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002814 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-13 11:11:17,385] Trial 17 finished with value: 0.9070720702659897 and parameters: {'max_depth': 7, 'learning_rate': 0.04635458582642965}. Best is trial 12 with value: 0.9086056516749924.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002700 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002845 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002819 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-13 11:11:18,726] Trial 18 finished with value: 0.9077209281950928 and parameters: {'max_depth': 12, 'learning_rate': 0.10778761507785327}. Best is trial 12 with value: 0.9086056516749924.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002678 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002824 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003346 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-13 11:11:19,955] Trial 19 finished with value: 0.9047127702524665 and parameters: {'max_depth': 8, 'learning_rate': 0.2816405432932658}. Best is trial 12 with value: 0.9086056516749924.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003371 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002836 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002827 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-13 11:11:21,411] Trial 20 finished with value: 0.8958358386203794 and parameters: {'max_depth': 14, 'learning_rate': 0.015130303509542068}. Best is trial 12 with value: 0.9086056516749924.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002766 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002839 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002779 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-13 11:11:22,884] Trial 21 finished with value: 0.9081337770600161 and parameters: {'max_depth': 11, 'learning_rate': 0.0440260476769109}. Best is trial 12 with value: 0.9086056516749924.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002800 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002845 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002857 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-13 11:11:24,267] Trial 22 finished with value: 0.9085761661979754 and parameters: {'max_depth': 11, 'learning_rate': 0.08244650883551981}. Best is trial 12 with value: 0.9086056516749924.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002786 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002748 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004072 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-13 11:11:25,900] Trial 23 finished with value: 0.9080158090586053 and parameters: {'max_depth': 12, 'learning_rate': 0.08342595262363596}. Best is trial 12 with value: 0.9086056516749924.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003977 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003951 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002845 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-13 11:11:27,861] Trial 24 finished with value: 0.9056859788660939 and parameters: {'max_depth': 16, 'learning_rate': 0.027897697438642732}. Best is trial 12 with value: 0.9086056516749924.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002720 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002806 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003439 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-13 11:11:29,212] Trial 25 finished with value: 0.9073375413231785 and parameters: {'max_depth': 9, 'learning_rate': 0.1317079276556743}. Best is trial 12 with value: 0.9086056516749924.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002716 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002764 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003239 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-13 11:11:30,644] Trial 26 finished with value: 0.9083992220238623 and parameters: {'max_depth': 10, 'learning_rate': 0.06827486711992602}. Best is trial 12 with value: 0.9086056516749924.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002844 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002754 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002684 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-13 11:11:31,911] Trial 27 finished with value: 0.9065117679226389 and parameters: {'max_depth': 10, 'learning_rate': 0.1796101600784516}. Best is trial 12 with value: 0.9086056516749924.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002814 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002866 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003649 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-13 11:11:33,317] Trial 28 finished with value: 0.9071605736650571 and parameters: {'max_depth': 6, 'learning_rate': 0.07894386854028558}. Best is trial 12 with value: 0.9086056516749924.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002939 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002816 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002700 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-13 11:11:34,804] Trial 29 finished with value: 0.9065412168689764 and parameters: {'max_depth': 10, 'learning_rate': 0.029295228367431654}. Best is trial 12 with value: 0.9086056516749924.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002847 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002748 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002697 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-13 11:11:36,253] Trial 30 finished with value: 0.896012777575824 and parameters: {'max_depth': 12, 'learning_rate': 0.01524229428500862}. Best is trial 12 with value: 0.9086056516749924.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002819 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003959 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002791 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-13 11:11:37,829] Trial 31 finished with value: 0.9075439527089687 and parameters: {'max_depth': 14, 'learning_rate': 0.05706788163023876}. Best is trial 12 with value: 0.9086056516749924.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003957 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004063 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003973 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-13 11:11:39,700] Trial 32 finished with value: 0.9058924372199005 and parameters: {'max_depth': 8, 'learning_rate': 0.18058107212665214}. Best is trial 12 with value: 0.9086056516749924.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004023 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002883 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002830 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-13 11:11:41,269] Trial 33 finished with value: 0.9073080454088244 and parameters: {'max_depth': 16, 'learning_rate': 0.037114146212882455}. Best is trial 12 with value: 0.9086056516749924.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002709 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002831 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003189 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-13 11:11:42,619] Trial 34 finished with value: 0.9074850026296087 and parameters: {'max_depth': 14, 'learning_rate': 0.1072350025526857}. Best is trial 12 with value: 0.9086056516749924.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002795 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002729 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002730 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-13 11:11:44,046] Trial 35 finished with value: 0.9077208760084079 and parameters: {'max_depth': 10, 'learning_rate': 0.07267945638491627}. Best is trial 12 with value: 0.9086056516749924.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002702 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002742 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002785 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-13 11:11:45,510] Trial 36 finished with value: 0.9028252744018951 and parameters: {'max_depth': 12, 'learning_rate': 0.02039889082242823}. Best is trial 12 with value: 0.9086056516749924.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002695 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002740 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002794 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-13 11:11:46,742] Trial 37 finished with value: 0.9053321374858841 and parameters: {'max_depth': 15, 'learning_rate': 0.20339475806542687}. Best is trial 12 with value: 0.9086056516749924.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002840 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002721 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002707 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-13 11:11:47,553] Trial 38 finished with value: 0.9012033226698715 and parameters: {'max_depth': 2, 'learning_rate': 0.10275631591381301}. Best is trial 12 with value: 0.9086056516749924.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002753 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002730 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002756 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-13 11:11:48,959] Trial 39 finished with value: 0.8905568110019385 and parameters: {'max_depth': 9, 'learning_rate': 0.01045962119727272}. Best is trial 12 with value: 0.9086056516749924.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002822 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002863 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003975 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-13 11:11:50,457] Trial 40 finished with value: 0.9069836660216236 and parameters: {'max_depth': 11, 'learning_rate': 0.14011592930338038}. Best is trial 12 with value: 0.9086056516749924.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003977 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004094 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006677 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-13 11:11:52,546] Trial 41 finished with value: 0.9073080375808217 and parameters: {'max_depth': 11, 'learning_rate': 0.0599780825911894}. Best is trial 12 with value: 0.9086056516749924.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002693 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002655 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002756 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-13 11:11:53,978] Trial 42 finished with value: 0.9080747747939709 and parameters: {'max_depth': 8, 'learning_rate': 0.06218450008557601}. Best is trial 12 with value: 0.9086056516749924.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002985 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002827 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002724 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-13 11:11:55,446] Trial 43 finished with value: 0.9073080088781449 and parameters: {'max_depth': 10, 'learning_rate': 0.037944678169516105}. Best is trial 12 with value: 0.9086056516749924.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002904 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002732 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003432 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-13 11:11:56,929] Trial 44 finished with value: 0.9060398906983282 and parameters: {'max_depth': 13, 'learning_rate': 0.02611119383690037}. Best is trial 12 with value: 0.9086056516749924.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002857 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002813 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002730 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-13 11:11:58,369] Trial 45 finished with value: 0.9077209073204188 and parameters: {'max_depth': 9, 'learning_rate': 0.06849717675668718}. Best is trial 12 with value: 0.9086056516749924.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002860 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002880 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002832 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-13 11:11:59,850] Trial 46 finished with value: 0.908045320628965 and parameters: {'max_depth': 11, 'learning_rate': 0.04793779856931414}. Best is trial 12 with value: 0.9086056516749924.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002865 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002821 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002712 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-13 11:12:01,210] Trial 47 finished with value: 0.9077503927974359 and parameters: {'max_depth': 14, 'learning_rate': 0.0956033650237822}. Best is trial 12 with value: 0.9086056516749924.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002717 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002699 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002725 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-13 11:12:02,730] Trial 48 finished with value: 0.8838032329432165 and parameters: {'max_depth': 12, 'learning_rate': 0.0040730812112151895}. Best is trial 12 with value: 0.9086056516749924.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003978 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004136 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004401 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-13 11:12:04,806] Trial 49 finished with value: 0.9041229250267452 and parameters: {'max_depth': 13, 'learning_rate': 0.02172911576895008}. Best is trial 12 with value: 0.9086056516749924.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002719 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002853 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002842 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-13 11:12:06,105] Trial 50 finished with value: 0.9077209516791012 and parameters: {'max_depth': 15, 'learning_rate': 0.13815284985078935}. Best is trial 12 with value: 0.9086056516749924.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002756 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002722 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002804 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-13 11:12:07,594] Trial 51 finished with value: 0.9076914296714046 and parameters: {'max_depth': 11, 'learning_rate': 0.03978297972506216}. Best is trial 12 with value: 0.9086056516749924.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002768 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002756 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002812 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-13 11:12:09,065] Trial 52 finished with value: 0.9078978410571948 and parameters: {'max_depth': 10, 'learning_rate': 0.0545509516150573}. Best is trial 12 with value: 0.9086056516749924.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002827 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002841 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002736 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-13 11:12:10,551] Trial 53 finished with value: 0.907396496621207 and parameters: {'max_depth': 11, 'learning_rate': 0.04306264890046594}. Best is trial 12 with value: 0.9086056516749924.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002695 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002832 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002844 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-13 11:12:11,951] Trial 54 finished with value: 0.9070425978356438 and parameters: {'max_depth': 12, 'learning_rate': 0.08721876434182275}. Best is trial 12 with value: 0.9086056516749924.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003887 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002718 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002716 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-13 11:12:13,404] Trial 55 finished with value: 0.9082812592411204 and parameters: {'max_depth': 10, 'learning_rate': 0.06815744851322927}. Best is trial 12 with value: 0.9086056516749924.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002830 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002737 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002637 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-13 11:12:14,656] Trial 56 finished with value: 0.9051550941570697 and parameters: {'max_depth': 9, 'learning_rate': 0.25557928629005305}. Best is trial 12 with value: 0.9086056516749924.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003959 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003957 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004062 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-13 11:12:16,728] Trial 57 finished with value: 0.9063642987882058 and parameters: {'max_depth': 7, 'learning_rate': 0.03082593021658499}. Best is trial 12 with value: 0.9086056516749924.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004311 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002741 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002738 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-13 11:12:18,140] Trial 58 finished with value: 0.9071605684463887 and parameters: {'max_depth': 10, 'learning_rate': 0.11882376624763062}. Best is trial 12 with value: 0.9086056516749924.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002770 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002719 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002873 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-13 11:12:19,441] Trial 59 finished with value: 0.8838032329432165 and parameters: {'max_depth': 9, 'learning_rate': 0.0010952834462201275}. Best is trial 12 with value: 0.9086056516749924.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002786 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002764 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002666 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-13 11:12:20,862] Trial 60 finished with value: 0.9082222778497493 and parameters: {'max_depth': 13, 'learning_rate': 0.07279952140751089}. Best is trial 12 with value: 0.9086056516749924.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002768 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002746 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002705 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-13 11:12:22,266] Trial 61 finished with value: 0.9081338214186983 and parameters: {'max_depth': 13, 'learning_rate': 0.07871817813351041}. Best is trial 12 with value: 0.9086056516749924.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002826 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002866 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002858 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-13 11:12:23,704] Trial 62 finished with value: 0.908133787497353 and parameters: {'max_depth': 12, 'learning_rate': 0.06587111414468605}. Best is trial 12 with value: 0.9086056516749924.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002818 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002805 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002875 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-13 11:12:25,171] Trial 63 finished with value: 0.907455467575241 and parameters: {'max_depth': 13, 'learning_rate': 0.05154363281334526}. Best is trial 12 with value: 0.9086056516749924.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002944 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002837 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002809 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-13 11:12:26,540] Trial 64 finished with value: 0.9074849739269321 and parameters: {'max_depth': 15, 'learning_rate': 0.0942775766891695}. Best is trial 12 with value: 0.9086056516749924.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002879 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004184 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004126 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-13 11:12:28,180] Trial 65 finished with value: 0.9066592396664062 and parameters: {'max_depth': 16, 'learning_rate': 0.15633609001634025}. Best is trial 12 with value: 0.9086056516749924.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004136 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003893 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002704 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-13 11:12:30,069] Trial 66 finished with value: 0.9064527813125993 and parameters: {'max_depth': 14, 'learning_rate': 0.0328937863175958}. Best is trial 12 with value: 0.9086056516749924.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002728 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002876 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002862 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-13 11:12:31,528] Trial 67 finished with value: 0.907986328800257 and parameters: {'max_depth': 12, 'learning_rate': 0.06737111632898399}. Best is trial 12 with value: 0.9086056516749924.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002688 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002736 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002875 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-13 11:12:32,955] Trial 68 finished with value: 0.9079863235815884 and parameters: {'max_depth': 10, 'learning_rate': 0.0798915327834015}. Best is trial 12 with value: 0.9086056516749924.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002872 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002867 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002860 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-13 11:12:34,292] Trial 69 finished with value: 0.9071901113287592 and parameters: {'max_depth': 11, 'learning_rate': 0.11798669677325781}. Best is trial 12 with value: 0.9086056516749924.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002723 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002798 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002871 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-13 11:12:35,769] Trial 70 finished with value: 0.9080747826219738 and parameters: {'max_depth': 11, 'learning_rate': 0.05010286630616612}. Best is trial 12 with value: 0.9086056516749924.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004030 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002837 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002804 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-13 11:12:37,181] Trial 71 finished with value: 0.9077799121957981 and parameters: {'max_depth': 13, 'learning_rate': 0.07824680541179654}. Best is trial 12 with value: 0.9086056516749924.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002722 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002818 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002842 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-13 11:12:38,569] Trial 72 finished with value: 0.9078683712361834 and parameters: {'max_depth': 14, 'learning_rate': 0.09567936171323643}. Best is trial 12 with value: 0.9086056516749924.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002790 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002862 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004134 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-13 11:12:40,339] Trial 73 finished with value: 0.9076913800940538 and parameters: {'max_depth': 12, 'learning_rate': 0.05709104570132408}. Best is trial 12 with value: 0.9086056516749924.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004058 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004105 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004006 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-13 11:12:42,042] Trial 74 finished with value: 0.9052141773124763 and parameters: {'max_depth': 13, 'learning_rate': 0.22002034271405793}. Best is trial 12 with value: 0.9086056516749924.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002695 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002837 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002732 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-13 11:12:43,462] Trial 75 finished with value: 0.9075144672319517 and parameters: {'max_depth': 10, 'learning_rate': 0.08123136700358066}. Best is trial 12 with value: 0.9086056516749924.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002747 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002837 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002852 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-13 11:12:44,929] Trial 76 finished with value: 0.9075144541852805 and parameters: {'max_depth': 8, 'learning_rate': 0.041173695371032173}. Best is trial 12 with value: 0.9086056516749924.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002848 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002864 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002881 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-13 11:12:46,223] Trial 77 finished with value: 0.9073670346281982 and parameters: {'max_depth': 15, 'learning_rate': 0.16138991150677534}. Best is trial 12 with value: 0.9086056516749924.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002784 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002823 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002816 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-13 11:12:47,568] Trial 78 finished with value: 0.9073965148865467 and parameters: {'max_depth': 12, 'learning_rate': 0.10764002405237551}. Best is trial 12 with value: 0.9086056516749924.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002719 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002821 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002822 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-13 11:12:48,992] Trial 79 finished with value: 0.9079273682835599 and parameters: {'max_depth': 11, 'learning_rate': 0.07157024415062294}. Best is trial 12 with value: 0.9086056516749924.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002781 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002877 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002839 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-13 11:12:50,456] Trial 80 finished with value: 0.9084286996728766 and parameters: {'max_depth': 13, 'learning_rate': 0.06084210089960696}. Best is trial 12 with value: 0.9086056516749924.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002753 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002795 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002820 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-13 11:12:51,972] Trial 81 finished with value: 0.9080158377612823 and parameters: {'max_depth': 13, 'learning_rate': 0.04899353580654856}. Best is trial 12 with value: 0.9086056516749924.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004151 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003936 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004087 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-13 11:12:54,086] Trial 82 finished with value: 0.9073669954881844 and parameters: {'max_depth': 14, 'learning_rate': 0.06450414073248696}. Best is trial 12 with value: 0.9086056516749924.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002848 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002733 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002844 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-13 11:12:55,584] Trial 83 finished with value: 0.9078093402674616 and parameters: {'max_depth': 13, 'learning_rate': 0.0581885479433368}. Best is trial 12 with value: 0.9086056516749924.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002712 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002837 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002811 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-13 11:12:56,973] Trial 84 finished with value: 0.9081927662793898 and parameters: {'max_depth': 10, 'learning_rate': 0.08808376918066217}. Best is trial 12 with value: 0.9086056516749924.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002824 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002855 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002746 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-13 11:12:58,468] Trial 85 finished with value: 0.9073375152298361 and parameters: {'max_depth': 10, 'learning_rate': 0.03646210119170438}. Best is trial 12 with value: 0.9086056516749924.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002825 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002691 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002722 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-13 11:12:59,860] Trial 86 finished with value: 0.9080748321993245 and parameters: {'max_depth': 10, 'learning_rate': 0.09025987517713509}. Best is trial 12 with value: 0.9086056516749924.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002795 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002836 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002720 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-13 11:13:01,200] Trial 87 finished with value: 0.9069836529749523 and parameters: {'max_depth': 9, 'learning_rate': 0.1304457631157945}. Best is trial 12 with value: 0.9086056516749924.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002696 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002753 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003012 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-13 11:13:02,683] Trial 88 finished with value: 0.9050666638193611 and parameters: {'max_depth': 11, 'learning_rate': 0.02409318274367889}. Best is trial 12 with value: 0.9086056516749924.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002960 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002734 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003968 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-13 11:13:04,178] Trial 89 finished with value: 0.9078978227918552 and parameters: {'max_depth': 9, 'learning_rate': 0.045648360255614785}. Best is trial 12 with value: 0.9086056516749924.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004041 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004055 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004033 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-13 11:13:06,113] Trial 90 finished with value: 0.9075734512326571 and parameters: {'max_depth': 12, 'learning_rate': 0.11155482676749623}. Best is trial 12 with value: 0.9086056516749924.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004132 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002857 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002752 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-13 11:13:07,697] Trial 91 finished with value: 0.9084582112432361 and parameters: {'max_depth': 12, 'learning_rate': 0.07395988788108121}. Best is trial 12 with value: 0.9086056516749924.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002798 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002797 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002776 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-13 11:13:08,833] Trial 92 finished with value: 0.9047128380951571 and parameters: {'max_depth': 4, 'learning_rate': 0.07111421872924154}. Best is trial 12 with value: 0.9086056516749924.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002712 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003047 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002737 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-13 11:13:10,306] Trial 93 finished with value: 0.9081632573183646 and parameters: {'max_depth': 11, 'learning_rate': 0.057900370909710064}. Best is trial 12 with value: 0.9086056516749924.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002861 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002809 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002733 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-13 11:13:11,704] Trial 94 finished with value: 0.9080748452459956 and parameters: {'max_depth': 12, 'learning_rate': 0.09264363682395975}. Best is trial 12 with value: 0.9086056516749924.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002785 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003008 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002776 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-13 11:13:13,030] Trial 95 finished with value: 0.8838032329432165 and parameters: {'max_depth': 11, 'learning_rate': 0.0026035003972086366}. Best is trial 12 with value: 0.9086056516749924.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002793 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002951 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002869 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-13 11:13:14,495] Trial 96 finished with value: 0.8991093815121644 and parameters: {'max_depth': 10, 'learning_rate': 0.016861335547901736}. Best is trial 12 with value: 0.9086056516749924.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002693 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002701 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002833 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-13 11:13:15,931] Trial 97 finished with value: 0.9075734721073311 and parameters: {'max_depth': 12, 'learning_rate': 0.06276711638619145}. Best is trial 12 with value: 0.9086056516749924.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002827 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002813 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003977 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-13 11:13:17,607] Trial 98 finished with value: 0.9068951652318903 and parameters: {'max_depth': 14, 'learning_rate': 0.10291097431845186}. Best is trial 12 with value: 0.9086056516749924.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19978\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004179 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 978\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116213 -> initscore=-2.028789\n",
            "[LightGBM] [Info] Start training from score -2.028789\n",
            "[LightGBM] [Info] Number of positive: 2626, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004165 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 980\n",
            "[LightGBM] [Info] Number of data points in the train set: 22605, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116169 -> initscore=-2.029220\n",
            "[LightGBM] [Info] Start training from score -2.029220\n",
            "[LightGBM] [Info] Number of positive: 2627, number of negative: 19979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002700 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 982\n",
            "[LightGBM] [Info] Number of data points in the train set: 22606, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116208 -> initscore=-2.028839\n",
            "[LightGBM] [Info] Start training from score -2.028839\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-13 11:13:19,425] Trial 99 finished with value: 0.8838032329432165 and parameters: {'max_depth': 10, 'learning_rate': 0.00551105620784358}. Best is trial 12 with value: 0.9086056516749924.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Random Forest CV Accuracy:\", study_rf.best_value)\n",
        "print(\"Best RF Params:\", study_rf.best_params)\n",
        "\n",
        "print(\"Gradient Boosting CV Accuracy:\", study_gb.best_value)\n",
        "print(\"Best GB Params:\", study_gb.best_params)\n",
        "\n",
        "print(\"XGBoost CV Accuracy:\", study_xgb.best_value)\n",
        "print(\"Best XGB Params:\", study_xgb.best_params)\n",
        "\n",
        "print(\"LightGBM CV Accuracy:\", study_lgb.best_value)\n",
        "print(\"Best LGBM Params:\", study_lgb.best_params)"
      ],
      "metadata": {
        "id": "vNMMnapdmphc",
        "outputId": "cfe487f9-82e5-4ea9-d578-dd1c58040f9c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest CV Accuracy: 0.9060104365333222\n",
            "Best RF Params: {'max_depth': 14, 'max_features': 10}\n",
            "Gradient Boosting CV Accuracy: 0.9067181740897606\n",
            "Best GB Params: {'max_depth': 5, 'learning_rate': 0.1164613636909911}\n",
            "XGBoost CV Accuracy: 0.907485005238943\n",
            "Best XGB Params: {'max_depth': 5, 'learning_rate': 0.12865417881274127}\n",
            "LightGBM CV Accuracy: 0.9086056516749924\n",
            "Best LGBM Params: {'max_depth': 12, 'learning_rate': 0.060251879695890825}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q2 b) evaluating best model on test data\n",
        "# Replace `study_lgb` with the actual best one\n",
        "best_lgb = LGBMClassifier(**study_lgb.best_params, random_state=17)\n",
        "best_lgb.fit(X_train, y_train)\n",
        "test_preds = best_lgb.predict(X_test)\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "test_accuracy = accuracy_score(y_test, test_preds)\n",
        "\n",
        "print(\"Test Accuracy of Best Tuned Model:\", test_accuracy)"
      ],
      "metadata": {
        "id": "oxdDj1B3mqne",
        "outputId": "075f4c8c-1821-4c2c-b37f-e29fc28f6a66",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 3940, number of negative: 29968\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004362 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 988\n",
            "[LightGBM] [Info] Number of data points in the train set: 33908, number of used features: 16\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116197 -> initscore=-2.028949\n",
            "[LightGBM] [Info] Start training from score -2.028949\n",
            "Test Accuracy of Best Tuned Model: 0.9050694505883394\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q2 a)**\n",
        "\n",
        "Using Optuna with **100 trials and 3-fold cross-validation**, the best models and their parameters were:\n",
        "\n",
        "**Random Forest** -> **CV Accuracy:** 0.9061\t**Best Parameters:** max_depth=13, max_features=6\n",
        "\n",
        "**Gradient Boosting** -> **CV Accuracy:**\t0.9067\t**Best Parameters:**  max_depth=4, learning_rate=0.2323\n",
        "\n",
        "**XGBoost** -> **CV Accuracy:**\t0.9082\tmax_depth=4, learning_rate=0.2350\n",
        "\n",
        "**LightGBM** -> **CV Accuracy:** 0.9095 **(Highest)**\t**Best Parameters:** max_depth=16, learning_rate=0.0771\n",
        "\n",
        "So, the best method based on highest cross-validation accuracy is:\n",
        "**LightGBM with max_depth=16 and learning_rate=0.0771**, and Cross-validation accuracy: **90.95%**.\n",
        "\n",
        "**Q2 b)**\n",
        "\n",
        "After training this best LightGBM model on the training set and evaluating it on the test set:\n",
        "\n",
        "Test Accuracy of Tuned LightGBM: 0.9048\n",
        "\n",
        "When we compare this with the default model performance (from Q1):\n",
        "\n",
        "LightGBM Default Accuracy: 0.9074\n",
        "\n",
        "Tuned LightGBM Accuracy: 0.9048\n",
        "\n",
        "**Finally,** although the tuned LightGBM model had the best cross-validation score, its test accuracy (90.48%) was slightly lower than the default model’s accuracy (90.74%).\n",
        "\n",
        "This might be due to:\n",
        "\n",
        "Slight overfitting to training folds during tuning\n",
        "\n",
        "Random fluctuations in test data performance\n",
        "\n",
        "Still, the difference is minimal, and both models perform very well overall."
      ],
      "metadata": {
        "id": "MIOlmVeMnP5N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For Q3 and Q4 ,use the following data."
      ],
      "metadata": {
        "id": "jR2k2_rfg5PD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dr=pd.read_csv('https://raw.githubusercontent.com/ogut77/DataScience/main/data/diamond.csv')\n",
        "dr"
      ],
      "metadata": {
        "id": "1XezD-d0fKRD",
        "outputId": "0ee06b0b-dba1-406f-e030-ebc638df2a7d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      Carat Weight              Cut Color Clarity Polish Symmetry Report  \\\n",
              "0             1.10            Ideal     H     SI1     VG       EX    GIA   \n",
              "1             0.83            Ideal     H     VS1     ID       ID   AGSL   \n",
              "2             0.85            Ideal     H     SI1     EX       EX    GIA   \n",
              "3             0.91            Ideal     E     SI1     VG       VG    GIA   \n",
              "4             0.83            Ideal     G     SI1     EX       EX    GIA   \n",
              "...            ...              ...   ...     ...    ...      ...    ...   \n",
              "5995          1.03            Ideal     D     SI1     EX       EX    GIA   \n",
              "5996          1.00        Very Good     D     SI1     VG       VG    GIA   \n",
              "5997          1.02            Ideal     D     SI1     EX       EX    GIA   \n",
              "5998          1.27  Signature-Ideal     G     VS1     EX       EX    GIA   \n",
              "5999          2.19            Ideal     E     VS1     EX       EX    GIA   \n",
              "\n",
              "      Price  \n",
              "0      5169  \n",
              "1      3470  \n",
              "2      3183  \n",
              "3      4370  \n",
              "4      3171  \n",
              "...     ...  \n",
              "5995   6250  \n",
              "5996   5328  \n",
              "5997   6157  \n",
              "5998  11206  \n",
              "5999  30507  \n",
              "\n",
              "[6000 rows x 8 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3bcda534-2765-41b9-bed4-44c5c9aa3090\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Carat Weight</th>\n",
              "      <th>Cut</th>\n",
              "      <th>Color</th>\n",
              "      <th>Clarity</th>\n",
              "      <th>Polish</th>\n",
              "      <th>Symmetry</th>\n",
              "      <th>Report</th>\n",
              "      <th>Price</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.10</td>\n",
              "      <td>Ideal</td>\n",
              "      <td>H</td>\n",
              "      <td>SI1</td>\n",
              "      <td>VG</td>\n",
              "      <td>EX</td>\n",
              "      <td>GIA</td>\n",
              "      <td>5169</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.83</td>\n",
              "      <td>Ideal</td>\n",
              "      <td>H</td>\n",
              "      <td>VS1</td>\n",
              "      <td>ID</td>\n",
              "      <td>ID</td>\n",
              "      <td>AGSL</td>\n",
              "      <td>3470</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.85</td>\n",
              "      <td>Ideal</td>\n",
              "      <td>H</td>\n",
              "      <td>SI1</td>\n",
              "      <td>EX</td>\n",
              "      <td>EX</td>\n",
              "      <td>GIA</td>\n",
              "      <td>3183</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.91</td>\n",
              "      <td>Ideal</td>\n",
              "      <td>E</td>\n",
              "      <td>SI1</td>\n",
              "      <td>VG</td>\n",
              "      <td>VG</td>\n",
              "      <td>GIA</td>\n",
              "      <td>4370</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.83</td>\n",
              "      <td>Ideal</td>\n",
              "      <td>G</td>\n",
              "      <td>SI1</td>\n",
              "      <td>EX</td>\n",
              "      <td>EX</td>\n",
              "      <td>GIA</td>\n",
              "      <td>3171</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5995</th>\n",
              "      <td>1.03</td>\n",
              "      <td>Ideal</td>\n",
              "      <td>D</td>\n",
              "      <td>SI1</td>\n",
              "      <td>EX</td>\n",
              "      <td>EX</td>\n",
              "      <td>GIA</td>\n",
              "      <td>6250</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5996</th>\n",
              "      <td>1.00</td>\n",
              "      <td>Very Good</td>\n",
              "      <td>D</td>\n",
              "      <td>SI1</td>\n",
              "      <td>VG</td>\n",
              "      <td>VG</td>\n",
              "      <td>GIA</td>\n",
              "      <td>5328</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5997</th>\n",
              "      <td>1.02</td>\n",
              "      <td>Ideal</td>\n",
              "      <td>D</td>\n",
              "      <td>SI1</td>\n",
              "      <td>EX</td>\n",
              "      <td>EX</td>\n",
              "      <td>GIA</td>\n",
              "      <td>6157</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5998</th>\n",
              "      <td>1.27</td>\n",
              "      <td>Signature-Ideal</td>\n",
              "      <td>G</td>\n",
              "      <td>VS1</td>\n",
              "      <td>EX</td>\n",
              "      <td>EX</td>\n",
              "      <td>GIA</td>\n",
              "      <td>11206</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5999</th>\n",
              "      <td>2.19</td>\n",
              "      <td>Ideal</td>\n",
              "      <td>E</td>\n",
              "      <td>VS1</td>\n",
              "      <td>EX</td>\n",
              "      <td>EX</td>\n",
              "      <td>GIA</td>\n",
              "      <td>30507</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>6000 rows × 8 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3bcda534-2765-41b9-bed4-44c5c9aa3090')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-3bcda534-2765-41b9-bed4-44c5c9aa3090 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-3bcda534-2765-41b9-bed4-44c5c9aa3090');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-254808ae-185b-4803-94a4-c0f75d619fae\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-254808ae-185b-4803-94a4-c0f75d619fae')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-254808ae-185b-4803-94a4-c0f75d619fae button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_eebee8f2-9218-49d2-8fea-844758fdebbf\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('dr')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_eebee8f2-9218-49d2-8fea-844758fdebbf button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('dr');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "dr",
              "summary": "{\n  \"name\": \"dr\",\n  \"rows\": 6000,\n  \"fields\": [\n    {\n      \"column\": \"Carat Weight\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.47569627995020364,\n        \"min\": 0.75,\n        \"max\": 2.91,\n        \"num_unique_values\": 196,\n        \"samples\": [\n          1.37,\n          2.56,\n          2.6\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Cut\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Very Good\",\n          \"Signature-Ideal\",\n          \"Fair\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Color\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"H\",\n          \"E\",\n          \"I\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Clarity\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 7,\n        \"samples\": [\n          \"SI1\",\n          \"VS1\",\n          \"IF\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Polish\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"ID\",\n          \"G\",\n          \"VG\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Symmetry\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"ID\",\n          \"G\",\n          \"EX\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Report\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"AGSL\",\n          \"GIA\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Price\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 10184,\n        \"min\": 2184,\n        \"max\": 101561,\n        \"num_unique_values\": 4821,\n        \"samples\": [\n          5497,\n          4273\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def Encoder(df):\n",
        "          from sklearn import preprocessing\n",
        "          columnsToEncode = list(df.select_dtypes(include=['category','object']))\n",
        "          le = preprocessing.LabelEncoder()\n",
        "          for feature in columnsToEncode:\n",
        "              try:\n",
        "                  df[feature] = le.fit_transform(df[feature])\n",
        "              except:\n",
        "                  print('Error encoding '+feature)\n",
        "          return df\n"
      ],
      "metadata": {
        "id": "zTuFQHpVfoxH"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dr=Encoder(dr)\n",
        "dr"
      ],
      "metadata": {
        "id": "nxX6G5tNgWOh",
        "outputId": "1980030f-1106-447f-81ff-485379037987",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      Carat Weight  Cut  Color  Clarity  Polish  Symmetry  Report  Price\n",
              "0             1.10    2      4        2       3         0       1   5169\n",
              "1             0.83    2      4        3       2         2       0   3470\n",
              "2             0.85    2      4        2       0         0       1   3183\n",
              "3             0.91    2      1        2       3         3       1   4370\n",
              "4             0.83    2      3        2       0         0       1   3171\n",
              "...            ...  ...    ...      ...     ...       ...     ...    ...\n",
              "5995          1.03    2      0        2       0         0       1   6250\n",
              "5996          1.00    4      0        2       3         3       1   5328\n",
              "5997          1.02    2      0        2       0         0       1   6157\n",
              "5998          1.27    3      3        3       0         0       1  11206\n",
              "5999          2.19    2      1        3       0         0       1  30507\n",
              "\n",
              "[6000 rows x 8 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-728e68ec-333a-42a7-b5cb-3c3cce9138c5\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Carat Weight</th>\n",
              "      <th>Cut</th>\n",
              "      <th>Color</th>\n",
              "      <th>Clarity</th>\n",
              "      <th>Polish</th>\n",
              "      <th>Symmetry</th>\n",
              "      <th>Report</th>\n",
              "      <th>Price</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.10</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>5169</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.83</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>3470</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.85</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>3183</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.91</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>4370</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.83</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>3171</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5995</th>\n",
              "      <td>1.03</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>6250</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5996</th>\n",
              "      <td>1.00</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>5328</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5997</th>\n",
              "      <td>1.02</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>6157</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5998</th>\n",
              "      <td>1.27</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>11206</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5999</th>\n",
              "      <td>2.19</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>30507</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>6000 rows × 8 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-728e68ec-333a-42a7-b5cb-3c3cce9138c5')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-728e68ec-333a-42a7-b5cb-3c3cce9138c5 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-728e68ec-333a-42a7-b5cb-3c3cce9138c5');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-e0b2c87d-13f9-4bff-8a52-e237a0bdcb9b\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e0b2c87d-13f9-4bff-8a52-e237a0bdcb9b')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-e0b2c87d-13f9-4bff-8a52-e237a0bdcb9b button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_4345cdd0-7bd7-4faa-951b-3b3edee535c6\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('dr')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_4345cdd0-7bd7-4faa-951b-3b3edee535c6 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('dr');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "dr",
              "summary": "{\n  \"name\": \"dr\",\n  \"rows\": 6000,\n  \"fields\": [\n    {\n      \"column\": \"Carat Weight\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.47569627995020364,\n        \"min\": 0.75,\n        \"max\": 2.91,\n        \"num_unique_values\": 196,\n        \"samples\": [\n          1.37,\n          2.56,\n          2.6\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Cut\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 4,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          4,\n          3,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Color\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 5,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          4,\n          1,\n          5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Clarity\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 6,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          2,\n          3,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Polish\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 3,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          2,\n          1,\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Symmetry\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 3,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          2,\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Report\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Price\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 10184,\n        \"min\": 2184,\n        \"max\": 101561,\n        \"num_unique_values\": 4821,\n        \"samples\": [\n          5497,\n          4273\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y = dr['Price'] #Output\n",
        "X = dr.drop('Price',axis=1)\n",
        "X"
      ],
      "metadata": {
        "id": "Kv7SnSAahe1-",
        "outputId": "093d2983-7707-4c91-f585-d066715463c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      Carat Weight  Cut  Color  Clarity  Polish  Symmetry  Report\n",
              "0             1.10    2      4        2       3         0       1\n",
              "1             0.83    2      4        3       2         2       0\n",
              "2             0.85    2      4        2       0         0       1\n",
              "3             0.91    2      1        2       3         3       1\n",
              "4             0.83    2      3        2       0         0       1\n",
              "...            ...  ...    ...      ...     ...       ...     ...\n",
              "5995          1.03    2      0        2       0         0       1\n",
              "5996          1.00    4      0        2       3         3       1\n",
              "5997          1.02    2      0        2       0         0       1\n",
              "5998          1.27    3      3        3       0         0       1\n",
              "5999          2.19    2      1        3       0         0       1\n",
              "\n",
              "[6000 rows x 7 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-76f32a48-4233-475b-9595-f3b70034c7ff\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Carat Weight</th>\n",
              "      <th>Cut</th>\n",
              "      <th>Color</th>\n",
              "      <th>Clarity</th>\n",
              "      <th>Polish</th>\n",
              "      <th>Symmetry</th>\n",
              "      <th>Report</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.10</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.83</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.85</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.91</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.83</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5995</th>\n",
              "      <td>1.03</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5996</th>\n",
              "      <td>1.00</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5997</th>\n",
              "      <td>1.02</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5998</th>\n",
              "      <td>1.27</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5999</th>\n",
              "      <td>2.19</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>6000 rows × 7 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-76f32a48-4233-475b-9595-f3b70034c7ff')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-76f32a48-4233-475b-9595-f3b70034c7ff button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-76f32a48-4233-475b-9595-f3b70034c7ff');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-cf681518-2d43-4102-9b8c-8633d7cfc0ef\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-cf681518-2d43-4102-9b8c-8633d7cfc0ef')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-cf681518-2d43-4102-9b8c-8633d7cfc0ef button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_8569da5d-2b5d-4076-a909-f29a91c49b8e\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('X')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_8569da5d-2b5d-4076-a909-f29a91c49b8e button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('X');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "X",
              "summary": "{\n  \"name\": \"X\",\n  \"rows\": 6000,\n  \"fields\": [\n    {\n      \"column\": \"Carat Weight\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.47569627995020364,\n        \"min\": 0.75,\n        \"max\": 2.91,\n        \"num_unique_values\": 196,\n        \"samples\": [\n          1.37,\n          2.56,\n          2.6\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Cut\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 4,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          4,\n          3,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Color\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 5,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          4,\n          1,\n          5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Clarity\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 6,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          2,\n          3,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Polish\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 3,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          2,\n          1,\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Symmetry\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 3,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          2,\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Report\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import  train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=17)"
      ],
      "metadata": {
        "id": "KOLT3hfbgz7F"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3)Using Linear Regression,Decison Tree Random Forest,XGBoost, Light GBM and Gradient Boosting Classifier with default parameters (no parameter specifications except random_state) calculate R2 statistics on test data. Which method gives the best accuracy on test data"
      ],
      "metadata": {
        "id": "GzkaMEVGZ0pS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Q3\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from lightgbm import LGBMRegressor\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Initialize models\n",
        "models = {\n",
        "    'Linear Regression': LinearRegression(),\n",
        "    'Decision Tree': DecisionTreeRegressor(random_state=17),\n",
        "    'Random Forest': RandomForestRegressor(random_state=17),\n",
        "    'Gradient Boosting': GradientBoostingRegressor(random_state=17),\n",
        "    'XGBoost': XGBRegressor(random_state=17),\n",
        "    'LightGBM': LGBMRegressor(random_state=17)\n",
        "}\n",
        "\n",
        "# Fit, predict, and evaluate R² score\n",
        "r2_scores = {}\n",
        "for name, model in models.items():\n",
        "    model.fit(X_train, y_train)\n",
        "    preds = model.predict(X_test)\n",
        "    score = r2_score(y_test, preds)\n",
        "    r2_scores[name] = score\n",
        "    print(f\"{name} R² Score: {score:.4f}\")\n",
        "\n",
        "# Best model\n",
        "best_model = max(r2_scores, key=r2_scores.get)\n",
        "print(f\"\\n✅ Best model based on R² score: {best_model} with score: {r2_scores[best_model]:.4f}\")\n"
      ],
      "metadata": {
        "id": "Wr7g6UP--ErZ",
        "outputId": "80e61389-ece2-4131-8e19-48429d376d68",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear Regression R² Score: 0.8234\n",
            "Decision Tree R² Score: 0.9592\n",
            "Random Forest R² Score: 0.9800\n",
            "Gradient Boosting R² Score: 0.9739\n",
            "XGBoost R² Score: 0.9809\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000194 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 193\n",
            "[LightGBM] [Info] Number of data points in the train set: 4500, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score 11827.946667\n",
            "LightGBM R² Score: 0.9813\n",
            "\n",
            "✅ Best model based on R² score: LightGBM with score: 0.9813\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q3:**\n",
        "\n",
        "After applying Linear Regression, Decision Tree, Random Forest, XGBoost, LightGBM, and Gradient Boosting on the diamond dataset, the best performing model on the test data in terms of R² score was:\n",
        "\n",
        "**LightGBM** with R² Score ≈ **0.9807**"
      ],
      "metadata": {
        "id": "zzhU53Do-NqL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4) Using optuna hyperparmeter optimization technique (100 trial)  with Random Forest,XGBoost, Light GBM and Gradient Boosting Regressor\n",
        "\n",
        "\n",
        "a)find best methods with  parameters  using Cross validation (CV=3) technique for the range of   parameters below. What are the best parameters for the method with highest cross validation R2?\n",
        "\n",
        "For random forest\n",
        "\n",
        "\n",
        "  \"max_depth\"   : trial.suggest_int(\"max_depth\", 2,  X_train.shape[1]),\n",
        "  \"max_features\": trial.suggest_int(\"max_features\", 2, X_train.shape[1])\n",
        "\n",
        "For XGBoost, Light GBM and Gradient Boosting Classifier\n",
        "\n",
        "  \"max_depth\": trial.suggest_int(\"max_depth\", 2, X_train.shape[1]),\n",
        "  \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.001, 0.3,log=True)\n",
        "\n",
        "where X_train.shape[1] is number of columnns in the train data.\n",
        "\n",
        " b)Evaluate the performance of the  method with highest cross validation R2 on test data. What is the R2 value? Are there any improvement of the same method with default parameters?\n"
      ],
      "metadata": {
        "id": "nksEpGl5bDv3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Q4\n",
        "\n",
        "import pandas as pd\n",
        "import optuna\n",
        "from sklearn.model_selection import cross_val_score, train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from lightgbm import LGBMRegressor\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Load dataset\n",
        "dr = pd.read_csv('https://raw.githubusercontent.com/ogut77/DataScience/main/data/diamond.csv')\n",
        "\n",
        "# Label encode categorical features\n",
        "for col in dr.select_dtypes(include=['object']).columns:\n",
        "    dr[col] = LabelEncoder().fit_transform(dr[col])\n",
        "\n",
        "# Split into features and target\n",
        "X = dr.drop(\"Price\", axis=1)\n",
        "y = dr[\"Price\"]\n",
        "\n",
        "# Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=17)\n",
        "\n",
        "# Dictionary to hold best scores and parameters\n",
        "best_scores = {}\n",
        "\n",
        "#Random Forest\n",
        "def rf_objective(trial):\n",
        "    model = RandomForestRegressor(\n",
        "        max_depth=trial.suggest_int(\"max_depth\", 2, X_train.shape[1]),\n",
        "        max_features=trial.suggest_int(\"max_features\", 2, X_train.shape[1]),\n",
        "        random_state=17\n",
        "    )\n",
        "    return cross_val_score(model, X_train, y_train, scoring=\"r2\", cv=3).mean()\n",
        "\n",
        "rf_study = optuna.create_study(direction=\"maximize\")\n",
        "rf_study.optimize(rf_objective, n_trials=100)\n",
        "best_scores[\"Random Forest\"] = (rf_study.best_value, rf_study.best_params)\n",
        "\n",
        "#Gradient Boosting\n",
        "def gb_objective(trial):\n",
        "    model = GradientBoostingRegressor(\n",
        "        max_depth=trial.suggest_int(\"max_depth\", 2, X_train.shape[1]),\n",
        "        learning_rate=trial.suggest_float(\"learning_rate\", 0.001, 0.3, log=True),\n",
        "        random_state=17\n",
        "    )\n",
        "    return cross_val_score(model, X_train, y_train, scoring=\"r2\", cv=3).mean()\n",
        "\n",
        "gb_study = optuna.create_study(direction=\"maximize\")\n",
        "gb_study.optimize(gb_objective, n_trials=100)\n",
        "best_scores[\"Gradient Boosting\"] = (gb_study.best_value, gb_study.best_params)\n",
        "\n",
        "#XGBoost\n",
        "def xgb_objective(trial):\n",
        "    model = XGBRegressor(\n",
        "        max_depth=trial.suggest_int(\"max_depth\", 2, X_train.shape[1]),\n",
        "        learning_rate=trial.suggest_float(\"learning_rate\", 0.001, 0.3, log=True),\n",
        "        random_state=17,\n",
        "        verbosity=0\n",
        "    )\n",
        "    return cross_val_score(model, X_train, y_train, scoring=\"r2\", cv=3).mean()\n",
        "\n",
        "xgb_study = optuna.create_study(direction=\"maximize\")\n",
        "xgb_study.optimize(xgb_objective, n_trials=100)\n",
        "best_scores[\"XGBoost\"] = (xgb_study.best_value, xgb_study.best_params)\n",
        "\n",
        "#LightGBM\n",
        "def lgbm_objective(trial):\n",
        "    model = LGBMRegressor(\n",
        "        max_depth=trial.suggest_int(\"max_depth\", 2, X_train.shape[1]),\n",
        "        learning_rate=trial.suggest_float(\"learning_rate\", 0.001, 0.3, log=True),\n",
        "        random_state=17,\n",
        "        verbose=-1\n",
        "    )\n",
        "    return cross_val_score(model, X_train, y_train, scoring=\"r2\", cv=3).mean()\n",
        "\n",
        "lgbm_study = optuna.create_study(direction=\"maximize\")\n",
        "lgbm_study.optimize(lgbm_objective, n_trials=100)\n",
        "best_scores[\"LightGBM\"] = (lgbm_study.best_value, lgbm_study.best_params)\n",
        "\n",
        "# Print the best model\n",
        "best_model_name = max(best_scores, key=lambda k: best_scores[k][0])\n",
        "best_model_score, best_model_params = best_scores[best_model_name]\n",
        "print(f\"✅ Best Model: {best_model_name}\")\n",
        "print(f\"CV R2 Score: {best_model_score}\")\n",
        "print(f\"Best Parameters: {best_model_params}\")\n"
      ],
      "metadata": {
        "id": "CuPxIplb-l7I",
        "outputId": "42a318c1-38f5-46d3-e769-a40ecb8dcd2a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-04-13 11:48:41,057] A new study created in memory with name: no-name-6b967c13-d918-4a52-966e-e4964f2e96f1\n",
            "[I 2025-04-13 11:48:42,273] Trial 0 finished with value: 0.9073338809892725 and parameters: {'max_depth': 4, 'max_features': 7}. Best is trial 0 with value: 0.9073338809892725.\n",
            "[I 2025-04-13 11:48:43,581] Trial 1 finished with value: 0.9634604096785503 and parameters: {'max_depth': 6, 'max_features': 5}. Best is trial 1 with value: 0.9634604096785503.\n",
            "[I 2025-04-13 11:48:44,215] Trial 2 finished with value: 0.8938298760966701 and parameters: {'max_depth': 6, 'max_features': 2}. Best is trial 1 with value: 0.9634604096785503.\n",
            "[I 2025-04-13 11:48:44,961] Trial 3 finished with value: 0.9407314378830723 and parameters: {'max_depth': 5, 'max_features': 4}. Best is trial 1 with value: 0.9634604096785503.\n",
            "[I 2025-04-13 11:48:45,539] Trial 4 finished with value: 0.7819324835226356 and parameters: {'max_depth': 4, 'max_features': 2}. Best is trial 1 with value: 0.9634604096785503.\n",
            "[I 2025-04-13 11:48:46,433] Trial 5 finished with value: 0.9634604096785503 and parameters: {'max_depth': 6, 'max_features': 5}. Best is trial 1 with value: 0.9634604096785503.\n",
            "[I 2025-04-13 11:48:47,228] Trial 6 finished with value: 0.9571036896251935 and parameters: {'max_depth': 6, 'max_features': 4}. Best is trial 1 with value: 0.9634604096785503.\n",
            "[I 2025-04-13 11:48:47,874] Trial 7 finished with value: 0.8938298760966701 and parameters: {'max_depth': 6, 'max_features': 2}. Best is trial 1 with value: 0.9634604096785503.\n",
            "[I 2025-04-13 11:48:48,577] Trial 8 finished with value: 0.9111873329071347 and parameters: {'max_depth': 7, 'max_features': 2}. Best is trial 1 with value: 0.9634604096785503.\n",
            "[I 2025-04-13 11:48:49,192] Trial 9 finished with value: 0.8437742151001094 and parameters: {'max_depth': 5, 'max_features': 2}. Best is trial 1 with value: 0.9634604096785503.\n",
            "[I 2025-04-13 11:48:49,839] Trial 10 finished with value: 0.77614653774809 and parameters: {'max_depth': 2, 'max_features': 6}. Best is trial 1 with value: 0.9634604096785503.\n",
            "[I 2025-04-13 11:48:50,835] Trial 11 finished with value: 0.9720609785033627 and parameters: {'max_depth': 7, 'max_features': 5}. Best is trial 11 with value: 0.9720609785033627.\n",
            "[I 2025-04-13 11:48:51,823] Trial 12 finished with value: 0.9720609785033627 and parameters: {'max_depth': 7, 'max_features': 5}. Best is trial 11 with value: 0.9720609785033627.\n",
            "[I 2025-04-13 11:48:52,905] Trial 13 finished with value: 0.9720558623263608 and parameters: {'max_depth': 7, 'max_features': 6}. Best is trial 11 with value: 0.9720609785033627.\n",
            "[I 2025-04-13 11:48:53,982] Trial 14 finished with value: 0.9684653578103969 and parameters: {'max_depth': 7, 'max_features': 4}. Best is trial 11 with value: 0.9720609785033627.\n",
            "[I 2025-04-13 11:48:54,922] Trial 15 finished with value: 0.77614653774809 and parameters: {'max_depth': 2, 'max_features': 6}. Best is trial 11 with value: 0.9720609785033627.\n",
            "[I 2025-04-13 11:48:55,883] Trial 16 finished with value: 0.8639582097697365 and parameters: {'max_depth': 3, 'max_features': 5}. Best is trial 11 with value: 0.9720609785033627.\n",
            "[I 2025-04-13 11:48:56,686] Trial 17 finished with value: 0.9533571582401862 and parameters: {'max_depth': 7, 'max_features': 3}. Best is trial 11 with value: 0.9720609785033627.\n",
            "[I 2025-04-13 11:48:57,623] Trial 18 finished with value: 0.9389705102712522 and parameters: {'max_depth': 5, 'max_features': 7}. Best is trial 11 with value: 0.9720609785033627.\n",
            "[I 2025-04-13 11:48:58,435] Trial 19 finished with value: 0.9533571582401862 and parameters: {'max_depth': 7, 'max_features': 3}. Best is trial 11 with value: 0.9720609785033627.\n",
            "[I 2025-04-13 11:48:59,132] Trial 20 finished with value: 0.8660534697962478 and parameters: {'max_depth': 3, 'max_features': 6}. Best is trial 11 with value: 0.9720609785033627.\n",
            "[I 2025-04-13 11:49:00,215] Trial 21 finished with value: 0.9720558623263608 and parameters: {'max_depth': 7, 'max_features': 6}. Best is trial 11 with value: 0.9720609785033627.\n",
            "[I 2025-04-13 11:49:01,204] Trial 22 finished with value: 0.9720609785033627 and parameters: {'max_depth': 7, 'max_features': 5}. Best is trial 11 with value: 0.9720609785033627.\n",
            "[I 2025-04-13 11:49:02,101] Trial 23 finished with value: 0.9634604096785503 and parameters: {'max_depth': 6, 'max_features': 5}. Best is trial 11 with value: 0.9720609785033627.\n",
            "[I 2025-04-13 11:49:03,098] Trial 24 finished with value: 0.9720609785033627 and parameters: {'max_depth': 7, 'max_features': 5}. Best is trial 11 with value: 0.9720609785033627.\n",
            "[I 2025-04-13 11:49:03,987] Trial 25 finished with value: 0.9684653578103969 and parameters: {'max_depth': 7, 'max_features': 4}. Best is trial 11 with value: 0.9720609785033627.\n",
            "[I 2025-04-13 11:49:04,718] Trial 26 finished with value: 0.9375844290578602 and parameters: {'max_depth': 6, 'max_features': 3}. Best is trial 11 with value: 0.9720609785033627.\n",
            "[I 2025-04-13 11:49:05,529] Trial 27 finished with value: 0.9459607335237087 and parameters: {'max_depth': 5, 'max_features': 5}. Best is trial 11 with value: 0.9720609785033627.\n",
            "[I 2025-04-13 11:49:06,753] Trial 28 finished with value: 0.9684653578103969 and parameters: {'max_depth': 7, 'max_features': 4}. Best is trial 11 with value: 0.9720609785033627.\n",
            "[I 2025-04-13 11:49:08,005] Trial 29 finished with value: 0.9073338809892725 and parameters: {'max_depth': 4, 'max_features': 7}. Best is trial 11 with value: 0.9720609785033627.\n",
            "[I 2025-04-13 11:49:08,992] Trial 30 finished with value: 0.9633435487968379 and parameters: {'max_depth': 6, 'max_features': 6}. Best is trial 11 with value: 0.9720609785033627.\n",
            "[I 2025-04-13 11:49:09,976] Trial 31 finished with value: 0.9720609785033627 and parameters: {'max_depth': 7, 'max_features': 5}. Best is trial 11 with value: 0.9720609785033627.\n",
            "[I 2025-04-13 11:49:10,951] Trial 32 finished with value: 0.9720609785033627 and parameters: {'max_depth': 7, 'max_features': 5}. Best is trial 11 with value: 0.9720609785033627.\n",
            "[I 2025-04-13 11:49:11,841] Trial 33 finished with value: 0.9634604096785503 and parameters: {'max_depth': 6, 'max_features': 5}. Best is trial 11 with value: 0.9720609785033627.\n",
            "[I 2025-04-13 11:49:12,729] Trial 34 finished with value: 0.9684653578103969 and parameters: {'max_depth': 7, 'max_features': 4}. Best is trial 11 with value: 0.9720609785033627.\n",
            "[I 2025-04-13 11:49:13,618] Trial 35 finished with value: 0.9634604096785503 and parameters: {'max_depth': 6, 'max_features': 5}. Best is trial 11 with value: 0.9720609785033627.\n",
            "[I 2025-04-13 11:49:14,691] Trial 36 finished with value: 0.9720558623263608 and parameters: {'max_depth': 7, 'max_features': 6}. Best is trial 11 with value: 0.9720609785033627.\n",
            "[I 2025-04-13 11:49:15,504] Trial 37 finished with value: 0.9459607335237087 and parameters: {'max_depth': 5, 'max_features': 5}. Best is trial 11 with value: 0.9720609785033627.\n",
            "[I 2025-04-13 11:49:16,305] Trial 38 finished with value: 0.9571036896251935 and parameters: {'max_depth': 6, 'max_features': 4}. Best is trial 11 with value: 0.9720609785033627.\n",
            "[I 2025-04-13 11:49:17,284] Trial 39 finished with value: 0.9720609785033627 and parameters: {'max_depth': 7, 'max_features': 5}. Best is trial 11 with value: 0.9720609785033627.\n",
            "[I 2025-04-13 11:49:18,091] Trial 40 finished with value: 0.9571036896251935 and parameters: {'max_depth': 6, 'max_features': 4}. Best is trial 11 with value: 0.9720609785033627.\n",
            "[I 2025-04-13 11:49:19,568] Trial 41 finished with value: 0.9720609785033627 and parameters: {'max_depth': 7, 'max_features': 5}. Best is trial 11 with value: 0.9720609785033627.\n",
            "[I 2025-04-13 11:49:20,871] Trial 42 finished with value: 0.9720609785033627 and parameters: {'max_depth': 7, 'max_features': 5}. Best is trial 11 with value: 0.9720609785033627.\n",
            "[I 2025-04-13 11:49:21,858] Trial 43 finished with value: 0.9720609785033627 and parameters: {'max_depth': 7, 'max_features': 5}. Best is trial 11 with value: 0.9720609785033627.\n",
            "[I 2025-04-13 11:49:22,822] Trial 44 finished with value: 0.9633435487968379 and parameters: {'max_depth': 6, 'max_features': 6}. Best is trial 11 with value: 0.9720609785033627.\n",
            "[I 2025-04-13 11:49:23,804] Trial 45 finished with value: 0.9720609785033627 and parameters: {'max_depth': 7, 'max_features': 5}. Best is trial 11 with value: 0.9720609785033627.\n",
            "[I 2025-04-13 11:49:24,689] Trial 46 finished with value: 0.9684653578103969 and parameters: {'max_depth': 7, 'max_features': 4}. Best is trial 11 with value: 0.9720609785033627.\n",
            "[I 2025-04-13 11:49:25,654] Trial 47 finished with value: 0.9633435487968379 and parameters: {'max_depth': 6, 'max_features': 6}. Best is trial 11 with value: 0.9720609785033627.\n",
            "[I 2025-04-13 11:49:26,631] Trial 48 finished with value: 0.9720609785033627 and parameters: {'max_depth': 7, 'max_features': 5}. Best is trial 11 with value: 0.9720609785033627.\n",
            "[I 2025-04-13 11:49:27,317] Trial 49 finished with value: 0.8660534697962478 and parameters: {'max_depth': 3, 'max_features': 6}. Best is trial 11 with value: 0.9720609785033627.\n",
            "[I 2025-04-13 11:49:28,063] Trial 50 finished with value: 0.9407314378830723 and parameters: {'max_depth': 5, 'max_features': 4}. Best is trial 11 with value: 0.9720609785033627.\n",
            "[I 2025-04-13 11:49:29,056] Trial 51 finished with value: 0.9720609785033627 and parameters: {'max_depth': 7, 'max_features': 5}. Best is trial 11 with value: 0.9720609785033627.\n",
            "[I 2025-04-13 11:49:30,037] Trial 52 finished with value: 0.9720609785033627 and parameters: {'max_depth': 7, 'max_features': 5}. Best is trial 11 with value: 0.9720609785033627.\n",
            "[I 2025-04-13 11:49:31,306] Trial 53 finished with value: 0.9720609785033627 and parameters: {'max_depth': 7, 'max_features': 5}. Best is trial 11 with value: 0.9720609785033627.\n",
            "[I 2025-04-13 11:49:32,769] Trial 54 finished with value: 0.9720609785033627 and parameters: {'max_depth': 7, 'max_features': 5}. Best is trial 11 with value: 0.9720609785033627.\n",
            "[I 2025-04-13 11:49:33,843] Trial 55 finished with value: 0.9720558623263608 and parameters: {'max_depth': 7, 'max_features': 6}. Best is trial 11 with value: 0.9720609785033627.\n",
            "[I 2025-04-13 11:49:34,734] Trial 56 finished with value: 0.9634604096785503 and parameters: {'max_depth': 6, 'max_features': 5}. Best is trial 11 with value: 0.9720609785033627.\n",
            "[I 2025-04-13 11:49:35,633] Trial 57 finished with value: 0.9684653578103969 and parameters: {'max_depth': 7, 'max_features': 4}. Best is trial 11 with value: 0.9720609785033627.\n",
            "[I 2025-04-13 11:49:36,619] Trial 58 finished with value: 0.9720609785033627 and parameters: {'max_depth': 7, 'max_features': 5}. Best is trial 11 with value: 0.9720609785033627.\n",
            "[I 2025-04-13 11:49:37,602] Trial 59 finished with value: 0.9633435487968379 and parameters: {'max_depth': 6, 'max_features': 6}. Best is trial 11 with value: 0.9720609785033627.\n",
            "[I 2025-04-13 11:49:38,594] Trial 60 finished with value: 0.9720609785033627 and parameters: {'max_depth': 7, 'max_features': 5}. Best is trial 11 with value: 0.9720609785033627.\n",
            "[I 2025-04-13 11:49:39,583] Trial 61 finished with value: 0.9720609785033627 and parameters: {'max_depth': 7, 'max_features': 5}. Best is trial 11 with value: 0.9720609785033627.\n",
            "[I 2025-04-13 11:49:40,577] Trial 62 finished with value: 0.9720609785033627 and parameters: {'max_depth': 7, 'max_features': 5}. Best is trial 11 with value: 0.9720609785033627.\n",
            "[I 2025-04-13 11:49:41,568] Trial 63 finished with value: 0.9720609785033627 and parameters: {'max_depth': 7, 'max_features': 5}. Best is trial 11 with value: 0.9720609785033627.\n",
            "[I 2025-04-13 11:49:42,552] Trial 64 finished with value: 0.9720609785033627 and parameters: {'max_depth': 7, 'max_features': 5}. Best is trial 11 with value: 0.9720609785033627.\n",
            "[I 2025-04-13 11:49:43,826] Trial 65 finished with value: 0.9684653578103969 and parameters: {'max_depth': 7, 'max_features': 4}. Best is trial 11 with value: 0.9720609785033627.\n",
            "[I 2025-04-13 11:49:45,110] Trial 66 finished with value: 0.9634604096785503 and parameters: {'max_depth': 6, 'max_features': 5}. Best is trial 11 with value: 0.9720609785033627.\n",
            "[I 2025-04-13 11:49:45,903] Trial 67 finished with value: 0.9153488693549434 and parameters: {'max_depth': 4, 'max_features': 6}. Best is trial 11 with value: 0.9720609785033627.\n",
            "[I 2025-04-13 11:49:46,795] Trial 68 finished with value: 0.9634604096785503 and parameters: {'max_depth': 6, 'max_features': 5}. Best is trial 11 with value: 0.9720609785033627.\n",
            "[I 2025-04-13 11:49:47,678] Trial 69 finished with value: 0.9684653578103969 and parameters: {'max_depth': 7, 'max_features': 4}. Best is trial 11 with value: 0.9720609785033627.\n",
            "[I 2025-04-13 11:49:48,247] Trial 70 finished with value: 0.7739509743309535 and parameters: {'max_depth': 2, 'max_features': 5}. Best is trial 11 with value: 0.9720609785033627.\n",
            "[I 2025-04-13 11:49:49,220] Trial 71 finished with value: 0.9720609785033627 and parameters: {'max_depth': 7, 'max_features': 5}. Best is trial 11 with value: 0.9720609785033627.\n",
            "[I 2025-04-13 11:49:50,236] Trial 72 finished with value: 0.9720609785033627 and parameters: {'max_depth': 7, 'max_features': 5}. Best is trial 11 with value: 0.9720609785033627.\n",
            "[I 2025-04-13 11:49:51,216] Trial 73 finished with value: 0.9720609785033627 and parameters: {'max_depth': 7, 'max_features': 5}. Best is trial 11 with value: 0.9720609785033627.\n",
            "[I 2025-04-13 11:49:52,200] Trial 74 finished with value: 0.9720609785033627 and parameters: {'max_depth': 7, 'max_features': 5}. Best is trial 11 with value: 0.9720609785033627.\n",
            "[I 2025-04-13 11:49:53,277] Trial 75 finished with value: 0.9720558623263608 and parameters: {'max_depth': 7, 'max_features': 6}. Best is trial 11 with value: 0.9720609785033627.\n",
            "[I 2025-04-13 11:49:54,254] Trial 76 finished with value: 0.9720609785033627 and parameters: {'max_depth': 7, 'max_features': 5}. Best is trial 11 with value: 0.9720609785033627.\n",
            "[I 2025-04-13 11:49:55,140] Trial 77 finished with value: 0.9571036896251935 and parameters: {'max_depth': 6, 'max_features': 4}. Best is trial 11 with value: 0.9720609785033627.\n",
            "[I 2025-04-13 11:49:56,622] Trial 78 finished with value: 0.9720609785033627 and parameters: {'max_depth': 7, 'max_features': 5}. Best is trial 11 with value: 0.9720609785033627.\n",
            "[I 2025-04-13 11:49:57,910] Trial 79 finished with value: 0.9720558623263608 and parameters: {'max_depth': 7, 'max_features': 6}. Best is trial 11 with value: 0.9720609785033627.\n",
            "[I 2025-04-13 11:49:58,737] Trial 80 finished with value: 0.9571036896251935 and parameters: {'max_depth': 6, 'max_features': 4}. Best is trial 11 with value: 0.9720609785033627.\n",
            "[I 2025-04-13 11:49:59,721] Trial 81 finished with value: 0.9720609785033627 and parameters: {'max_depth': 7, 'max_features': 5}. Best is trial 11 with value: 0.9720609785033627.\n",
            "[I 2025-04-13 11:50:00,702] Trial 82 finished with value: 0.9720609785033627 and parameters: {'max_depth': 7, 'max_features': 5}. Best is trial 11 with value: 0.9720609785033627.\n",
            "[I 2025-04-13 11:50:01,695] Trial 83 finished with value: 0.9720609785033627 and parameters: {'max_depth': 7, 'max_features': 5}. Best is trial 11 with value: 0.9720609785033627.\n",
            "[I 2025-04-13 11:50:02,693] Trial 84 finished with value: 0.9720609785033627 and parameters: {'max_depth': 7, 'max_features': 5}. Best is trial 11 with value: 0.9720609785033627.\n",
            "[I 2025-04-13 11:50:03,680] Trial 85 finished with value: 0.9720609785033627 and parameters: {'max_depth': 7, 'max_features': 5}. Best is trial 11 with value: 0.9720609785033627.\n",
            "[I 2025-04-13 11:50:04,669] Trial 86 finished with value: 0.9720609785033627 and parameters: {'max_depth': 7, 'max_features': 5}. Best is trial 11 with value: 0.9720609785033627.\n",
            "[I 2025-04-13 11:50:05,571] Trial 87 finished with value: 0.9634604096785503 and parameters: {'max_depth': 6, 'max_features': 5}. Best is trial 11 with value: 0.9720609785033627.\n",
            "[I 2025-04-13 11:50:06,656] Trial 88 finished with value: 0.9720558623263608 and parameters: {'max_depth': 7, 'max_features': 6}. Best is trial 11 with value: 0.9720609785033627.\n",
            "[I 2025-04-13 11:50:07,875] Trial 89 finished with value: 0.9720609785033627 and parameters: {'max_depth': 7, 'max_features': 5}. Best is trial 11 with value: 0.9720609785033627.\n",
            "[I 2025-04-13 11:50:09,258] Trial 90 finished with value: 0.9684653578103969 and parameters: {'max_depth': 7, 'max_features': 4}. Best is trial 11 with value: 0.9720609785033627.\n",
            "[I 2025-04-13 11:50:10,341] Trial 91 finished with value: 0.9720609785033627 and parameters: {'max_depth': 7, 'max_features': 5}. Best is trial 11 with value: 0.9720609785033627.\n",
            "[I 2025-04-13 11:50:11,339] Trial 92 finished with value: 0.9720609785033627 and parameters: {'max_depth': 7, 'max_features': 5}. Best is trial 11 with value: 0.9720609785033627.\n",
            "[I 2025-04-13 11:50:12,333] Trial 93 finished with value: 0.9720609785033627 and parameters: {'max_depth': 7, 'max_features': 5}. Best is trial 11 with value: 0.9720609785033627.\n",
            "[I 2025-04-13 11:50:13,325] Trial 94 finished with value: 0.9720609785033627 and parameters: {'max_depth': 7, 'max_features': 5}. Best is trial 11 with value: 0.9720609785033627.\n",
            "[I 2025-04-13 11:50:14,318] Trial 95 finished with value: 0.9720609785033627 and parameters: {'max_depth': 7, 'max_features': 5}. Best is trial 11 with value: 0.9720609785033627.\n",
            "[I 2025-04-13 11:50:15,307] Trial 96 finished with value: 0.9720609785033627 and parameters: {'max_depth': 7, 'max_features': 5}. Best is trial 11 with value: 0.9720609785033627.\n",
            "[I 2025-04-13 11:50:15,965] Trial 97 finished with value: 0.8938298760966701 and parameters: {'max_depth': 6, 'max_features': 2}. Best is trial 11 with value: 0.9720609785033627.\n",
            "[I 2025-04-13 11:50:16,947] Trial 98 finished with value: 0.9720609785033627 and parameters: {'max_depth': 7, 'max_features': 5}. Best is trial 11 with value: 0.9720609785033627.\n",
            "[I 2025-04-13 11:50:17,736] Trial 99 finished with value: 0.9153488693549434 and parameters: {'max_depth': 4, 'max_features': 6}. Best is trial 11 with value: 0.9720609785033627.\n",
            "[I 2025-04-13 11:50:17,738] A new study created in memory with name: no-name-6e4efc51-b610-4f6e-ac44-eb5ec56e27b3\n",
            "[I 2025-04-13 11:50:18,443] Trial 0 finished with value: 0.9600702996099217 and parameters: {'max_depth': 3, 'learning_rate': 0.04025421824586969}. Best is trial 0 with value: 0.9600702996099217.\n",
            "[I 2025-04-13 11:50:19,720] Trial 1 finished with value: 0.9680731086931571 and parameters: {'max_depth': 6, 'learning_rate': 0.02411130848637788}. Best is trial 1 with value: 0.9680731086931571.\n",
            "[I 2025-04-13 11:50:20,517] Trial 2 finished with value: 0.32046018381874725 and parameters: {'max_depth': 2, 'learning_rate': 0.0027358517984536687}. Best is trial 1 with value: 0.9680731086931571.\n",
            "[I 2025-04-13 11:50:21,742] Trial 3 finished with value: 0.46430711398470814 and parameters: {'max_depth': 4, 'learning_rate': 0.003660111289733044}. Best is trial 1 with value: 0.9680731086931571.\n",
            "[I 2025-04-13 11:50:22,953] Trial 4 finished with value: 0.5945018298089619 and parameters: {'max_depth': 6, 'learning_rate': 0.004925044477229136}. Best is trial 1 with value: 0.9680731086931571.\n",
            "[I 2025-04-13 11:50:23,823] Trial 5 finished with value: 0.9611023636594046 and parameters: {'max_depth': 4, 'learning_rate': 0.02684001867222824}. Best is trial 1 with value: 0.9680731086931571.\n",
            "[I 2025-04-13 11:50:25,174] Trial 6 finished with value: 0.9826912798976544 and parameters: {'max_depth': 7, 'learning_rate': 0.2257002396111984}. Best is trial 6 with value: 0.9826912798976544.\n",
            "[I 2025-04-13 11:50:25,889] Trial 7 finished with value: 0.9727713769743783 and parameters: {'max_depth': 3, 'learning_rate': 0.062190682165606234}. Best is trial 6 with value: 0.9826912798976544.\n",
            "[I 2025-04-13 11:50:26,758] Trial 8 finished with value: 0.9544648037610829 and parameters: {'max_depth': 4, 'learning_rate': 0.024167441977297352}. Best is trial 6 with value: 0.9826912798976544.\n",
            "[I 2025-04-13 11:50:27,314] Trial 9 finished with value: 0.7615656639902383 and parameters: {'max_depth': 2, 'learning_rate': 0.013802980290978841}. Best is trial 6 with value: 0.9826912798976544.\n",
            "[I 2025-04-13 11:50:28,688] Trial 10 finished with value: 0.9824619719580822 and parameters: {'max_depth': 7, 'learning_rate': 0.2564096960132346}. Best is trial 6 with value: 0.9826912798976544.\n",
            "[I 2025-04-13 11:50:30,042] Trial 11 finished with value: 0.9828938493012617 and parameters: {'max_depth': 7, 'learning_rate': 0.2443669909073624}. Best is trial 11 with value: 0.9828938493012617.\n",
            "[I 2025-04-13 11:50:31,393] Trial 12 finished with value: 0.981746311184524 and parameters: {'max_depth': 7, 'learning_rate': 0.2737097208396276}. Best is trial 11 with value: 0.9828938493012617.\n",
            "[I 2025-04-13 11:50:32,934] Trial 13 finished with value: 0.9841385626993016 and parameters: {'max_depth': 6, 'learning_rate': 0.11471310597737491}. Best is trial 13 with value: 0.9841385626993016.\n",
            "[I 2025-04-13 11:50:34,474] Trial 14 finished with value: 0.9840850320419152 and parameters: {'max_depth': 6, 'learning_rate': 0.09694626511439991}. Best is trial 13 with value: 0.9841385626993016.\n",
            "[I 2025-04-13 11:50:35,501] Trial 15 finished with value: 0.9842627630141149 and parameters: {'max_depth': 5, 'learning_rate': 0.06645441100408439}. Best is trial 15 with value: 0.9842627630141149.\n",
            "[I 2025-04-13 11:50:36,520] Trial 16 finished with value: 0.17939191655120598 and parameters: {'max_depth': 5, 'learning_rate': 0.0011015167964103039}. Best is trial 15 with value: 0.9842627630141149.\n",
            "[I 2025-04-13 11:50:37,543] Trial 17 finished with value: 0.9854730530363077 and parameters: {'max_depth': 5, 'learning_rate': 0.1068526458780159}. Best is trial 17 with value: 0.9854730530363077.\n",
            "[I 2025-04-13 11:50:38,565] Trial 18 finished with value: 0.8352521505763993 and parameters: {'max_depth': 5, 'learning_rate': 0.010596759735112204}. Best is trial 17 with value: 0.9854730530363077.\n",
            "[I 2025-04-13 11:50:39,589] Trial 19 finished with value: 0.9849459087505731 and parameters: {'max_depth': 5, 'learning_rate': 0.09650465606383488}. Best is trial 17 with value: 0.9854730530363077.\n",
            "[I 2025-04-13 11:50:40,614] Trial 20 finished with value: 0.9853691211859675 and parameters: {'max_depth': 5, 'learning_rate': 0.12546442346562517}. Best is trial 17 with value: 0.9854730530363077.\n",
            "[I 2025-04-13 11:50:41,641] Trial 21 finished with value: 0.985655098375152 and parameters: {'max_depth': 5, 'learning_rate': 0.11682187583254106}. Best is trial 21 with value: 0.985655098375152.\n",
            "[I 2025-04-13 11:50:42,665] Trial 22 finished with value: 0.9857468171822616 and parameters: {'max_depth': 5, 'learning_rate': 0.13703282648112997}. Best is trial 22 with value: 0.9857468171822616.\n",
            "[I 2025-04-13 11:50:43,543] Trial 23 finished with value: 0.9859776688629727 and parameters: {'max_depth': 4, 'learning_rate': 0.16179386114338054}. Best is trial 23 with value: 0.9859776688629727.\n",
            "[I 2025-04-13 11:50:44,331] Trial 24 finished with value: 0.9651492008568701 and parameters: {'max_depth': 3, 'learning_rate': 0.0445725282691052}. Best is trial 23 with value: 0.9859776688629727.\n",
            "[I 2025-04-13 11:50:45,519] Trial 25 finished with value: 0.9862904995021932 and parameters: {'max_depth': 4, 'learning_rate': 0.17019578659493778}. Best is trial 25 with value: 0.9862904995021932.\n",
            "[I 2025-04-13 11:50:46,645] Trial 26 finished with value: 0.986676966337516 and parameters: {'max_depth': 4, 'learning_rate': 0.16622386132105366}. Best is trial 26 with value: 0.986676966337516.\n",
            "[I 2025-04-13 11:50:47,512] Trial 27 finished with value: 0.9859614887918422 and parameters: {'max_depth': 4, 'learning_rate': 0.16785864095558278}. Best is trial 26 with value: 0.986676966337516.\n",
            "[I 2025-04-13 11:50:48,213] Trial 28 finished with value: 0.9816746211594222 and parameters: {'max_depth': 3, 'learning_rate': 0.17631669417218176}. Best is trial 26 with value: 0.986676966337516.\n",
            "[I 2025-04-13 11:50:48,931] Trial 29 finished with value: 0.9704612842463601 and parameters: {'max_depth': 3, 'learning_rate': 0.05511150156923022}. Best is trial 26 with value: 0.986676966337516.\n",
            "[I 2025-04-13 11:50:49,800] Trial 30 finished with value: 0.9842012578813808 and parameters: {'max_depth': 4, 'learning_rate': 0.07475123537939324}. Best is trial 26 with value: 0.986676966337516.\n",
            "[I 2025-04-13 11:50:50,681] Trial 31 finished with value: 0.9866625977504903 and parameters: {'max_depth': 4, 'learning_rate': 0.165660200608589}. Best is trial 26 with value: 0.986676966337516.\n",
            "[I 2025-04-13 11:50:51,545] Trial 32 finished with value: 0.9857361152771157 and parameters: {'max_depth': 4, 'learning_rate': 0.16777023687026515}. Best is trial 26 with value: 0.986676966337516.\n",
            "[I 2025-04-13 11:50:52,253] Trial 33 finished with value: 0.9538579705676739 and parameters: {'max_depth': 3, 'learning_rate': 0.03538625347396098}. Best is trial 26 with value: 0.986676966337516.\n",
            "[I 2025-04-13 11:50:53,114] Trial 34 finished with value: 0.9868233141493619 and parameters: {'max_depth': 4, 'learning_rate': 0.18649815054613553}. Best is trial 34 with value: 0.9868233141493619.\n",
            "[I 2025-04-13 11:50:53,980] Trial 35 finished with value: 0.9847710886489659 and parameters: {'max_depth': 4, 'learning_rate': 0.08522636671543596}. Best is trial 34 with value: 0.9868233141493619.\n",
            "[I 2025-04-13 11:50:54,698] Trial 36 finished with value: 0.9831756009894969 and parameters: {'max_depth': 3, 'learning_rate': 0.2060964848568122}. Best is trial 34 with value: 0.9868233141493619.\n",
            "[I 2025-04-13 11:50:55,260] Trial 37 finished with value: 0.9667429405061571 and parameters: {'max_depth': 2, 'learning_rate': 0.28472135829425016}. Best is trial 34 with value: 0.9868233141493619.\n",
            "[I 2025-04-13 11:50:56,135] Trial 38 finished with value: 0.9732177720184092 and parameters: {'max_depth': 4, 'learning_rate': 0.0359680831881579}. Best is trial 34 with value: 0.9868233141493619.\n",
            "[I 2025-04-13 11:50:57,278] Trial 39 finished with value: 0.8030150360447585 and parameters: {'max_depth': 4, 'learning_rate': 0.010115595836294755}. Best is trial 34 with value: 0.9868233141493619.\n",
            "[I 2025-04-13 11:50:58,289] Trial 40 finished with value: 0.8842538478092693 and parameters: {'max_depth': 3, 'learning_rate': 0.017901115643616444}. Best is trial 34 with value: 0.9868233141493619.\n",
            "[I 2025-04-13 11:50:59,286] Trial 41 finished with value: 0.9865189036399453 and parameters: {'max_depth': 4, 'learning_rate': 0.15430410359062993}. Best is trial 34 with value: 0.9868233141493619.\n",
            "[I 2025-04-13 11:51:00,155] Trial 42 finished with value: 0.9861436371918093 and parameters: {'max_depth': 4, 'learning_rate': 0.14507649523075608}. Best is trial 34 with value: 0.9868233141493619.\n",
            "[I 2025-04-13 11:51:01,020] Trial 43 finished with value: 0.9859984739724935 and parameters: {'max_depth': 4, 'learning_rate': 0.207004595772988}. Best is trial 34 with value: 0.9868233141493619.\n",
            "[I 2025-04-13 11:51:01,898] Trial 44 finished with value: 0.9814895153920689 and parameters: {'max_depth': 4, 'learning_rate': 0.052629382380044}. Best is trial 34 with value: 0.9868233141493619.\n",
            "[I 2025-04-13 11:51:02,784] Trial 45 finished with value: 0.9865881592296156 and parameters: {'max_depth': 4, 'learning_rate': 0.21038265297856326}. Best is trial 34 with value: 0.9868233141493619.\n",
            "[I 2025-04-13 11:51:03,500] Trial 46 finished with value: 0.9836494558290677 and parameters: {'max_depth': 3, 'learning_rate': 0.22751392155036732}. Best is trial 34 with value: 0.9868233141493619.\n",
            "[I 2025-04-13 11:51:04,379] Trial 47 finished with value: 0.9856932200153463 and parameters: {'max_depth': 4, 'learning_rate': 0.29757323160257926}. Best is trial 34 with value: 0.9868233141493619.\n",
            "[I 2025-04-13 11:51:04,948] Trial 48 finished with value: 0.952636000535039 and parameters: {'max_depth': 2, 'learning_rate': 0.08373017339373226}. Best is trial 34 with value: 0.9868233141493619.\n",
            "[I 2025-04-13 11:51:05,658] Trial 49 finished with value: 0.614910898591627 and parameters: {'max_depth': 3, 'learning_rate': 0.006311636584494767}. Best is trial 34 with value: 0.9868233141493619.\n",
            "[I 2025-04-13 11:51:06,700] Trial 50 finished with value: 0.19825664009723085 and parameters: {'max_depth': 5, 'learning_rate': 0.0012305270409311844}. Best is trial 34 with value: 0.9868233141493619.\n",
            "[I 2025-04-13 11:51:07,572] Trial 51 finished with value: 0.986513292802727 and parameters: {'max_depth': 4, 'learning_rate': 0.195063061648106}. Best is trial 34 with value: 0.9868233141493619.\n",
            "[I 2025-04-13 11:51:08,440] Trial 52 finished with value: 0.9861534973893042 and parameters: {'max_depth': 4, 'learning_rate': 0.20976792376210257}. Best is trial 34 with value: 0.9868233141493619.\n",
            "[I 2025-04-13 11:51:09,567] Trial 53 finished with value: 0.9860164349390175 and parameters: {'max_depth': 4, 'learning_rate': 0.12680530261821213}. Best is trial 34 with value: 0.9868233141493619.\n",
            "[I 2025-04-13 11:51:10,836] Trial 54 finished with value: 0.985802020809886 and parameters: {'max_depth': 4, 'learning_rate': 0.23200983468120506}. Best is trial 34 with value: 0.9868233141493619.\n",
            "[I 2025-04-13 11:51:11,913] Trial 55 finished with value: 0.984921880644087 and parameters: {'max_depth': 5, 'learning_rate': 0.09664126288414597}. Best is trial 34 with value: 0.9868233141493619.\n",
            "[I 2025-04-13 11:51:12,785] Trial 56 finished with value: 0.9865392704788404 and parameters: {'max_depth': 4, 'learning_rate': 0.13696942646791616}. Best is trial 34 with value: 0.9868233141493619.\n",
            "[I 2025-04-13 11:51:13,966] Trial 57 finished with value: 0.9841743494267163 and parameters: {'max_depth': 6, 'learning_rate': 0.13461059386998894}. Best is trial 34 with value: 0.9868233141493619.\n",
            "[I 2025-04-13 11:51:14,684] Trial 58 finished with value: 0.9753445995000419 and parameters: {'max_depth': 3, 'learning_rate': 0.07418401416827314}. Best is trial 34 with value: 0.9868233141493619.\n",
            "[I 2025-04-13 11:51:15,701] Trial 59 finished with value: 0.9853624163539757 and parameters: {'max_depth': 5, 'learning_rate': 0.10626421526943357}. Best is trial 34 with value: 0.9868233141493619.\n",
            "[I 2025-04-13 11:51:16,561] Trial 60 finished with value: 0.9861291706068526 and parameters: {'max_depth': 4, 'learning_rate': 0.2610195849797817}. Best is trial 34 with value: 0.9868233141493619.\n",
            "[I 2025-04-13 11:51:17,424] Trial 61 finished with value: 0.9868161837077576 and parameters: {'max_depth': 4, 'learning_rate': 0.18499380906721546}. Best is trial 34 with value: 0.9868233141493619.\n",
            "[I 2025-04-13 11:51:18,283] Trial 62 finished with value: 0.9866833560435585 and parameters: {'max_depth': 4, 'learning_rate': 0.1450160656832353}. Best is trial 34 with value: 0.9868233141493619.\n",
            "[I 2025-04-13 11:51:19,139] Trial 63 finished with value: 0.9861391871772538 and parameters: {'max_depth': 4, 'learning_rate': 0.11817401488579189}. Best is trial 34 with value: 0.9868233141493619.\n",
            "[I 2025-04-13 11:51:20,183] Trial 64 finished with value: 0.985269294601526 and parameters: {'max_depth': 5, 'learning_rate': 0.14664404332260286}. Best is trial 34 with value: 0.9868233141493619.\n",
            "[I 2025-04-13 11:51:21,095] Trial 65 finished with value: 0.9863783573810596 and parameters: {'max_depth': 4, 'learning_rate': 0.18593367358385607}. Best is trial 34 with value: 0.9868233141493619.\n",
            "[I 2025-04-13 11:51:22,285] Trial 66 finished with value: 0.9859997164246734 and parameters: {'max_depth': 4, 'learning_rate': 0.2618481431716593}. Best is trial 34 with value: 0.9868233141493619.\n",
            "[I 2025-04-13 11:51:23,292] Trial 67 finished with value: 0.9780719121410096 and parameters: {'max_depth': 3, 'learning_rate': 0.09710522966507558}. Best is trial 34 with value: 0.9868233141493619.\n",
            "[I 2025-04-13 11:51:24,331] Trial 68 finished with value: 0.9837548889270341 and parameters: {'max_depth': 5, 'learning_rate': 0.05840002232773451}. Best is trial 34 with value: 0.9868233141493619.\n",
            "[I 2025-04-13 11:51:25,188] Trial 69 finished with value: 0.9854740680005959 and parameters: {'max_depth': 4, 'learning_rate': 0.2349290655485172}. Best is trial 34 with value: 0.9868233141493619.\n",
            "[I 2025-04-13 11:51:26,051] Trial 70 finished with value: 0.9856687100281771 and parameters: {'max_depth': 4, 'learning_rate': 0.18973874766981258}. Best is trial 34 with value: 0.9868233141493619.\n",
            "[I 2025-04-13 11:51:26,919] Trial 71 finished with value: 0.9865089986258851 and parameters: {'max_depth': 4, 'learning_rate': 0.15154507902235811}. Best is trial 34 with value: 0.9868233141493619.\n",
            "[I 2025-04-13 11:51:27,782] Trial 72 finished with value: 0.9864281124338593 and parameters: {'max_depth': 4, 'learning_rate': 0.1598376724119791}. Best is trial 34 with value: 0.9868233141493619.\n",
            "[I 2025-04-13 11:51:28,665] Trial 73 finished with value: 0.9861628783312791 and parameters: {'max_depth': 4, 'learning_rate': 0.1350166226612394}. Best is trial 34 with value: 0.9868233141493619.\n",
            "[I 2025-04-13 11:51:29,530] Trial 74 finished with value: 0.9857249034860199 and parameters: {'max_depth': 4, 'learning_rate': 0.1159607274917013}. Best is trial 34 with value: 0.9868233141493619.\n",
            "[I 2025-04-13 11:51:30,403] Trial 75 finished with value: 0.9845420784591169 and parameters: {'max_depth': 4, 'learning_rate': 0.08118372915419501}. Best is trial 34 with value: 0.9868233141493619.\n",
            "[I 2025-04-13 11:51:31,105] Trial 76 finished with value: 0.9825613598150914 and parameters: {'max_depth': 3, 'learning_rate': 0.29530585017391364}. Best is trial 34 with value: 0.9868233141493619.\n",
            "[I 2025-04-13 11:51:32,126] Trial 77 finished with value: 0.9844503021556895 and parameters: {'max_depth': 5, 'learning_rate': 0.1807294383098843}. Best is trial 34 with value: 0.9868233141493619.\n",
            "[I 2025-04-13 11:51:32,991] Trial 78 finished with value: 0.30137334349116257 and parameters: {'max_depth': 4, 'learning_rate': 0.0020835968869264724}. Best is trial 34 with value: 0.9868233141493619.\n",
            "[I 2025-04-13 11:51:33,893] Trial 79 finished with value: 0.9384600223524732 and parameters: {'max_depth': 3, 'learning_rate': 0.02827159242221531}. Best is trial 34 with value: 0.9868233141493619.\n",
            "[I 2025-04-13 11:51:35,094] Trial 80 finished with value: 0.9857863921059864 and parameters: {'max_depth': 4, 'learning_rate': 0.2281906142681521}. Best is trial 34 with value: 0.9868233141493619.\n",
            "[I 2025-04-13 11:51:36,103] Trial 81 finished with value: 0.9865212487970619 and parameters: {'max_depth': 4, 'learning_rate': 0.19456134803782305}. Best is trial 34 with value: 0.9868233141493619.\n",
            "[I 2025-04-13 11:51:36,970] Trial 82 finished with value: 0.9861898565087323 and parameters: {'max_depth': 4, 'learning_rate': 0.1558993021105244}. Best is trial 34 with value: 0.9868233141493619.\n",
            "[I 2025-04-13 11:51:37,822] Trial 83 finished with value: 0.9862547197425272 and parameters: {'max_depth': 4, 'learning_rate': 0.18020873084168462}. Best is trial 34 with value: 0.9868233141493619.\n",
            "[I 2025-04-13 11:51:38,691] Trial 84 finished with value: 0.9854310663875084 and parameters: {'max_depth': 4, 'learning_rate': 0.10505638449898143}. Best is trial 34 with value: 0.9868233141493619.\n",
            "[I 2025-04-13 11:51:39,551] Trial 85 finished with value: 0.9860898987090532 and parameters: {'max_depth': 4, 'learning_rate': 0.20745549659679033}. Best is trial 34 with value: 0.9868233141493619.\n",
            "[I 2025-04-13 11:51:40,582] Trial 86 finished with value: 0.9841184462296481 and parameters: {'max_depth': 5, 'learning_rate': 0.06542803448315282}. Best is trial 34 with value: 0.9868233141493619.\n",
            "[I 2025-04-13 11:51:41,451] Trial 87 finished with value: 0.9860466770663944 and parameters: {'max_depth': 4, 'learning_rate': 0.12833653318445706}. Best is trial 34 with value: 0.9868233141493619.\n",
            "[I 2025-04-13 11:51:42,313] Trial 88 finished with value: 0.9867013539154712 and parameters: {'max_depth': 4, 'learning_rate': 0.25760150809565074}. Best is trial 34 with value: 0.9868233141493619.\n",
            "[I 2025-04-13 11:51:43,175] Trial 89 finished with value: 0.9860340894935237 and parameters: {'max_depth': 4, 'learning_rate': 0.25284355365989886}. Best is trial 34 with value: 0.9868233141493619.\n",
            "[I 2025-04-13 11:51:43,871] Trial 90 finished with value: 0.9813913601665004 and parameters: {'max_depth': 3, 'learning_rate': 0.25932162648795865}. Best is trial 34 with value: 0.9868233141493619.\n",
            "[I 2025-04-13 11:51:44,730] Trial 91 finished with value: 0.9864817752100549 and parameters: {'max_depth': 4, 'learning_rate': 0.16660581766351576}. Best is trial 34 with value: 0.9868233141493619.\n",
            "[I 2025-04-13 11:51:45,592] Trial 92 finished with value: 0.9862964986234991 and parameters: {'max_depth': 4, 'learning_rate': 0.22087499699914964}. Best is trial 34 with value: 0.9868233141493619.\n",
            "[I 2025-04-13 11:51:46,813] Trial 93 finished with value: 0.9863835272593809 and parameters: {'max_depth': 4, 'learning_rate': 0.14441870876168694}. Best is trial 34 with value: 0.9868233141493619.\n",
            "[I 2025-04-13 11:51:47,972] Trial 94 finished with value: 0.9861577378313884 and parameters: {'max_depth': 4, 'learning_rate': 0.19592449180249}. Best is trial 34 with value: 0.9868233141493619.\n",
            "[I 2025-04-13 11:51:48,838] Trial 95 finished with value: 0.9858470090351839 and parameters: {'max_depth': 4, 'learning_rate': 0.11892791921095316}. Best is trial 34 with value: 0.9868233141493619.\n",
            "[I 2025-04-13 11:51:49,700] Trial 96 finished with value: 0.9848201219082524 and parameters: {'max_depth': 4, 'learning_rate': 0.08935402321344123}. Best is trial 34 with value: 0.9868233141493619.\n",
            "[I 2025-04-13 11:51:50,588] Trial 97 finished with value: 0.9863839763540853 and parameters: {'max_depth': 4, 'learning_rate': 0.2932537035656739}. Best is trial 34 with value: 0.9868233141493619.\n",
            "[I 2025-04-13 11:51:51,456] Trial 98 finished with value: 0.9862130395495221 and parameters: {'max_depth': 4, 'learning_rate': 0.16763664350162685}. Best is trial 34 with value: 0.9868233141493619.\n",
            "[I 2025-04-13 11:51:52,470] Trial 99 finished with value: 0.9859192116502955 and parameters: {'max_depth': 5, 'learning_rate': 0.13580348685074745}. Best is trial 34 with value: 0.9868233141493619.\n",
            "[I 2025-04-13 11:51:52,472] A new study created in memory with name: no-name-c7b9c329-e27e-474e-bb9e-f066e73d7f13\n",
            "[I 2025-04-13 11:51:52,635] Trial 0 finished with value: 0.9775502880414327 and parameters: {'max_depth': 3, 'learning_rate': 0.09747530914950193}. Best is trial 0 with value: 0.9775502880414327.\n",
            "[I 2025-04-13 11:51:52,855] Trial 1 finished with value: 0.2500358025232951 and parameters: {'max_depth': 5, 'learning_rate': 0.0016672309428397835}. Best is trial 0 with value: 0.9775502880414327.\n",
            "[I 2025-04-13 11:51:53,007] Trial 2 finished with value: 0.1495158076286316 and parameters: {'max_depth': 3, 'learning_rate': 0.0010195131905425402}. Best is trial 0 with value: 0.9775502880414327.\n",
            "[I 2025-04-13 11:51:53,341] Trial 3 finished with value: 0.741443415482839 and parameters: {'max_depth': 7, 'learning_rate': 0.007922077082793376}. Best is trial 0 with value: 0.9775502880414327.\n",
            "[I 2025-04-13 11:51:53,496] Trial 4 finished with value: 0.4938562909762065 and parameters: {'max_depth': 2, 'learning_rate': 0.005121509355081129}. Best is trial 0 with value: 0.9775502880414327.\n",
            "[I 2025-04-13 11:51:53,632] Trial 5 finished with value: 0.9651325345039368 and parameters: {'max_depth': 2, 'learning_rate': 0.2233829676862235}. Best is trial 0 with value: 0.9775502880414327.\n",
            "[I 2025-04-13 11:51:53,964] Trial 6 finished with value: 0.9811411301294962 and parameters: {'max_depth': 7, 'learning_rate': 0.03817664844861826}. Best is trial 6 with value: 0.9811411301294962.\n",
            "[I 2025-04-13 11:51:54,283] Trial 7 finished with value: 0.9844966332117716 and parameters: {'max_depth': 7, 'learning_rate': 0.0674305358932335}. Best is trial 7 with value: 0.9844966332117716.\n",
            "[I 2025-04-13 11:51:54,484] Trial 8 finished with value: 0.24712826808293661 and parameters: {'max_depth': 4, 'learning_rate': 0.001699816160824048}. Best is trial 7 with value: 0.9844966332117716.\n",
            "[I 2025-04-13 11:51:54,747] Trial 9 finished with value: 0.9806981285413107 and parameters: {'max_depth': 6, 'learning_rate': 0.0401656428739496}. Best is trial 7 with value: 0.9844966332117716.\n",
            "[I 2025-04-13 11:51:55,010] Trial 10 finished with value: 0.9823679129282633 and parameters: {'max_depth': 6, 'learning_rate': 0.25053244297245747}. Best is trial 7 with value: 0.9844966332117716.\n",
            "[I 2025-04-13 11:51:55,292] Trial 11 finished with value: 0.9836728374163309 and parameters: {'max_depth': 6, 'learning_rate': 0.2874566068826736}. Best is trial 7 with value: 0.9844966332117716.\n",
            "[I 2025-04-13 11:51:55,583] Trial 12 finished with value: 0.9851145545641581 and parameters: {'max_depth': 6, 'learning_rate': 0.12013070657761775}. Best is trial 12 with value: 0.9851145545641581.\n",
            "[I 2025-04-13 11:51:55,911] Trial 13 finished with value: 0.9844762086868286 and parameters: {'max_depth': 7, 'learning_rate': 0.06838162025052887}. Best is trial 12 with value: 0.9851145545641581.\n",
            "[I 2025-04-13 11:51:56,141] Trial 14 finished with value: 0.930962344010671 and parameters: {'max_depth': 5, 'learning_rate': 0.018178413994147113}. Best is trial 12 with value: 0.9851145545641581.\n",
            "[I 2025-04-13 11:51:56,419] Trial 15 finished with value: 0.985002597173055 and parameters: {'max_depth': 6, 'learning_rate': 0.08451568156338034}. Best is trial 12 with value: 0.9851145545641581.\n",
            "[I 2025-04-13 11:51:56,667] Trial 16 finished with value: 0.985393762588501 and parameters: {'max_depth': 5, 'learning_rate': 0.12105199856070378}. Best is trial 16 with value: 0.985393762588501.\n",
            "[I 2025-04-13 11:51:56,865] Trial 17 finished with value: 0.9194656411806742 and parameters: {'max_depth': 4, 'learning_rate': 0.01841674154914704}. Best is trial 16 with value: 0.985393762588501.\n",
            "[I 2025-04-13 11:51:57,094] Trial 18 finished with value: 0.9857828617095947 and parameters: {'max_depth': 5, 'learning_rate': 0.14145403894765257}. Best is trial 18 with value: 0.9857828617095947.\n",
            "[I 2025-04-13 11:51:57,330] Trial 19 finished with value: 0.9852447509765625 and parameters: {'max_depth': 5, 'learning_rate': 0.15534221276657698}. Best is trial 18 with value: 0.9857828617095947.\n",
            "[I 2025-04-13 11:51:57,553] Trial 20 finished with value: 0.9662485917409261 and parameters: {'max_depth': 4, 'learning_rate': 0.03293562156289927}. Best is trial 18 with value: 0.9857828617095947.\n",
            "[I 2025-04-13 11:51:57,781] Trial 21 finished with value: 0.9854611357053121 and parameters: {'max_depth': 5, 'learning_rate': 0.1541111741768805}. Best is trial 18 with value: 0.9857828617095947.\n",
            "[I 2025-04-13 11:51:58,198] Trial 22 finished with value: 0.9856341481208801 and parameters: {'max_depth': 5, 'learning_rate': 0.16411999528304366}. Best is trial 18 with value: 0.9857828617095947.\n",
            "[I 2025-04-13 11:51:59,358] Trial 23 finished with value: 0.9851781725883484 and parameters: {'max_depth': 5, 'learning_rate': 0.1711061464864306}. Best is trial 18 with value: 0.9857828617095947.\n",
            "[I 2025-04-13 11:51:59,862] Trial 24 finished with value: 0.9790990352630615 and parameters: {'max_depth': 4, 'learning_rate': 0.05238753500976403}. Best is trial 18 with value: 0.9857828617095947.\n",
            "[I 2025-04-13 11:52:00,212] Trial 25 finished with value: 0.9809801379839579 and parameters: {'max_depth': 3, 'learning_rate': 0.15751824507940534}. Best is trial 18 with value: 0.9857828617095947.\n",
            "[I 2025-04-13 11:52:00,450] Trial 26 finished with value: 0.9653423428535461 and parameters: {'max_depth': 5, 'learning_rate': 0.026651202296519693}. Best is trial 18 with value: 0.9857828617095947.\n",
            "[I 2025-04-13 11:52:00,663] Trial 27 finished with value: 0.8216856718063354 and parameters: {'max_depth': 4, 'learning_rate': 0.011334492378711923}. Best is trial 18 with value: 0.9857828617095947.\n",
            "[I 2025-04-13 11:52:00,882] Trial 28 finished with value: 0.984778622786204 and parameters: {'max_depth': 5, 'learning_rate': 0.2014479153369217}. Best is trial 18 with value: 0.9857828617095947.\n",
            "[I 2025-04-13 11:52:01,151] Trial 29 finished with value: 0.985201915105184 and parameters: {'max_depth': 6, 'learning_rate': 0.09624292300622968}. Best is trial 18 with value: 0.9857828617095947.\n",
            "[I 2025-04-13 11:52:01,328] Trial 30 finished with value: 0.9826282461484274 and parameters: {'max_depth': 3, 'learning_rate': 0.2939205658571648}. Best is trial 18 with value: 0.9857828617095947.\n",
            "[I 2025-04-13 11:52:01,557] Trial 31 finished with value: 0.9852540691693624 and parameters: {'max_depth': 5, 'learning_rate': 0.11999501414504771}. Best is trial 18 with value: 0.9857828617095947.\n",
            "[I 2025-04-13 11:52:01,806] Trial 32 finished with value: 0.9845617016156515 and parameters: {'max_depth': 5, 'learning_rate': 0.10796804301607686}. Best is trial 18 with value: 0.9857828617095947.\n",
            "[I 2025-04-13 11:52:02,038] Trial 33 finished with value: 0.9842148224512736 and parameters: {'max_depth': 5, 'learning_rate': 0.07032553353331374}. Best is trial 18 with value: 0.9857828617095947.\n",
            "[I 2025-04-13 11:52:02,232] Trial 34 finished with value: 0.9846669435501099 and parameters: {'max_depth': 4, 'learning_rate': 0.14410204783445513}. Best is trial 18 with value: 0.9857828617095947.\n",
            "[I 2025-04-13 11:52:02,515] Trial 35 finished with value: 0.9829889933268229 and parameters: {'max_depth': 6, 'learning_rate': 0.050684913363769554}. Best is trial 18 with value: 0.9857828617095947.\n",
            "[I 2025-04-13 11:52:02,756] Trial 36 finished with value: 0.984920601050059 and parameters: {'max_depth': 5, 'learning_rate': 0.20188963067192786}. Best is trial 18 with value: 0.9857828617095947.\n",
            "[I 2025-04-13 11:52:02,946] Trial 37 finished with value: 0.37152932087580365 and parameters: {'max_depth': 4, 'learning_rate': 0.0028025022481963716}. Best is trial 18 with value: 0.9857828617095947.\n",
            "[I 2025-04-13 11:52:03,174] Trial 38 finished with value: 0.9852532347043356 and parameters: {'max_depth': 5, 'learning_rate': 0.08732893138162028}. Best is trial 18 with value: 0.9857828617095947.\n",
            "[I 2025-04-13 11:52:03,465] Trial 39 finished with value: 0.9642995595932007 and parameters: {'max_depth': 6, 'learning_rate': 0.024509773087262897}. Best is trial 18 with value: 0.9857828617095947.\n",
            "[I 2025-04-13 11:52:03,630] Trial 40 finished with value: 0.9675788482030233 and parameters: {'max_depth': 3, 'learning_rate': 0.05208502628482428}. Best is trial 18 with value: 0.9857828617095947.\n",
            "[I 2025-04-13 11:52:03,878] Trial 41 finished with value: 0.9857771992683411 and parameters: {'max_depth': 5, 'learning_rate': 0.13489394319569176}. Best is trial 18 with value: 0.9857828617095947.\n",
            "[I 2025-04-13 11:52:04,103] Trial 42 finished with value: 0.984943171342214 and parameters: {'max_depth': 5, 'learning_rate': 0.20856686025827517}. Best is trial 18 with value: 0.9857828617095947.\n",
            "[I 2025-04-13 11:52:04,341] Trial 43 finished with value: 0.9856062332789103 and parameters: {'max_depth': 5, 'learning_rate': 0.12330467496948962}. Best is trial 18 with value: 0.9857828617095947.\n",
            "[I 2025-04-13 11:52:04,537] Trial 44 finished with value: 0.9846647580464681 and parameters: {'max_depth': 4, 'learning_rate': 0.14476116237970277}. Best is trial 18 with value: 0.9857828617095947.\n",
            "[I 2025-04-13 11:52:04,825] Trial 45 finished with value: 0.9834225575129191 and parameters: {'max_depth': 6, 'learning_rate': 0.24592163898247826}. Best is trial 18 with value: 0.9857828617095947.\n",
            "[I 2025-04-13 11:52:05,053] Trial 46 finished with value: 0.5814767479896545 and parameters: {'max_depth': 5, 'learning_rate': 0.005175191823773963}. Best is trial 18 with value: 0.9857828617095947.\n",
            "[I 2025-04-13 11:52:05,335] Trial 47 finished with value: 0.9847103754679362 and parameters: {'max_depth': 6, 'learning_rate': 0.07244694150045196}. Best is trial 18 with value: 0.9857828617095947.\n",
            "[I 2025-04-13 11:52:05,538] Trial 48 finished with value: 0.9838434855143229 and parameters: {'max_depth': 4, 'learning_rate': 0.09925860599001846}. Best is trial 18 with value: 0.9857828617095947.\n",
            "[I 2025-04-13 11:52:05,683] Trial 49 finished with value: 0.9625581105550131 and parameters: {'max_depth': 2, 'learning_rate': 0.1784241010974786}. Best is trial 18 with value: 0.9857828617095947.\n",
            "[I 2025-04-13 11:52:05,922] Trial 50 finished with value: 0.985035240650177 and parameters: {'max_depth': 5, 'learning_rate': 0.296112928884327}. Best is trial 18 with value: 0.9857828617095947.\n",
            "[I 2025-04-13 11:52:06,173] Trial 51 finished with value: 0.9852607051531473 and parameters: {'max_depth': 5, 'learning_rate': 0.13027762554080566}. Best is trial 18 with value: 0.9857828617095947.\n",
            "[I 2025-04-13 11:52:06,421] Trial 52 finished with value: 0.9847456216812134 and parameters: {'max_depth': 5, 'learning_rate': 0.11448043732412881}. Best is trial 18 with value: 0.9857828617095947.\n",
            "[I 2025-04-13 11:52:06,656] Trial 53 finished with value: 0.16131355365117392 and parameters: {'max_depth': 5, 'learning_rate': 0.0010181342703998498}. Best is trial 18 with value: 0.9857828617095947.\n",
            "[I 2025-04-13 11:52:06,949] Trial 54 finished with value: 0.9848009745279948 and parameters: {'max_depth': 6, 'learning_rate': 0.08297473884422364}. Best is trial 18 with value: 0.9857828617095947.\n",
            "[I 2025-04-13 11:52:07,178] Trial 55 finished with value: 0.9850320816040039 and parameters: {'max_depth': 5, 'learning_rate': 0.1790369157125615}. Best is trial 18 with value: 0.9857828617095947.\n",
            "[I 2025-04-13 11:52:07,383] Trial 56 finished with value: 0.985157310962677 and parameters: {'max_depth': 4, 'learning_rate': 0.23405321927558326}. Best is trial 18 with value: 0.9857828617095947.\n",
            "[I 2025-04-13 11:52:07,617] Trial 57 finished with value: 0.983387271563212 and parameters: {'max_depth': 5, 'learning_rate': 0.057702367626853726}. Best is trial 18 with value: 0.9857828617095947.\n",
            "[I 2025-04-13 11:52:07,919] Trial 58 finished with value: 0.9815241694450378 and parameters: {'max_depth': 6, 'learning_rate': 0.0422023578673172}. Best is trial 18 with value: 0.9857828617095947.\n",
            "[I 2025-04-13 11:52:08,145] Trial 59 finished with value: 0.9856761693954468 and parameters: {'max_depth': 5, 'learning_rate': 0.1332487630054159}. Best is trial 18 with value: 0.9857828617095947.\n",
            "[I 2025-04-13 11:52:08,348] Trial 60 finished with value: 0.9852025111516317 and parameters: {'max_depth': 4, 'learning_rate': 0.1400750966314014}. Best is trial 18 with value: 0.9857828617095947.\n",
            "[I 2025-04-13 11:52:08,583] Trial 61 finished with value: 0.9851654171943665 and parameters: {'max_depth': 5, 'learning_rate': 0.10860046046615945}. Best is trial 18 with value: 0.9857828617095947.\n",
            "[I 2025-04-13 11:52:08,809] Trial 62 finished with value: 0.9853355288505554 and parameters: {'max_depth': 5, 'learning_rate': 0.17507158606986106}. Best is trial 18 with value: 0.9857828617095947.\n",
            "[I 2025-04-13 11:52:09,060] Trial 63 finished with value: 0.9846135179201762 and parameters: {'max_depth': 5, 'learning_rate': 0.079065815775348}. Best is trial 18 with value: 0.9857828617095947.\n",
            "[I 2025-04-13 11:52:09,295] Trial 64 finished with value: 0.9854680498441061 and parameters: {'max_depth': 5, 'learning_rate': 0.12673540203004377}. Best is trial 18 with value: 0.9857828617095947.\n",
            "[I 2025-04-13 11:52:09,533] Trial 65 finished with value: 0.9847023288408915 and parameters: {'max_depth': 5, 'learning_rate': 0.2260648760986911}. Best is trial 18 with value: 0.9857828617095947.\n",
            "[I 2025-04-13 11:52:09,805] Trial 66 finished with value: 0.9840221206347147 and parameters: {'max_depth': 6, 'learning_rate': 0.06154915104534739}. Best is trial 18 with value: 0.9857828617095947.\n",
            "[I 2025-04-13 11:52:10,055] Trial 67 finished with value: 0.984847903251648 and parameters: {'max_depth': 5, 'learning_rate': 0.09589221302374396}. Best is trial 18 with value: 0.9857828617095947.\n",
            "[I 2025-04-13 11:52:10,392] Trial 68 finished with value: 0.984860340754191 and parameters: {'max_depth': 4, 'learning_rate': 0.13549431062867562}. Best is trial 18 with value: 0.9857828617095947.\n",
            "[I 2025-04-13 11:52:11,148] Trial 69 finished with value: 0.9853174090385437 and parameters: {'max_depth': 5, 'learning_rate': 0.16398827477352712}. Best is trial 18 with value: 0.9857828617095947.\n",
            "[I 2025-04-13 11:52:12,200] Trial 70 finished with value: 0.9842687249183655 and parameters: {'max_depth': 7, 'learning_rate': 0.12040894060091432}. Best is trial 18 with value: 0.9857828617095947.\n",
            "[I 2025-04-13 11:52:12,667] Trial 71 finished with value: 0.8307971556981405 and parameters: {'max_depth': 5, 'learning_rate': 0.011030558681439186}. Best is trial 18 with value: 0.9857828617095947.\n",
            "[I 2025-04-13 11:52:12,896] Trial 72 finished with value: 0.9849973917007446 and parameters: {'max_depth': 5, 'learning_rate': 0.10025939337893681}. Best is trial 18 with value: 0.9857828617095947.\n",
            "[I 2025-04-13 11:52:13,141] Trial 73 finished with value: 0.9845619400342306 and parameters: {'max_depth': 5, 'learning_rate': 0.25036443060851205}. Best is trial 18 with value: 0.9857828617095947.\n",
            "[I 2025-04-13 11:52:13,380] Trial 74 finished with value: 0.9850354393323263 and parameters: {'max_depth': 5, 'learning_rate': 0.17807829976574516}. Best is trial 18 with value: 0.9857828617095947.\n",
            "[I 2025-04-13 11:52:13,613] Trial 75 finished with value: 0.9855729738871256 and parameters: {'max_depth': 5, 'learning_rate': 0.12683960454108736}. Best is trial 18 with value: 0.9857828617095947.\n",
            "[I 2025-04-13 11:52:13,835] Trial 76 finished with value: 0.9851672450701395 and parameters: {'max_depth': 5, 'learning_rate': 0.20574026733309606}. Best is trial 18 with value: 0.9857828617095947.\n",
            "[I 2025-04-13 11:52:14,129] Trial 77 finished with value: 0.9844494859377543 and parameters: {'max_depth': 6, 'learning_rate': 0.15180454601317067}. Best is trial 18 with value: 0.9857828617095947.\n",
            "[I 2025-04-13 11:52:14,344] Trial 78 finished with value: 0.9830686847368876 and parameters: {'max_depth': 4, 'learning_rate': 0.09154626165949924}. Best is trial 18 with value: 0.9857828617095947.\n",
            "[I 2025-04-13 11:52:14,584] Trial 79 finished with value: 0.9859151442845663 and parameters: {'max_depth': 5, 'learning_rate': 0.1257613156032291}. Best is trial 79 with value: 0.9859151442845663.\n",
            "[I 2025-04-13 11:52:14,818] Trial 80 finished with value: 0.9846166968345642 and parameters: {'max_depth': 5, 'learning_rate': 0.07382448847569519}. Best is trial 79 with value: 0.9859151442845663.\n",
            "[I 2025-04-13 11:52:15,044] Trial 81 finished with value: 0.9853816231091818 and parameters: {'max_depth': 5, 'learning_rate': 0.12843935812619797}. Best is trial 79 with value: 0.9859151442845663.\n",
            "[I 2025-04-13 11:52:15,297] Trial 82 finished with value: 0.2358581225077311 and parameters: {'max_depth': 5, 'learning_rate': 0.0015581388530589689}. Best is trial 79 with value: 0.9859151442845663.\n",
            "[I 2025-04-13 11:52:15,538] Trial 83 finished with value: 0.9842244784037272 and parameters: {'max_depth': 5, 'learning_rate': 0.10825757257595735}. Best is trial 79 with value: 0.9859151442845663.\n",
            "[I 2025-04-13 11:52:15,764] Trial 84 finished with value: 0.9854979515075684 and parameters: {'max_depth': 5, 'learning_rate': 0.153311762291198}. Best is trial 79 with value: 0.9859151442845663.\n",
            "[I 2025-04-13 11:52:15,994] Trial 85 finished with value: 0.9853970607121786 and parameters: {'max_depth': 5, 'learning_rate': 0.12846925363360487}. Best is trial 79 with value: 0.9859151442845663.\n",
            "[I 2025-04-13 11:52:16,292] Trial 86 finished with value: 0.9844403266906738 and parameters: {'max_depth': 6, 'learning_rate': 0.1877050598501218}. Best is trial 79 with value: 0.9859151442845663.\n",
            "[I 2025-04-13 11:52:16,489] Trial 87 finished with value: 0.9848365386327108 and parameters: {'max_depth': 4, 'learning_rate': 0.26341651999656657}. Best is trial 79 with value: 0.9859151442845663.\n",
            "[I 2025-04-13 11:52:16,716] Trial 88 finished with value: 0.9853354493776957 and parameters: {'max_depth': 5, 'learning_rate': 0.1510279690022131}. Best is trial 79 with value: 0.9859151442845663.\n",
            "[I 2025-04-13 11:52:16,996] Trial 89 finished with value: 0.9850314656893412 and parameters: {'max_depth': 6, 'learning_rate': 0.08445491291729017}. Best is trial 79 with value: 0.9859151442845663.\n",
            "[I 2025-04-13 11:52:17,255] Trial 90 finished with value: 0.9838876326878866 and parameters: {'max_depth': 5, 'learning_rate': 0.06351860750797503}. Best is trial 79 with value: 0.9859151442845663.\n",
            "[I 2025-04-13 11:52:17,489] Trial 91 finished with value: 0.9853370388348898 and parameters: {'max_depth': 5, 'learning_rate': 0.15362592220336052}. Best is trial 79 with value: 0.9859151442845663.\n",
            "[I 2025-04-13 11:52:17,715] Trial 92 finished with value: 0.9849729736646017 and parameters: {'max_depth': 5, 'learning_rate': 0.2025176022726283}. Best is trial 79 with value: 0.9859151442845663.\n",
            "[I 2025-04-13 11:52:17,953] Trial 93 finished with value: 0.9852719306945801 and parameters: {'max_depth': 5, 'learning_rate': 0.11512245133433334}. Best is trial 79 with value: 0.9859151442845663.\n",
            "[I 2025-04-13 11:52:18,202] Trial 94 finished with value: 0.9852672020594279 and parameters: {'max_depth': 5, 'learning_rate': 0.13560097021692263}. Best is trial 79 with value: 0.9859151442845663.\n",
            "[I 2025-04-13 11:52:18,443] Trial 95 finished with value: 0.984876016775767 and parameters: {'max_depth': 5, 'learning_rate': 0.16125953061933743}. Best is trial 79 with value: 0.9859151442845663.\n",
            "[I 2025-04-13 11:52:18,669] Trial 96 finished with value: 0.9843221306800842 and parameters: {'max_depth': 5, 'learning_rate': 0.2165711906106561}. Best is trial 79 with value: 0.9859151442845663.\n",
            "[I 2025-04-13 11:52:18,818] Trial 97 finished with value: 0.9563836256663004 and parameters: {'max_depth': 2, 'learning_rate': 0.1044004190141595}. Best is trial 79 with value: 0.9859151442845663.\n",
            "[I 2025-04-13 11:52:19,051] Trial 98 finished with value: 0.985573430856069 and parameters: {'max_depth': 5, 'learning_rate': 0.12093371530451458}. Best is trial 79 with value: 0.9859151442845663.\n",
            "[I 2025-04-13 11:52:19,307] Trial 99 finished with value: 0.9852835536003113 and parameters: {'max_depth': 5, 'learning_rate': 0.09134383925593113}. Best is trial 79 with value: 0.9859151442845663.\n",
            "[I 2025-04-13 11:52:19,310] A new study created in memory with name: no-name-7a3844a9-8ded-4c99-a4e5-a9e4f716325a\n",
            "[I 2025-04-13 11:52:19,490] Trial 0 finished with value: 0.9594013978460291 and parameters: {'max_depth': 5, 'learning_rate': 0.03819822686349274}. Best is trial 0 with value: 0.9594013978460291.\n",
            "[I 2025-04-13 11:52:19,599] Trial 1 finished with value: 0.9337316215845449 and parameters: {'max_depth': 3, 'learning_rate': 0.03382728160051053}. Best is trial 0 with value: 0.9594013978460291.\n",
            "[I 2025-04-13 11:52:19,773] Trial 2 finished with value: 0.9094714883312026 and parameters: {'max_depth': 5, 'learning_rate': 0.018042845856525554}. Best is trial 0 with value: 0.9594013978460291.\n",
            "[I 2025-04-13 11:52:19,975] Trial 3 finished with value: 0.2237015599544981 and parameters: {'max_depth': 7, 'learning_rate': 0.0014726141303420632}. Best is trial 0 with value: 0.9594013978460291.\n",
            "[I 2025-04-13 11:52:20,151] Trial 4 finished with value: 0.9692333938120923 and parameters: {'max_depth': 5, 'learning_rate': 0.05365124124759353}. Best is trial 4 with value: 0.9692333938120923.\n",
            "[I 2025-04-13 11:52:20,318] Trial 5 finished with value: 0.9809227455846447 and parameters: {'max_depth': 5, 'learning_rate': 0.27280317434649237}. Best is trial 5 with value: 0.9809227455846447.\n",
            "[I 2025-04-13 11:52:20,413] Trial 6 finished with value: 0.9650225164230091 and parameters: {'max_depth': 2, 'learning_rate': 0.26264528060379194}. Best is trial 5 with value: 0.9809227455846447.\n",
            "[I 2025-04-13 11:52:20,519] Trial 7 finished with value: 0.2911429113925133 and parameters: {'max_depth': 3, 'learning_rate': 0.002206816272484904}. Best is trial 5 with value: 0.9809227455846447.\n",
            "[I 2025-04-13 11:52:20,602] Trial 8 finished with value: 0.1484376905422822 and parameters: {'max_depth': 2, 'learning_rate': 0.0011036017558358694}. Best is trial 5 with value: 0.9809227455846447.\n",
            "[I 2025-04-13 11:52:20,802] Trial 9 finished with value: 0.5601655489647143 and parameters: {'max_depth': 7, 'learning_rate': 0.004832851324273667}. Best is trial 5 with value: 0.9809227455846447.\n",
            "[I 2025-04-13 11:52:20,967] Trial 10 finished with value: 0.9818292400096537 and parameters: {'max_depth': 6, 'learning_rate': 0.26666603907104114}. Best is trial 10 with value: 0.9818292400096537.\n",
            "[I 2025-04-13 11:52:21,135] Trial 11 finished with value: 0.9834356925847176 and parameters: {'max_depth': 6, 'learning_rate': 0.2953769563900058}. Best is trial 11 with value: 0.9834356925847176.\n",
            "[I 2025-04-13 11:52:21,334] Trial 12 finished with value: 0.9766206783120103 and parameters: {'max_depth': 6, 'learning_rate': 0.10705426493800958}. Best is trial 11 with value: 0.9834356925847176.\n",
            "[I 2025-04-13 11:52:21,524] Trial 13 finished with value: 0.9773653846855668 and parameters: {'max_depth': 6, 'learning_rate': 0.12028516735121665}. Best is trial 11 with value: 0.9834356925847176.\n",
            "[I 2025-04-13 11:52:21,706] Trial 14 finished with value: 0.9779463643977696 and parameters: {'max_depth': 6, 'learning_rate': 0.13054749659430398}. Best is trial 11 with value: 0.9834356925847176.\n",
            "[I 2025-04-13 11:52:21,924] Trial 15 finished with value: 0.7120928237224474 and parameters: {'max_depth': 7, 'learning_rate': 0.007507181341104923}. Best is trial 11 with value: 0.9834356925847176.\n",
            "[I 2025-04-13 11:52:22,060] Trial 16 finished with value: 0.9827631221399122 and parameters: {'max_depth': 4, 'learning_rate': 0.2771013863393515}. Best is trial 11 with value: 0.9834356925847176.\n",
            "[I 2025-04-13 11:52:22,204] Trial 17 finished with value: 0.9734490141346578 and parameters: {'max_depth': 4, 'learning_rate': 0.08574363660796955}. Best is trial 11 with value: 0.9834356925847176.\n",
            "[I 2025-04-13 11:52:22,368] Trial 18 finished with value: 0.8780199296130117 and parameters: {'max_depth': 4, 'learning_rate': 0.015826374873516593}. Best is trial 11 with value: 0.9834356925847176.\n",
            "[I 2025-04-13 11:52:22,493] Trial 19 finished with value: 0.9736284118993676 and parameters: {'max_depth': 3, 'learning_rate': 0.15267521388716604}. Best is trial 11 with value: 0.9834356925847176.\n",
            "[I 2025-04-13 11:52:22,675] Trial 20 finished with value: 0.9710713012807349 and parameters: {'max_depth': 4, 'learning_rate': 0.06601189425442347}. Best is trial 11 with value: 0.9834356925847176.\n",
            "[I 2025-04-13 11:52:22,936] Trial 21 finished with value: 0.9824280471745125 and parameters: {'max_depth': 6, 'learning_rate': 0.2812066799785626}. Best is trial 11 with value: 0.9834356925847176.\n",
            "[I 2025-04-13 11:52:23,198] Trial 22 finished with value: 0.9809123428755955 and parameters: {'max_depth': 6, 'learning_rate': 0.19008576602199143}. Best is trial 11 with value: 0.9834356925847176.\n",
            "[I 2025-04-13 11:52:23,408] Trial 23 finished with value: 0.982217621228187 and parameters: {'max_depth': 4, 'learning_rate': 0.2988113761118805}. Best is trial 11 with value: 0.9834356925847176.\n",
            "[I 2025-04-13 11:52:23,633] Trial 24 finished with value: 0.980480447078579 and parameters: {'max_depth': 5, 'learning_rate': 0.17788704048284726}. Best is trial 11 with value: 0.9834356925847176.\n",
            "[I 2025-04-13 11:52:23,940] Trial 25 finished with value: 0.9737855521021453 and parameters: {'max_depth': 7, 'learning_rate': 0.0792403956433609}. Best is trial 11 with value: 0.9834356925847176.\n",
            "[I 2025-04-13 11:52:24,203] Trial 26 finished with value: 0.9796326500960469 and parameters: {'max_depth': 6, 'learning_rate': 0.17009663642112755}. Best is trial 11 with value: 0.9834356925847176.\n",
            "[I 2025-04-13 11:52:24,493] Trial 27 finished with value: 0.9649182028727404 and parameters: {'max_depth': 5, 'learning_rate': 0.044291316175765275}. Best is trial 11 with value: 0.9834356925847176.\n",
            "[I 2025-04-13 11:52:24,730] Trial 28 finished with value: 0.87503386104297 and parameters: {'max_depth': 4, 'learning_rate': 0.015565753635770651}. Best is trial 11 with value: 0.9834356925847176.\n",
            "[I 2025-04-13 11:52:24,964] Trial 29 finished with value: 0.9581986118890379 and parameters: {'max_depth': 5, 'learning_rate': 0.03724458681255887}. Best is trial 11 with value: 0.9834356925847176.\n",
            "[I 2025-04-13 11:52:25,086] Trial 30 finished with value: 0.976585659012187 and parameters: {'max_depth': 3, 'learning_rate': 0.1923922680051625}. Best is trial 11 with value: 0.9834356925847176.\n",
            "[I 2025-04-13 11:52:25,221] Trial 31 finished with value: 0.98366647442386 and parameters: {'max_depth': 4, 'learning_rate': 0.2945995204485734}. Best is trial 31 with value: 0.98366647442386.\n",
            "[I 2025-04-13 11:52:25,360] Trial 32 finished with value: 0.9826742144787901 and parameters: {'max_depth': 4, 'learning_rate': 0.2996533260140613}. Best is trial 31 with value: 0.98366647442386.\n",
            "[I 2025-04-13 11:52:25,495] Trial 33 finished with value: 0.9118207406897642 and parameters: {'max_depth': 3, 'learning_rate': 0.02559441625774209}. Best is trial 31 with value: 0.98366647442386.\n",
            "[I 2025-04-13 11:52:25,638] Trial 34 finished with value: 0.9740881758619487 and parameters: {'max_depth': 4, 'learning_rate': 0.09073017278507922}. Best is trial 31 with value: 0.98366647442386.\n",
            "[I 2025-04-13 11:52:25,759] Trial 35 finished with value: 0.9769109034866578 and parameters: {'max_depth': 3, 'learning_rate': 0.2032588965678373}. Best is trial 31 with value: 0.98366647442386.\n",
            "[I 2025-04-13 11:52:25,899] Trial 36 finished with value: 0.9777245725661065 and parameters: {'max_depth': 4, 'learning_rate': 0.13764054766064548}. Best is trial 31 with value: 0.98366647442386.\n",
            "[I 2025-04-13 11:52:26,071] Trial 37 finished with value: 0.9711749687558034 and parameters: {'max_depth': 5, 'learning_rate': 0.06254124103832101}. Best is trial 31 with value: 0.98366647442386.\n",
            "[I 2025-04-13 11:52:26,212] Trial 38 finished with value: 0.9804800848059725 and parameters: {'max_depth': 4, 'learning_rate': 0.216914503476346}. Best is trial 31 with value: 0.98366647442386.\n",
            "[I 2025-04-13 11:52:26,340] Trial 39 finished with value: 0.7216884307021277 and parameters: {'max_depth': 3, 'learning_rate': 0.009284202383290693}. Best is trial 31 with value: 0.98366647442386.\n",
            "[I 2025-04-13 11:52:26,539] Trial 40 finished with value: 0.33117698115606237 and parameters: {'max_depth': 5, 'learning_rate': 0.0023936316898526895}. Best is trial 31 with value: 0.98366647442386.\n",
            "[I 2025-04-13 11:52:26,689] Trial 41 finished with value: 0.9818677749255981 and parameters: {'max_depth': 5, 'learning_rate': 0.24528365740896543}. Best is trial 31 with value: 0.98366647442386.\n",
            "[I 2025-04-13 11:52:26,826] Trial 42 finished with value: 0.9820311403535714 and parameters: {'max_depth': 4, 'learning_rate': 0.28624888947400406}. Best is trial 31 with value: 0.98366647442386.\n",
            "[I 2025-04-13 11:52:27,014] Trial 43 finished with value: 0.9823957555040086 and parameters: {'max_depth': 7, 'learning_rate': 0.290266344914378}. Best is trial 31 with value: 0.98366647442386.\n",
            "[I 2025-04-13 11:52:27,204] Trial 44 finished with value: 0.9761543231069713 and parameters: {'max_depth': 6, 'learning_rate': 0.10623678062756355}. Best is trial 31 with value: 0.98366647442386.\n",
            "[I 2025-04-13 11:52:27,315] Trial 45 finished with value: 0.9581101359069443 and parameters: {'max_depth': 2, 'learning_rate': 0.1481406861559008}. Best is trial 31 with value: 0.98366647442386.\n",
            "[I 2025-04-13 11:52:27,501] Trial 46 finished with value: 0.9821746280164666 and parameters: {'max_depth': 6, 'learning_rate': 0.22484298071237732}. Best is trial 31 with value: 0.98366647442386.\n",
            "[I 2025-04-13 11:52:27,640] Trial 47 finished with value: 0.9816449629885383 and parameters: {'max_depth': 4, 'learning_rate': 0.22564899105009215}. Best is trial 31 with value: 0.98366647442386.\n",
            "[I 2025-04-13 11:52:27,800] Trial 48 finished with value: 0.9773219864940083 and parameters: {'max_depth': 5, 'learning_rate': 0.11971353807064138}. Best is trial 31 with value: 0.98366647442386.\n",
            "[I 2025-04-13 11:52:27,942] Trial 49 finished with value: 0.9791759311101021 and parameters: {'max_depth': 4, 'learning_rate': 0.16159329195186864}. Best is trial 31 with value: 0.98366647442386.\n",
            "[I 2025-04-13 11:52:28,113] Trial 50 finished with value: 0.9829857651945022 and parameters: {'max_depth': 6, 'learning_rate': 0.29309337777749206}. Best is trial 31 with value: 0.98366647442386.\n",
            "[I 2025-04-13 11:52:28,287] Trial 51 finished with value: 0.9822334684798179 and parameters: {'max_depth': 6, 'learning_rate': 0.2541194456536009}. Best is trial 31 with value: 0.98366647442386.\n",
            "[I 2025-04-13 11:52:28,486] Trial 52 finished with value: 0.9824783716846635 and parameters: {'max_depth': 7, 'learning_rate': 0.2971678852981396}. Best is trial 31 with value: 0.98366647442386.\n",
            "[I 2025-04-13 11:52:28,696] Trial 53 finished with value: 0.9812721687465035 and parameters: {'max_depth': 7, 'learning_rate': 0.2146196018099609}. Best is trial 31 with value: 0.98366647442386.\n",
            "[I 2025-04-13 11:52:28,894] Trial 54 finished with value: 0.9796128059405876 and parameters: {'max_depth': 7, 'learning_rate': 0.16034812364184206}. Best is trial 31 with value: 0.98366647442386.\n",
            "[I 2025-04-13 11:52:29,105] Trial 55 finished with value: 0.9752559909336873 and parameters: {'max_depth': 7, 'learning_rate': 0.09826468697387238}. Best is trial 31 with value: 0.98366647442386.\n",
            "[I 2025-04-13 11:52:29,268] Trial 56 finished with value: 0.9779505932918644 and parameters: {'max_depth': 5, 'learning_rate': 0.13247515720589648}. Best is trial 31 with value: 0.98366647442386.\n",
            "[I 2025-04-13 11:52:29,460] Trial 57 finished with value: 0.9827521980046385 and parameters: {'max_depth': 7, 'learning_rate': 0.29086972656778276}. Best is trial 31 with value: 0.98366647442386.\n",
            "[I 2025-04-13 11:52:29,612] Trial 58 finished with value: 0.9797662626488225 and parameters: {'max_depth': 4, 'learning_rate': 0.18018061803443858}. Best is trial 31 with value: 0.98366647442386.\n",
            "[I 2025-04-13 11:52:29,770] Trial 59 finished with value: 0.9821548840181764 and parameters: {'max_depth': 5, 'learning_rate': 0.2252966003576099}. Best is trial 31 with value: 0.98366647442386.\n",
            "[I 2025-04-13 11:52:29,963] Trial 60 finished with value: 0.9729326405485036 and parameters: {'max_depth': 6, 'learning_rate': 0.07603316683507125}. Best is trial 31 with value: 0.98366647442386.\n",
            "[I 2025-04-13 11:52:30,153] Trial 61 finished with value: 0.9827711422355999 and parameters: {'max_depth': 7, 'learning_rate': 0.29684617120285617}. Best is trial 31 with value: 0.98366647442386.\n",
            "[I 2025-04-13 11:52:30,346] Trial 62 finished with value: 0.9819606625850706 and parameters: {'max_depth': 7, 'learning_rate': 0.24816140186974567}. Best is trial 31 with value: 0.98366647442386.\n",
            "[I 2025-04-13 11:52:30,537] Trial 63 finished with value: 0.9823616722764412 and parameters: {'max_depth': 7, 'learning_rate': 0.2996422524126346}. Best is trial 31 with value: 0.98366647442386.\n",
            "[I 2025-04-13 11:52:30,742] Trial 64 finished with value: 0.9794112584933178 and parameters: {'max_depth': 7, 'learning_rate': 0.16041011000291183}. Best is trial 31 with value: 0.98366647442386.\n",
            "[I 2025-04-13 11:52:30,912] Trial 65 finished with value: 0.9807789795264842 and parameters: {'max_depth': 6, 'learning_rate': 0.19815979192764835}. Best is trial 31 with value: 0.98366647442386.\n",
            "[I 2025-04-13 11:52:31,056] Trial 66 finished with value: 0.9773609710279393 and parameters: {'max_depth': 4, 'learning_rate': 0.11960072560312446}. Best is trial 31 with value: 0.98366647442386.\n",
            "[I 2025-04-13 11:52:31,226] Trial 67 finished with value: 0.9815950415874238 and parameters: {'max_depth': 6, 'learning_rate': 0.24268512220565866}. Best is trial 31 with value: 0.98366647442386.\n",
            "[I 2025-04-13 11:52:31,427] Trial 68 finished with value: 0.9804211405092832 and parameters: {'max_depth': 7, 'learning_rate': 0.1832604719660065}. Best is trial 31 with value: 0.98366647442386.\n",
            "[I 2025-04-13 11:52:31,576] Trial 69 finished with value: 0.43722257053971464 and parameters: {'max_depth': 4, 'learning_rate': 0.003546974870994757}. Best is trial 31 with value: 0.98366647442386.\n",
            "[I 2025-04-13 11:52:31,708] Trial 70 finished with value: 0.9063147070828061 and parameters: {'max_depth': 3, 'learning_rate': 0.024179380028199207}. Best is trial 31 with value: 0.98366647442386.\n",
            "[I 2025-04-13 11:52:31,893] Trial 71 finished with value: 0.9823016110229746 and parameters: {'max_depth': 7, 'learning_rate': 0.2638638105181145}. Best is trial 31 with value: 0.98366647442386.\n",
            "[I 2025-04-13 11:52:32,084] Trial 72 finished with value: 0.982336495571191 and parameters: {'max_depth': 7, 'learning_rate': 0.28855816676379353}. Best is trial 31 with value: 0.98366647442386.\n",
            "[I 2025-04-13 11:52:32,283] Trial 73 finished with value: 0.9808814904171238 and parameters: {'max_depth': 7, 'learning_rate': 0.2024178717799329}. Best is trial 31 with value: 0.98366647442386.\n",
            "[I 2025-04-13 11:52:32,477] Trial 74 finished with value: 0.9821773966861705 and parameters: {'max_depth': 7, 'learning_rate': 0.25236560506438094}. Best is trial 31 with value: 0.98366647442386.\n",
            "[I 2025-04-13 11:52:32,673] Trial 75 finished with value: 0.9783038956563251 and parameters: {'max_depth': 6, 'learning_rate': 0.14221432963210393}. Best is trial 31 with value: 0.98366647442386.\n",
            "[I 2025-04-13 11:52:32,820] Trial 76 finished with value: 0.1645004860318755 and parameters: {'max_depth': 4, 'learning_rate': 0.0010846894088641236}. Best is trial 31 with value: 0.98366647442386.\n",
            "[I 2025-04-13 11:52:33,014] Trial 77 finished with value: 0.9807391593703502 and parameters: {'max_depth': 7, 'learning_rate': 0.1825471919849083}. Best is trial 31 with value: 0.98366647442386.\n",
            "[I 2025-04-13 11:52:33,158] Trial 78 finished with value: 0.981975914311812 and parameters: {'max_depth': 4, 'learning_rate': 0.2884485708160193}. Best is trial 31 with value: 0.98366647442386.\n",
            "[I 2025-04-13 11:52:33,351] Trial 79 finished with value: 0.9812644407004942 and parameters: {'max_depth': 7, 'learning_rate': 0.22353429326528618}. Best is trial 31 with value: 0.98366647442386.\n",
            "[I 2025-04-13 11:52:33,525] Trial 80 finished with value: 0.983506283784971 and parameters: {'max_depth': 6, 'learning_rate': 0.29506593966377886}. Best is trial 31 with value: 0.98366647442386.\n",
            "[I 2025-04-13 11:52:33,703] Trial 81 finished with value: 0.9830354466975516 and parameters: {'max_depth': 6, 'learning_rate': 0.2971868618764938}. Best is trial 31 with value: 0.98366647442386.\n",
            "[I 2025-04-13 11:52:33,876] Trial 82 finished with value: 0.9822402557444113 and parameters: {'max_depth': 6, 'learning_rate': 0.24586164760650145}. Best is trial 31 with value: 0.98366647442386.\n",
            "[I 2025-04-13 11:52:34,053] Trial 83 finished with value: 0.9806012604825302 and parameters: {'max_depth': 6, 'learning_rate': 0.19358281328470647}. Best is trial 31 with value: 0.98366647442386.\n",
            "[I 2025-04-13 11:52:34,268] Trial 84 finished with value: 0.8469994635148351 and parameters: {'max_depth': 6, 'learning_rate': 0.012204261007416315}. Best is trial 31 with value: 0.98366647442386.\n",
            "[I 2025-04-13 11:52:34,438] Trial 85 finished with value: 0.9827372448846002 and parameters: {'max_depth': 6, 'learning_rate': 0.2545795143515373}. Best is trial 31 with value: 0.98366647442386.\n",
            "[I 2025-04-13 11:52:34,611] Trial 86 finished with value: 0.9824509959927128 and parameters: {'max_depth': 6, 'learning_rate': 0.251887966235229}. Best is trial 31 with value: 0.98366647442386.\n",
            "[I 2025-04-13 11:52:34,794] Trial 87 finished with value: 0.9818593551370065 and parameters: {'max_depth': 6, 'learning_rate': 0.20984863930573708}. Best is trial 31 with value: 0.98366647442386.\n",
            "[I 2025-04-13 11:52:35,012] Trial 88 finished with value: 0.9819976102435118 and parameters: {'max_depth': 6, 'learning_rate': 0.25348976404849005}. Best is trial 31 with value: 0.98366647442386.\n",
            "[I 2025-04-13 11:52:35,306] Trial 89 finished with value: 0.2037825703209923 and parameters: {'max_depth': 5, 'learning_rate': 0.0013456819099876352}. Best is trial 31 with value: 0.98366647442386.\n",
            "[I 2025-04-13 11:52:35,582] Trial 90 finished with value: 0.9795530452106149 and parameters: {'max_depth': 6, 'learning_rate': 0.16059317782423824}. Best is trial 31 with value: 0.98366647442386.\n",
            "[I 2025-04-13 11:52:35,811] Trial 91 finished with value: 0.9828977432440656 and parameters: {'max_depth': 5, 'learning_rate': 0.29464743348430233}. Best is trial 31 with value: 0.98366647442386.\n",
            "[I 2025-04-13 11:52:36,041] Trial 92 finished with value: 0.9821495798975395 and parameters: {'max_depth': 5, 'learning_rate': 0.22273788028255892}. Best is trial 31 with value: 0.98366647442386.\n",
            "[I 2025-04-13 11:52:36,294] Trial 93 finished with value: 0.9823663478085064 and parameters: {'max_depth': 6, 'learning_rate': 0.269967637805319}. Best is trial 31 with value: 0.98366647442386.\n",
            "[I 2025-04-13 11:52:36,538] Trial 94 finished with value: 0.9805044173134158 and parameters: {'max_depth': 5, 'learning_rate': 0.17493357084091776}. Best is trial 31 with value: 0.98366647442386.\n",
            "[I 2025-04-13 11:52:36,802] Trial 95 finished with value: 0.9830912881152392 and parameters: {'max_depth': 6, 'learning_rate': 0.29737492779287145}. Best is trial 31 with value: 0.98366647442386.\n",
            "[I 2025-04-13 11:52:37,073] Trial 96 finished with value: 0.9817660640068994 and parameters: {'max_depth': 6, 'learning_rate': 0.2123115183730089}. Best is trial 31 with value: 0.98366647442386.\n",
            "[I 2025-04-13 11:52:37,290] Trial 97 finished with value: 0.9830352007030304 and parameters: {'max_depth': 6, 'learning_rate': 0.29708946108518264}. Best is trial 31 with value: 0.98366647442386.\n",
            "[I 2025-04-13 11:52:37,464] Trial 98 finished with value: 0.9823956494832423 and parameters: {'max_depth': 6, 'learning_rate': 0.22909448470914823}. Best is trial 31 with value: 0.98366647442386.\n",
            "[I 2025-04-13 11:52:37,639] Trial 99 finished with value: 0.983010422669588 and parameters: {'max_depth': 6, 'learning_rate': 0.2989106975850874}. Best is trial 31 with value: 0.98366647442386.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Best Model: Gradient Boosting\n",
            "CV R2 Score: 0.9868233141493619\n",
            "Best Parameters: {'max_depth': 4, 'learning_rate': 0.18649815054613553}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Re-train best model on train set and evaluate on test\n",
        "best_model = LGBMRegressor(**best_model_params, random_state=17)\n",
        "best_model.fit(X_train, y_train)\n",
        "preds = best_model.predict(X_test)\n",
        "r2 = r2_score(y_test, preds)\n",
        "print(f\"Test R² Score of Tuned Model: {r2}\")\n"
      ],
      "metadata": {
        "id": "fCb0i-M5-vRo",
        "outputId": "2762936f-20c0-48ea-ea09-cc4ab6d7325c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test R² Score of Tuned Model: 0.9803303385658326\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q4 a)**\n",
        "\n",
        "a) Best Model and Parameters Using Cross Validation (CV=3)\n",
        "We used Optuna to tune the hyperparameters of these regressors below using 100 trials and 3-fold cross-validation:\n",
        "\n",
        "Random Forest Regressor, Gradient Boosting Regressor, XGBoost Regressor, LightGBM Regressor\n",
        "\n",
        "After running Optuna for each model, we compared their average R² scores (how well they explain the target variable).\n",
        "\n",
        "The model with the highest average cross-validation R² score was:\n",
        "\n",
        "**Gradient Boosting Regressor** with best Cross-Validation R² Score: **0.9868**, and best parameters\tare **max_depth = 4, learning_rate = 0.18649815054613553**\n",
        "\n",
        "**Test R² Score**: 0.9803\n",
        "\n",
        "From Q3, the default Gradient Boosting Regressor had test R² Score: **0.9800**\n",
        "\n",
        "After tuning, Test R² Score: **0.9803**\n",
        "\n",
        "**Conclusion**: There is a slight improvement in performance when using the optimized parameters compared to the default ones. Although the gain is small, it confirms that hyperparameter tuning via Optuna helped the model generalize marginally better on unseen test data.\n",
        "\n",
        "**Q4 b)**\n",
        "\n",
        "**Evaluation on Test Data**: After finding the best model (Gradient Boosting Regressor), we trained it again using the entire training data and then tested it on the test set.\n",
        "\n",
        "**Test R² Score (Tuned Model):**\t0.9803\n",
        "\n",
        "In Q3, when we used the same Gradient Boosting Regressor with its default parameters,** the R² score on the test data** was **0.9800.**\n",
        "\n",
        "With Optuna-tuned parameters, the R² score increased slightly to 0.9803. Even though the improvement (from 0.9800 to 0.9803) is small, it shows that tuning helped the model perform slightly better on the test data."
      ],
      "metadata": {
        "id": "nWn9oBl7CB4h"
      }
    }
  ]
}